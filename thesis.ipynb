{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.7.10","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[{"sourceId":23812,"sourceType":"datasetVersion","datasetId":17810}],"dockerImageVersionId":30146,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# CELL 1: Setup, Imports, and Configuration\nprint(\"--- Cell 1: Setup, Imports, and Configuration ---\")\n\n# Core Libraries\nimport os\nimport numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nimport cv2\nimport random\nimport glob\nfrom math import ceil\nimport gc\nimport inspect\nfrom functools import partial\nimport json\nimport traceback\nimport time  # For timing experiments\n\n# Scikit-learn\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.metrics import (\n    classification_report, confusion_matrix, roc_auc_score, roc_curve,\n    accuracy_score, precision_score, recall_score, f1_score, precision_recall_curve\n)\nfrom sklearn.utils import class_weight\n\n# TensorFlow / Keras\nimport tensorflow as tf\nfrom tensorflow import keras\nfrom tensorflow.keras import layers, models, applications\nfrom tensorflow.keras.preprocessing.image import load_img, img_to_array\nfrom tensorflow.keras.layers import (\n    Dense, Dropout, BatchNormalization, GlobalAveragePooling2D, GlobalMaxPooling2D,\n    Input, Conv2D, Add, Multiply, Activation, Concatenate, Reshape, Layer, Softmax\n)\nfrom tensorflow.keras.layers.experimental import preprocessing as keras_preprocessing\nfrom tensorflow.keras.callbacks import EarlyStopping, ReduceLROnPlateau, ModelCheckpoint\nfrom tensorflow.keras.mixed_precision import Policy, set_global_policy\nimport tensorflow_addons as tfa\n# print(f\"TensorFlow Version: {tf.__version__}\")\n# print(f\"Keras Version: {keras.__version__}\")\n\n# --- Configuration ---\nSEED = 42\nIMG_SIZE = 224 # Using a single dimension for square images\nIMG_SHAPE = (IMG_SIZE, IMG_SIZE, 3)\nBATCH_SIZE_PER_REPLICA = 32 # Adjust based on GPU memory\nEPOCHS_HEAD = 30       # Epochs for initial training (head only)\nEPOCHS_FINETUNE = 40     # Epochs for fine-tuning\nLEARNING_RATE = .0001\nLEARNING_RATE_FINETUNE = .00001\nDROPOUT_RATE = 0.3\nPATIENCE_EARLY_STOPPING = 8\nPATIENCE_REDUCE_LR = 4\nMIN_LR = 1e-7\nTARGET_METRIC = 'f1_opt' \n\n# Experiment Tracking - Simple dictionary for results\nresults = {}\n# Directory for saving model checkpoints\nCHECKPOINT_DIR = \"/kaggle/working/checkpoints\"\nGRADCAM_DIR = \"/kaggle/working/gradcam_outputs\"\nMETRICS_DIR = \"/kaggle/working/saved\"\nPLOTS_DIR = \"/kaggle/working/plots\"\n# Create directories if they don't exist\nos.makedirs(CHECKPOINT_DIR, exist_ok=True)\nos.makedirs(GRADCAM_DIR, exist_ok=True)\nos.makedirs(METRICS_DIR, exist_ok=True)\nos.makedirs(PLOTS_DIR, exist_ok=True)\n\n\nprint(f\"Checkpoint directory: {CHECKPOINT_DIR}\")\nprint(f\"Grad-CAM output directory: {GRADCAM_DIR}\")\n\n# --- Hardware Setup ---\n# Mixed Precision (Optional but recommended for speed/memory on compatible GPUs)\ntry:\n    policy = Policy('mixed_float16')\n    set_global_policy(policy)\n    print('Mixed precision enabled: Compute dtype=%s, Variable dtype=%s' % (\n          policy.compute_dtype, policy.variable_dtype))\nexcept Exception as e:\n    print(f\"Could not enable mixed precision: {e}\")\n\n# GPU Configuration\ngpus = tf.config.list_physical_devices('GPU')\nif gpus:\n    try:\n        for gpu in gpus:\n            tf.config.experimental.set_memory_growth(gpu, True)\n        print(f\"Found {len(gpus)} GPUs. Memory growth enabled.\")\n        # Set up distribution strategy\n        if len(gpus) > 1:\n            strategy = tf.distribute.MirroredStrategy()\n            print(f\"Using MirroredStrategy with {strategy.num_replicas_in_sync} devices.\")\n        else:\n            strategy = tf.distribute.get_strategy() # Default strategy for single GPU\n            print(\"Using default strategy for single GPU.\")\n    except RuntimeError as e:\n        print(f\"GPU setup error: {e}. Falling back to default strategy.\")\n        strategy = tf.distribute.get_strategy()\nelse:\n    print(\"No GPUs found. Using default strategy (CPU).\")\n    strategy = tf.distribute.get_strategy()\n\n# Calculate Global Batch Size\nGLOBAL_BATCH_SIZE = BATCH_SIZE_PER_REPLICA * strategy.num_replicas_in_sync\nprint(f\"Global Batch Size: {GLOBAL_BATCH_SIZE}\")\n\n# AUTOTUNE for tf.data pipelines\nAUTOTUNE = tf.data.AUTOTUNE\n\n# --- Reproducibility ---\n# os.environ['TF_DETERMINISTIC_OPS'] = '1' # Commented out: Caused UnimplementedError on GPU\n# os.environ['TF_CUDNN_DETERMINISTIC'] = '1' # Also comment out or set to '0' if needed\nprint(\"Note: TF_DETERMINISTIC_OPS disabled for GPU compatibility. Minor non-determinism may occur.\")\nrandom.seed(SEED)\nnp.random.seed(SEED)\ntf.random.set_seed(SEED)\n# Set Python hash seed (for certain operations)\nos.environ['PYTHONHASHSEED'] = str(SEED)\n\nprint(\"--- Setup Complete ---\")\n\n\n","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","_kg_hide-input":true,"trusted":true,"execution":{"iopub.status.busy":"2025-06-03T15:32:48.173086Z","iopub.execute_input":"2025-06-03T15:32:48.173793Z","iopub.status.idle":"2025-06-03T15:32:48.198646Z","shell.execute_reply.started":"2025-06-03T15:32:48.173759Z","shell.execute_reply":"2025-06-03T15:32:48.197889Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# CELL 2: Data Loading and Path Definition\nprint(\"\\n--- Cell 2: Data Loading and Path Definition ---\")\n\n# --- Dataset Paths ---\n# Adjust BASE_PATH if your dataset is located elsewhere\ntry:\n    if os.path.exists(\"/kaggle/input/chest-xray-pneumonia/chest_xray/\"):\n        BASE_PATH = \"/kaggle/input/chest-xray-pneumonia/chest_xray/\"\n        print(\"Using Kaggle dataset path.\")\n    else:\n        # Example for local structure - MODIFY AS NEEDED\n        local_path = \"./chest_xray/\"\n        if os.path.exists(local_path):\n             BASE_PATH = local_path\n             print(f\"Using local dataset path: {BASE_PATH}\")\n        else:\n             raise FileNotFoundError(\"Dataset base path not found locally or on Kaggle.\")\n\n    TRAIN_PATH = os.path.join(BASE_PATH, \"train\")\n    VAL_PATH = os.path.join(BASE_PATH, \"val\")\n    TEST_PATH = os.path.join(BASE_PATH, \"test\")\n\n    # Basic check for subdirectories\n    for p in [TRAIN_PATH, VAL_PATH, TEST_PATH]:\n        if not os.path.exists(p):\n            print(f\"WARNING: Dataset directory does not exist: {p}\")\n        elif not os.listdir(p):\n             print(f\"WARNING: Dataset directory is empty: {p}\")\n\nexcept FileNotFoundError as e:\n    print(f\"ERROR: {e}\")\n    print(\"Please ensure the dataset is available and BASE_PATH is set correctly.\")\n    # Optionally, raise the error again to stop execution\n    # raise\n\n# --- Load All Image Paths and Labels ---\ndef load_image_paths_and_labels(base_dir, label_map=None):\n    \"\"\"Loads image paths and numeric labels from subdirectories.\"\"\"\n    paths = []\n    labels = []\n    new_label_map = {}\n    is_new_map = False\n    if label_map is None:\n        label_map = {}\n        is_new_map = True\n\n    print(f\"Loading data from: {base_dir}\")\n    try:\n        categories = sorted([d for d in os.listdir(base_dir) if os.path.isdir(os.path.join(base_dir, d))])\n        if not categories:\n            print(f\"ERROR: No category subdirectories found in {base_dir}\")\n            return [], [], {}\n\n        for i, category in enumerate(categories):\n            if is_new_map:\n                label_map[category.upper()] = i # Use uppercase for consistency\n            label = label_map.get(category.upper())\n            if label is None:\n                 print(f\"Warning: Category '{category}' not found in provided label map. Skipping.\")\n                 continue\n\n            category_path = os.path.join(base_dir, category)\n            image_files = []\n            # Common image extensions (consider adding .png if needed)\n            for ext in ['*.jpeg', '*.jpg', '*.png']:\n                image_files.extend(glob.glob(os.path.join(category_path, ext)))\n\n            print(f\"  Found {len(image_files)} images for '{category}' (Label: {label})\")\n            paths.extend(image_files)\n            labels.extend([label] * len(image_files))\n\n    except FileNotFoundError:\n        print(f\"ERROR: Directory not found: {base_dir}\")\n    except Exception as e:\n        print(f\"ERROR loading from {base_dir}: {e}\")\n\n    if not paths:\n        print(f\"WARNING: No images found in {base_dir}\")\n\n    return paths, labels, label_map if is_new_map else {}\n\n\n# Load paths and create label map from the training directory initially\nprint(\"Loading initial paths from TRAIN directory to define labels...\")\ntry:\n    train_paths_orig, train_labels_orig, label_dict = load_image_paths_and_labels(TRAIN_PATH)\n    if not label_dict:\n        raise ValueError(\"Could not determine label mapping from training data.\")\n\n    print(\"\\nLoading paths from VAL directory...\")\n    val_paths_orig, val_labels_orig, _ = load_image_paths_and_labels(VAL_PATH, label_dict)\n    print(\"\\nLoading paths from TEST directory...\")\n    test_paths_orig, test_labels_orig, _ = load_image_paths_and_labels(TEST_PATH, label_dict)\n\n    # Create inverse mapping for display purposes\n    inv_label_dict = {v: k for k, v in label_dict.items()}\n    print(f\"\\nLabel Mapping: {label_dict}\")\n    print(f\"Inverse Label Mapping: {inv_label_dict}\")\n\n    # Combine all paths and labels\n    all_paths = train_paths_orig + val_paths_orig + test_paths_orig\n    all_labels = train_labels_orig + val_labels_orig + test_labels_orig\n    print(f\"\\nTotal images found across all sets: {len(all_paths)}\")\n\n    if len(all_paths) == 0:\n        raise ValueError(\"No images loaded. Check dataset paths and structure.\")\n    if len(all_paths) != len(all_labels):\n         raise ValueError(\"Mismatch between loaded image paths and labels count.\")\n\n    # Clean up original lists to save memory\n    del train_paths_orig, train_labels_orig, val_paths_orig, val_labels_orig\n    del test_paths_orig, test_labels_orig\n    gc.collect()\n\nexcept Exception as e:\n    print(f\"ERROR during data loading: {e}\")\n    # Consider raising the error to halt if loading is critical\n    # raise\n\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-03T15:32:48.321875Z","iopub.execute_input":"2025-06-03T15:32:48.322587Z","iopub.status.idle":"2025-06-03T15:32:48.773132Z","shell.execute_reply.started":"2025-06-03T15:32:48.32255Z","shell.execute_reply":"2025-06-03T15:32:48.772489Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# CELL 3: Stratified Splitting, Class Weights, and Visualization\nprint(\"\\n--- Cell 3: Stratified Splitting, Class Weights, and Visualization ---\")\n\nif 'all_paths' not in locals() or not all_paths:\n    print(\"ERROR: 'all_paths' not available. Cannot perform splitting. Check Cell 2.\")\nelse:\n    print(f\"Performing 75/5/20 stratified split on {len(all_paths)} images...\")\n\n    # --- Perform Stratified Split ---\n    # Ensure labels are numpy array for stratification\n    all_labels_np = np.array(all_labels)\n\n    # Split 1: Separate Test set (20%)\n    try:\n        train_val_paths, test_paths, train_val_labels, test_labels = train_test_split(\n            all_paths, all_labels_np,\n            test_size=0.20,  # 20% for test\n            random_state=SEED,\n            stratify=all_labels_np,\n            shuffle=True\n        )\n\n        # Split 2: Separate Validation set from the remaining Train/Val (target 5% of original total)\n        # Calculate validation size relative to the train_val set\n        original_total = len(all_paths)\n        val_target_size = 0.05\n        test_actual_size = len(test_paths) / original_total\n        # Relative size: val_target / (1 - test_actual_size)\n        val_relative_size = val_target_size / (1.0 - test_actual_size)\n\n        if val_relative_size >= 1.0 or val_relative_size <= 0:\n             print(f\"Warning: Calculated relative validation size ({val_relative_size:.3f}) is invalid. Setting validation set to empty.\")\n             train_paths, val_paths, train_labels, val_labels = train_val_paths, [], train_val_labels, []\n        else:\n            train_paths, val_paths, train_labels, val_labels = train_test_split(\n                train_val_paths, train_val_labels,\n                test_size=val_relative_size,\n                random_state=SEED,\n                stratify=train_val_labels,\n                shuffle=True\n            )\n\n        print(\"\\nSplit complete:\")\n        print(f\"  Train Set:      {len(train_paths):>6} images ({len(train_paths)/original_total:7.1%})\")\n        print(f\"  Validation Set: {len(val_paths):>6} images ({len(val_paths)/original_total:7.1%})\")\n        print(f\"  Test Set:       {len(test_paths):>6} images ({len(test_paths)/original_total:7.1%})\")\n        print(f\"  Total Verified: {len(train_paths) + len(val_paths) + len(test_paths):>6} images\")\n\n        # Convert labels back to lists (optional, but consistent with input)\n        train_labels = list(train_labels)\n        val_labels = list(val_labels)\n        test_labels = list(test_labels)\n\n    except ValueError as e:\n         print(f\"ERROR during train/test split: {e}\")\n         print(\"This might happen if a class has only 1 sample. Check dataset balance.\")\n         # Consider stopping execution if split fails\n         raise\n    except Exception as e:\n        print(f\"An unexpected error occurred during splitting: {e}\")\n        raise\n\n    # Clean up intermediate variables\n    del all_paths, all_labels, all_labels_np, train_val_paths, train_val_labels\n    gc.collect()\n\n    # --- Calculate Class Weights (using the new train_labels) ---\n    print(\"\\nCalculating Class Weights for Training Set...\")\n    unique_classes, class_counts = np.unique(train_labels, return_counts=True)\n\n    if len(unique_classes) < 2:\n        print(\"WARNING: Only one class found in the training set. Class weights set to None.\")\n        class_weights_dict = None\n    else:\n        # Calculate balanced weights\n        total_samples = len(train_labels)\n        num_classes = len(unique_classes)\n        weights = total_samples / (num_classes * class_counts)\n        class_weights_dict = dict(zip(unique_classes, weights))\n        print(\"Calculated Class Weights:\")\n        for cls, weight in class_weights_dict.items():\n            print(f\"  Class {cls} ({inv_label_dict[cls]}): {weight:.4f}\")\n\n    # --- Visualize Split Distributions ---\n    def plot_split_distributions(split_data, label_map_inv):\n        \"\"\"Generates bar plots showing class counts and percentages within each split.\"\"\"\n        num_splits = len(split_data)\n        if num_splits == 0: return\n\n        fig, axes = plt.subplots(1, num_splits, figsize=(6 * num_splits, 5), sharey=False)\n        if num_splits == 1: axes = [axes] # Ensure iterable\n\n        fig.suptitle(f'Dataset Split Class Distribution', fontsize=16, y=1.03)\n        class_names = sorted(list(label_map_inv.values()))\n        palette = sns.color_palette('viridis', n_colors=len(class_names))\n\n        for i, (name, (paths, labels)) in enumerate(split_data.items()):\n            ax = axes[i]\n            count = len(labels)\n            ax.set_title(f\"{name} Set ({count} images)\")\n\n            if count > 0:\n                counts_series = pd.Series(labels).map(label_map_inv).value_counts().reindex(class_names, fill_value=0)\n                percentages = (counts_series / count) * 100\n\n                sns.barplot(x=counts_series.index, y=percentages.values, ax=ax, palette=palette, order=class_names)\n                ax.set_ylabel(\"Percentage (%)\" if i == 0 else \"\")\n                ax.set_ylim(0, 105)\n                ax.tick_params(axis='x', rotation=0)\n\n                # Add percentage labels\n                if ax.containers:\n                    try:\n                        ax.bar_label(ax.containers[0], fmt='%.1f%%', padding=3, fontsize=9)\n                    except IndexError: pass # Handle potential issues\n            else:\n                ax.text(0.5, 0.5, 'No Data', ha='center', va='center', transform=ax.transAxes)\n                ax.set_ylim(0, 105)\n            ax.grid(axis='y', linestyle='--', alpha=0.7)\n\n        plt.tight_layout(rect=[0, 0, 1, 0.97])\n        plt.show()\n\n    split_summary_data = {\n        \"Train\": (train_paths, train_labels),\n        \"Validation\": (val_paths, val_labels),\n        \"Test\": (test_paths, test_labels)\n    }\n    plot_split_distributions(split_summary_data, inv_label_dict)\n\n\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-03T15:32:48.778334Z","iopub.execute_input":"2025-06-03T15:32:48.778582Z","iopub.status.idle":"2025-06-03T15:32:49.542481Z","shell.execute_reply.started":"2025-06-03T15:32:48.778553Z","shell.execute_reply":"2025-06-03T15:32:49.541799Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# CELL 4: Augmentation Layer and Preprocessing Functions\nprint(\"\\n--- Cell 4: Augmentation Layer and Preprocessing Functions ---\")\n\n# --- Standard Augmentation Layer ---\n# Using Keras preprocessing layers for GPU acceleration\nstandard_augmentation = tf.keras.Sequential([\n    layers.Input(shape=IMG_SHAPE),\n    keras_preprocessing.RandomFlip(\"horizontal\", seed=SEED),\n    keras_preprocessing.RandomRotation(0.1, seed=SEED), # Slight rotation\n    keras_preprocessing.RandomZoom(height_factor=0.1, width_factor=0.1, seed=SEED), # Slight zoom\n    # keras_preprocessing.RandomContrast(0.1, seed=SEED), # Optional: slight contrast change\n    # keras_preprocessing.RandomBrightness(0.1, seed=SEED), # Optional: slight brightness change\n], name='standard_augmentation')\n\n# --- Image Decoding ---\n@tf.function\ndef decode_image(image_bytes):\n    \"\"\"Decodes JPEG/PNG, converts to float32, ensures 1 channel.\"\"\"\n    # Try JPEG first, then PNG\n    img = tf.io.decode_image(image_bytes, channels=1, expand_animations=False) # Ensure single channel\n    img = tf.cast(img, tf.float32)\n    return img\n\n# --- CLAHE Application (using tf.py_function) ---\n# --- CLAHE Application (using tf.py_function) ---\ndef apply_cv_clahe_np(image_np, clip_limit, grid_size): # grid_size might arrive as tensor tuple\n    \"\"\"Applies CLAHE using OpenCV to a NumPy array, handling potential tensor inputs.\"\"\"\n    # Input image is expected float32, convert to uint8 for OpenCV\n    if image_np.shape[-1] != 1: # Ensure single channel\n        print(\"Warning: Image for CLAHE is not single channel, attempting grayscale conversion.\")\n        if len(image_np.shape) == 3 and image_np.shape[-1] == 3:\n             image_uint8 = cv2.cvtColor(image_np.astype(np.uint8), cv2.COLOR_RGB2GRAY)\n        else:\n             print(\"Error: Cannot convert image to grayscale for CLAHE.\")\n             return image_np.astype(np.float32) # Return original as float\n    else:\n        # Squeeze channel dim if present, ensure uint8\n        image_uint8 = np.squeeze(image_np).astype(np.uint8)\n\n    # --- ADD EXPLICIT CONVERSION FOR grid_size ---\n    try:\n        # Check if grid_size items are TF Tensors (have .numpy()) and convert, otherwise assume Python type\n        tile_h = int(grid_size[0].numpy()) if hasattr(grid_size[0], 'numpy') else int(grid_size[0])\n        tile_w = int(grid_size[1].numpy()) if hasattr(grid_size[1], 'numpy') else int(grid_size[1])\n        cv2_grid_size = (tile_h, tile_w)\n        # print(f\"Debug: Converted grid_size to {cv2_grid_size}\") # Optional debug print\n    except Exception as e:\n        # Fallback to default if conversion fails for any reason\n        print(f\"Warning: Could not parse grid_size ({grid_size}). Using default (8, 8). Error: {e}\")\n        cv2_grid_size = (8, 8)\n    # --- END CONVERSION ---\n\n    try:\n        # Use the converted tuple of Python ints\n        clahe = cv2.createCLAHE(clipLimit=float(clip_limit), tileGridSize=cv2_grid_size)\n        clahe_img = clahe.apply(image_uint8)\n        # Add channel dimension back and cast to float32 for TF\n        processed_image = np.expand_dims(clahe_img, axis=-1).astype(np.float32)\n    except Exception as cv_e:\n         print(f\"Error during cv2.createCLAHE or apply: {cv_e}\")\n         # Return original image (uint8 converted back to float32 with channel) if CLAHE fails\n         processed_image = np.expand_dims(image_uint8, axis=-1).astype(np.float32)\n\n    return processed_image\ndef tf_apply_clahe(image, clip_limit, grid_size=(8, 8)):\n    \"\"\"TensorFlow wrapper for applying CLAHE using py_function.\"\"\"\n    # Input image is expected to be float32, shape [H, W, 1]\n    # Need float32 output from py_func\n    processed_image = tf.py_function(\n        func=apply_cv_clahe_np,\n        inp=[image, clip_limit, grid_size], # Pass clip_limit and grid_size\n        Tout=tf.float32\n    )\n    # Ensure shape is set after py_function\n    processed_image.set_shape([None, None, 1]) # Keep channel dim\n    return processed_image\n\n# --- Unified Preprocessing Function ---\n@tf.function\ndef preprocess_image(image_path, label, img_size=IMG_SIZE, apply_augment=False, augment_layer=None,\n                     apply_clahe=False, clahe_clip_limit=2.0, clahe_grid_size=(8, 8)):\n    \"\"\"\n    Loads, decodes, optionally applies CLAHE, resizes, optionally applies augmentation,\n    and preprocesses the image for DenseNet121.\n    \"\"\"\n    image_bytes = tf.io.read_file(image_path)\n    image = decode_image(image_bytes) # Decodes to grayscale float32 [H, W, 1]\n\n    # 1. Apply CLAHE (if enabled) - BEFORE resizing for better effect\n    if apply_clahe:\n        image = tf_apply_clahe(image, clip_limit=clahe_clip_limit, grid_size=clahe_grid_size)\n\n    # 2. Resize\n    image = tf.image.resize(image, [img_size, img_size], method=tf.image.ResizeMethod.BILINEAR) # Use bilinear for float images\n\n    # 3. Convert Grayscale to RGB (Required by DenseNet)\n    image = tf.image.grayscale_to_rgb(image) # Converts [H, W, 1] to [H, W, 3]\n\n    # 4. Apply Augmentation (if enabled) - AFTER resizing and RGB conversion\n    if apply_augment and augment_layer is not None:\n        # Keras layers expect batch dimension\n        image = tf.expand_dims(image, axis=0)\n        image = augment_layer(image, training=True) # Apply augmentation\n        image = tf.squeeze(image, axis=0) # Remove batch dimension\n        image = tf.cast(image, tf.float32) # Ensure float32 after augmentation\n\n    # 5. Preprocess for DenseNet121\n    image = tf.keras.applications.densenet.preprocess_input(image)\n\n    return image, label\n\n# --- Dataset Building Function ---\ndef build_dataset(paths, labels, preprocess_fn_base, preprocess_args,\n                  batch_size, dataset_name=\"Dataset\",\n                  shuffle=False, augment_in_map=False, oversample=False,\n                  cache=True):\n    \"\"\"\n    Builds a tf.data.Dataset with preprocessing, optional shuffling,\n    optional oversampling, batching, and prefetching.\n\n    Args:\n        paths (list): List of image file paths.\n        labels (list): List of corresponding labels.\n        preprocess_fn_base (function): The base preprocessing function (e.g., preprocess_image).\n        preprocess_args (dict): Dictionary of arguments for the preprocessing function.\n        batch_size (int): Global batch size.\n        dataset_name (str): Name for printing messages.\n        shuffle (bool): Whether to shuffle the dataset (typically True for train).\n        augment_in_map (bool): Whether to apply augmentation within the map function.\n                               (Passed via preprocess_args['apply_augment']).\n        oversample (bool): Whether to oversample the minority class (typically True for train).\n        cache (bool or str): Whether to cache the dataset (True for memory, file path for disk).\n    \"\"\"\n    if not paths:\n        print(f\"WARNING [{dataset_name}]: Empty paths list provided. Returning None.\")\n        return None\n    if len(paths) != len(labels):\n        print(f\"ERROR [{dataset_name}]: Mismatch paths ({len(paths)}) vs labels ({len(labels)}).\")\n        return None\n\n    AUTO = tf.data.AUTOTUNE\n    num_classes = len(np.unique(labels))\n\n    # Create the partial function for mapping BEFORE creating the dataset slices\n    # This ensures all necessary arguments are bound.\n    map_fn = partial(preprocess_fn_base, **preprocess_args)\n\n    ds = tf.data.Dataset.from_tensor_slices((paths, labels))\n\n    # Apply mapping function (preprocessing)\n    ds = ds.map(map_fn, num_parallel_calls=AUTO)\n\n    # Apply caching (after mapping, before repeating/shuffling/sampling)\n    if cache:\n        if isinstance(cache, str): # Disk caching\n            safe_suffix = \"\".join(c if c.isalnum() else \"_\" for c in dataset_name)\n            cache_file = os.path.join(CHECKPOINT_DIR, f\"tf_cache_{safe_suffix}\") # Use checkpoint dir\n            ds = ds.cache(cache_file)\n            print(f\"[{dataset_name}] Caching to disk: {cache_file}\")\n        else: # Memory caching\n            ds = ds.cache()\n            print(f\"[{dataset_name}] Caching to memory.\")\n\n    # --- Oversampling (if enabled, typically only for training set) ---\n    if oversample and num_classes > 1 and shuffle: # Only makes sense for training\n        print(f\"[{dataset_name}] Applying oversampling...\")\n        unique_cls, _ = np.unique(labels, return_counts=True)\n        datasets_by_class = []\n        # Filter dataset for each class\n        for cls_index in unique_cls:\n            datasets_by_class.append(ds.filter(lambda img, lbl: lbl == cls_index))\n\n        # Define desired distribution (equal probability for each class)\n        target_dist = [1.0 / num_classes] * num_classes\n\n        # Use sample_from_datasets for resampling\n        # Note: This samples indefinitely, so take() is needed if used without repeat()\n        # Since we usually repeat() for training, this should be fine.\n        ds = tf.data.experimental.sample_from_datasets(\n            datasets_by_class, weights=target_dist, seed=SEED\n        )\n        print(f\"[{dataset_name}] Oversampling applied.\")\n\n    # Apply shuffling (if enabled, typically for training)\n    if shuffle:\n        buffer_size = min(len(paths), 5000) # Adjust buffer size based on dataset size/memory\n        ds = ds.shuffle(buffer_size=buffer_size, seed=SEED, reshuffle_each_iteration=True)\n        print(f\"[{dataset_name}] Shuffling applied (buffer={buffer_size}).\")\n\n    # Apply batching\n    ds = ds.batch(batch_size)\n\n    # Apply prefetching\n    ds = ds.prefetch(buffer_size=AUTO)\n\n    # Apply distribution options\n    options = tf.data.Options()\n    options.experimental_distribute.auto_shard_policy = tf.data.experimental.AutoShardPolicy.DATA\n    ds = ds.with_options(options)\n\n    args_str = \", \".join(f\"{k}={v}\" for k,v in preprocess_args.items())\n    print(f\"-> [{dataset_name}] Built: items={len(paths)}, shuffle={shuffle}, oversample={oversample}, map_args=({args_str})\")\n    return ds\n\n\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-03T15:32:49.544178Z","iopub.execute_input":"2025-06-03T15:32:49.544844Z","iopub.status.idle":"2025-06-03T15:32:49.77526Z","shell.execute_reply.started":"2025-06-03T15:32:49.544799Z","shell.execute_reply":"2025-06-03T15:32:49.774357Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# CELL 5: Model Building Functions (DenseNet121, Attention Layers)\nprint(\"\\n--- Cell 5: Model Building Functions ---\")\n\n# --- Attention Layer Implementations ---\n# Using the provided implementations, ensuring they are Layers\nclass SelfAttention(Layer):\n    def __init__(self, units=64, **kwargs):\n        super().__init__(**kwargs)\n        self.units = units\n        self.query_layer = Dense(units, name='query')\n        self.key_layer = Dense(units, name='key')\n        self.value_layer = Dense(None, name='value') # Output channels inferred in build\n        self.softmax = Softmax(axis=-1)\n        self.add = Add()\n\n    def build(self, input_shape):\n        channels = input_shape[-1]\n        if channels is None:\n            raise ValueError(\"Channel dimension must be known for SelfAttention.\")\n        # Correctly set output units for value layer\n        self.value_layer.units = channels\n        super().build(input_shape)\n\n    def call(self, inputs):\n        # B = Batch size, H = Height, W = Width, C = Channels\n        input_shape = tf.shape(inputs)\n        B, H, W = input_shape[0], input_shape[1], input_shape[2]\n        C = tf.compat.dimension_value(inputs.shape[-1]) # Static preferred\n\n        flattened = Reshape((H * W, C))(inputs) # Shape: (B, H*W, C)\n\n        q = self.query_layer(flattened)  # Shape: (B, H*W, units)\n        k = self.key_layer(flattened)  # Shape: (B, H*W, units)\n        v = self.value_layer(flattened)  # Shape: (B, H*W, C)\n\n        # Attention Scores\n        scores = tf.matmul(q, k, transpose_b=True)  # Shape: (B, H*W, H*W)\n\n        # Scaling\n        dk = tf.cast(tf.shape(k)[-1], tf.float32)\n        scaled_scores = scores / tf.math.sqrt(dk)\n\n        # Weights\n        weights = self.softmax(scaled_scores) # Shape: (B, H*W, H*W)\n\n        # Weighted Values\n        attention_output = tf.matmul(weights, v)  # Shape: (B, H*W, C)\n\n        # Reshape and Residual Connection\n        output_reshaped = Reshape((H, W, C))(attention_output)\n        output = self.add([inputs, output_reshaped])\n        return output\n\n    def get_config(self):\n        config = super().get_config()\n        config.update({\"units\": self.units})\n        return config\n\nclass ChannelAttention(Layer):\n    def __init__(self, ratio=8, **kwargs):\n        super().__init__(**kwargs)\n        self.ratio = ratio\n        self.avg_pool = GlobalAveragePooling2D(keepdims=True)\n        self.max_pool = GlobalMaxPooling2D(keepdims=True)\n        # Dense layers will be built in build()\n\n    def build(self, input_shape):\n        channels = input_shape[-1]\n        if channels is None: raise ValueError(\"Channel dimension required.\")\n        self.shared_dense_1 = Dense(channels // self.ratio, activation='relu', kernel_initializer='he_normal', use_bias=True, name='ca_dense_1')\n        self.shared_dense_2 = Dense(channels, kernel_initializer='he_normal', use_bias=True, name='ca_dense_2')\n        super().build(input_shape)\n\n    def call(self, inputs):\n        avg_pooled = self.avg_pool(inputs)\n        max_pooled = self.max_pool(inputs)\n\n        avg_out = self.shared_dense_2(self.shared_dense_1(avg_pooled))\n        max_out = self.shared_dense_2(self.shared_dense_1(max_pooled))\n\n        attention = Activation('sigmoid')(Add()([avg_out, max_out]))\n        return Multiply()([inputs, attention])\n\n    def get_config(self):\n        config = super().get_config()\n        config.update({\"ratio\": self.ratio})\n        return config\n\nclass SpatialAttention(Layer):\n    def __init__(self, kernel_size=7, **kwargs):\n        super().__init__(**kwargs)\n        self.kernel_size = kernel_size\n        self.concat = Concatenate(axis=-1)\n        # Conv2D layer built in build()\n\n    def build(self, input_shape):\n         self.conv2d = Conv2D(1, kernel_size=self.kernel_size, padding='same', activation='sigmoid', kernel_initializer='he_normal', use_bias=False, name='sa_conv')\n         super().build(input_shape)\n\n    def call(self, inputs):\n        avg_pooled = tf.reduce_mean(inputs, axis=-1, keepdims=True) # Avg across channels\n        max_pooled = tf.reduce_max(inputs, axis=-1, keepdims=True)  # Max across channels\n\n        concat = self.concat([avg_pooled, max_pooled]) # Shape: (B, H, W, 2)\n        attention = self.conv2d(concat) # Shape: (B, H, W, 1)\n\n        return Multiply()([inputs, attention])\n\n    def get_config(self):\n        config = super().get_config()\n        config.update({\"kernel_size\": self.kernel_size})\n        return config\n\nclass CBAM(Layer):\n    def __init__(self, ratio=8, kernel_size=7, **kwargs):\n        super().__init__(**kwargs)\n        self.ratio = ratio\n        self.kernel_size = kernel_size\n        self.channel_attn = ChannelAttention(ratio=ratio, name='cbam_channel')\n        self.spatial_attn = SpatialAttention(kernel_size=kernel_size, name='cbam_spatial')\n\n    def call(self, inputs):\n        x = self.channel_attn(inputs)\n        x = self.spatial_attn(x)\n        return x\n\n    def get_config(self):\n        config = super().get_config()\n        config.update({\n            \"ratio\": self.ratio,\n            \"kernel_size\": self.kernel_size\n        })\n        return config\n\n# Store custom layers for potential loading later\ncustom_objects_map = {\n    'SelfAttention': SelfAttention,\n    'ChannelAttention': ChannelAttention,\n    'SpatialAttention': SpatialAttention,\n    'CBAM': CBAM,\n}\n\n\n# --- Model Creation Functions ---\ndef create_base_model(input_shape, trainable=False):\n    \"\"\"Creates the DenseNet121 base model.\"\"\"\n    base = applications.DenseNet121(weights='imagenet', include_top=False, input_shape=input_shape)\n    base.trainable = trainable\n    return base\n\ndef add_classification_head(inputs, num_classes, pooling_type='avg', attention_type=None, dropout_rate=0.3):\n    \"\"\"Adds attention (optional), pooling, and classification layers.\"\"\"\n    x = inputs\n\n    # 1. Attention (Optional)\n    if attention_type:\n        attn_name = f'attn_{attention_type}'\n        if attention_type == 'self': x = SelfAttention(units=64, name=attn_name)(x)\n        elif attention_type == 'channel': x = ChannelAttention(ratio=8, name=attn_name)(x)\n        elif attention_type == 'spatial': x = SpatialAttention(kernel_size=7, name=attn_name)(x)\n        elif attention_type == 'cbam': x = CBAM(ratio=8, kernel_size=7, name=attn_name)(x)\n        else: raise ValueError(f\"Unknown attention type: {attention_type}\")\n\n    # 2. Pooling\n    pool_name = f'pool_{pooling_type}'\n    if pooling_type == 'avg': x = GlobalAveragePooling2D(name=pool_name)(x)\n    elif pooling_type == 'max': x = GlobalMaxPooling2D(name=pool_name)(x)\n    elif pooling_type == 'hybrid':\n        avg_pool = GlobalAveragePooling2D(name='pool_hybrid_avg')(x)\n        max_pool = GlobalMaxPooling2D(name='pool_hybrid_max')(x)\n        x = Concatenate(name='pool_hybrid_concat')([avg_pool, max_pool])\n    else: raise ValueError(f\"Unknown pooling type: {pooling_type}\")\n\n    # 3. Classification Head Layers\n    x = BatchNormalization(name='head_bn_1')(x)\n    x = Dropout(dropout_rate, seed=SEED, name='head_dropout_1')(x)\n    x = Dense(128, activation='relu', name='head_dense_1', kernel_initializer='he_normal')(x)\n    x = BatchNormalization(name='head_bn_2')(x)\n    x = Dropout(dropout_rate, seed=SEED, name='head_dropout_2')(x)\n\n    # Final Output Layer\n    if num_classes == 1: # Binary classification\n        activation = 'sigmoid'\n        units = 1\n    else: # Multi-class\n        activation = 'softmax'\n        units = num_classes\n\n    outputs = Dense(units, activation=activation, name='classifier_output')(x)\n    # If using mixed precision, ensure output is float32\n    if tf.keras.mixed_precision.global_policy().compute_dtype == 'float16':\n         outputs = Activation('linear', dtype='float32')(outputs)\n\n    return outputs\n\ndef build_full_model(input_shape, num_classes, pooling='avg', attention=None, dropout=DROPOUT_RATE, base_trainable=False):\n    \"\"\"Builds the complete model with base and head.\"\"\"\n    inputs = Input(shape=input_shape, name='input_image')\n    base_model = create_base_model(input_shape, trainable=base_trainable)\n    base_output = base_model(inputs, training=base_trainable) # Control training mode\n\n    outputs = add_classification_head(\n        base_output,\n        num_classes=num_classes,\n        pooling_type=pooling,\n        attention_type=attention,\n        dropout_rate=dropout\n    )\n\n    model_name = f'DenseNet121_P-{pooling or \"none\"}_A-{attention or \"none\"}'\n    model = keras.Model(inputs=inputs, outputs=outputs, name=model_name)\n    return model\n\nprint(\"Model building functions defined.\")\n\n\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-03T15:32:49.777194Z","iopub.execute_input":"2025-06-03T15:32:49.777422Z","iopub.status.idle":"2025-06-03T15:32:49.818955Z","shell.execute_reply.started":"2025-06-03T15:32:49.777395Z","shell.execute_reply":"2025-06-03T15:32:49.818103Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# CELL 6: Core Training, Evaluation, and Enhanced Visualization Utilities\nprint(\"\\n--- Cell 6: Core Training, Evaluation, and Enhanced Visualization Utilities ---\")\n\nimport time\nimport numpy as np\nimport pandas as pd\nimport tensorflow as tf # Ensure tensorflow is imported\nfrom tensorflow import keras # Ensure keras is imported\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nfrom sklearn.metrics import (\n    accuracy_score, precision_score, recall_score, f1_score, # For find_optimal_threshold & evaluate_model\n    roc_auc_score, confusion_matrix, classification_report, # For evaluate_model & plotting\n    roc_curve, # For evaluate_model & plotting\n    precision_recall_curve, average_precision_score # For new plotting\n)\nimport os\nimport traceback # For debugging if needed\nfrom math import ceil\n\n# --- Matplotlib Publication Quality Settings ---\nplt.rcParams.update({\n    'font.size': 11,\n    'font.family': 'serif',\n    'axes.labelsize': 12,\n    'axes.titlesize': 14,\n    'xtick.labelsize': 10,\n    'ytick.labelsize': 10,\n    'legend.fontsize': 11,\n    'figure.titlesize': 16,\n    'axes.linewidth': 1.2,\n    'grid.linewidth': 0.8,\n    'lines.linewidth': 2.0,\n    'patch.linewidth': 0.5,\n    'savefig.dpi': 300,\n    # 'savefig.format': 'pdf', # Default format for savefig can be specified directly in the call\n    'savefig.bbox': 'tight'\n})\n\n# --- Training Function ---\ndef train_model(model, train_ds, val_ds, epochs, class_weights, strategy, learning_rate,\n                initial_epoch=0, callbacks=None, stage_name=\"Training\"):\n    \"\"\"Compiles and trains the model within the strategy scope.\"\"\"\n    if train_ds is None:\n        print(f\"ERROR [{stage_name}]: Training dataset is None. Cannot train.\")\n        return None, None\n\n    with strategy.scope():\n        # IMPORTANT: For F1-score to be in history, add it here.\n        # Ensure 'import tensorflow_addons as tfa' is in Cell 1.\n        metrics_list = [\n            'accuracy',\n            tf.keras.metrics.Precision(name='precision'),\n            tf.keras.metrics.Recall(name='recall'),\n            # tf.keras.metrics.AUC(name='auc')\n            # Example to add F1-score (make sure tfa is imported in Cell 1):\n            tfa.metrics.F1Score(num_classes=1, threshold=0.5, name='f1_score', average='micro'), # For binary\n        ]\n        model.compile(\n            optimizer=keras.optimizers.Adam(learning_rate=learning_rate),\n            loss='binary_crossentropy', # Assuming binary classification from num_classes=1 elsewhere\n            metrics=metrics_list\n        )\n\n    print(f\"\\n--- Starting {stage_name} ---\")\n    print(f\"Epochs: {epochs}, Initial Epoch: {initial_epoch}\")\n    print(f\"Learning Rate: {learning_rate}\")\n    print(f\"Class Weights: {'Applied' if class_weights else 'None'}\")\n\n    start_time = time.time()\n    history = model.fit(\n        train_ds,\n        validation_data=val_ds,\n        epochs=epochs,\n        initial_epoch=initial_epoch,\n        class_weight=class_weights,\n        callbacks=callbacks or [],\n        verbose=1\n    )\n    end_time = time.time()\n    training_duration = end_time - start_time\n    print(f\"--- {stage_name} Finished (Duration: {training_duration:.2f} seconds) ---\")\n    return history, training_duration\n\n# --- Threshold Optimization ---\ndef find_optimal_threshold(y_true, y_pred_proba, target_metric='f1'):\n    \"\"\"Finds the threshold maximizing the target metric on validation predictions.\"\"\"\n    best_threshold = 0.5\n    best_score = -1.0\n\n    if y_true is None or y_pred_proba is None or len(y_true) == 0 or len(y_true) != len(y_pred_proba):\n        print(\"Warning: Invalid inputs for threshold optimization. Returning default 0.5.\")\n        return best_threshold\n\n    y_true = np.array(y_true).astype(int) # Ensure numpy array\n    y_pred_proba = np.array(y_pred_proba) # Ensure numpy array\n\n    thresholds = np.arange(0.01, 1.0, 0.01)\n    scores = []\n\n    for thresh in thresholds:\n        y_pred_binary = (y_pred_proba >= thresh).astype(int)\n        if target_metric == 'f1':\n            score = f1_score(y_true, y_pred_binary, zero_division=0)\n        elif target_metric == 'accuracy':\n            score = accuracy_score(y_true, y_pred_binary)\n        elif target_metric == 'precision':\n            score = precision_score(y_true, y_pred_binary, zero_division=0)\n        elif target_metric == 'recall':\n            score = recall_score(y_true, y_pred_binary, zero_division=0)\n        else: # Default to F1\n            print(f\"Warning: Unknown target_metric '{target_metric}' for threshold optimization. Defaulting to F1.\")\n            score = f1_score(y_true, y_pred_binary, zero_division=0)\n        scores.append(score)\n\n    if scores:\n        best_idx = np.argmax(scores)\n        best_score = scores[best_idx]\n        best_threshold = thresholds[best_idx]\n    else:\n        print(\"Warning: Could not compute scores for threshold optimization. Using default 0.5.\")\n    # print(f\"Best threshold for '{target_metric}': {best_threshold:.3f} with score: {best_score:.4f}\") # Optional debug\n    return best_threshold\n\n# --- Enhanced Plotting Functions ---\ndef plot_training_history_enhanced(history, title_suffix=\"\", save_dir=None, config_name=\"model\"):\n    if history is None or not history.history:\n        print(\"No history data found to plot.\")\n        return\n    history_df = pd.DataFrame(history.history)\n    history_df['epoch'] = np.arange(1, len(history_df) + 1)\n\n    potential_metrics_map = {\n        'loss': 'Loss', 'accuracy': 'Accuracy', 'precision': 'Precision', 'recall': 'Recall',\n        'f1_score': 'F1-score', 'f1': 'F1-score', # Common keys for F1\n        'auc': 'AUC'\n    }\n    available_history_keys = list(history_df.columns)\n    metrics_for_plotting = []\n\n    # Add base metrics if available\n    for key in ['loss', 'accuracy', 'precision', 'recall']:\n        if key in available_history_keys:\n            metrics_for_plotting.append({'key': key, 'name': potential_metrics_map.get(key, key.title())})\n\n    # Determine 5th metric: F1-score (preferred) or AUC\n    f1_key_to_use = None\n    if 'f1_score' in available_history_keys: f1_key_to_use = 'f1_score'\n    elif 'f1' in available_history_keys: f1_key_to_use = 'f1'\n\n    if f1_key_to_use:\n        if len(metrics_for_plotting) < 5:\n            metrics_for_plotting.append({'key': f1_key_to_use, 'name': potential_metrics_map.get(f1_key_to_use, 'F1-score')})\n    elif 'auc' in available_history_keys: # If F1 not found, try AUC\n        if len(metrics_for_plotting) < 5:\n            metrics_for_plotting.append({'key': 'auc', 'name': potential_metrics_map.get('auc', 'AUC')})\n    \n    num_actual_plots = len(metrics_for_plotting)\n    if num_actual_plots == 0:\n        print(\"No plottable metrics found in history.\")\n        return\n\n    fig = plt.figure() # Initialize figure; size set below\n    subplot_definitions = []\n\n    if num_actual_plots == 5:\n        fig.set_size_inches(18, 10) \n        gs_fig = fig.add_gridspec(2, 6, hspace=0.45, wspace=0.5) # hspace for title, wspace for between plots\n        subplot_definitions = [\n            gs_fig[0, 0:2], gs_fig[0, 2:4], gs_fig[0, 4:6], \n            gs_fig[1, 1:3], gs_fig[1, 3:5]                  \n        ]\n    elif num_actual_plots == 4:\n        fig.set_size_inches(12, 10) \n        gs_fig = fig.add_gridspec(2, 2, hspace=0.35, wspace=0.3)\n        subplot_definitions = [gs_fig[0,0], gs_fig[0,1], gs_fig[1,0], gs_fig[1,1]]\n    elif num_actual_plots > 0: # 1, 2, or 3 plots\n        fig.set_size_inches(6 * num_actual_plots, 5.5) \n        gs_fig = fig.add_gridspec(1, num_actual_plots, hspace=0.3, wspace=0.25 if num_actual_plots > 1 else 0)\n        subplot_definitions = [gs_fig[0,i] for i in range(num_actual_plots)]\n    else: # Should be caught by num_actual_plots == 0\n        return\n\n    colors = {'train': '#2E86AB', 'val': '#A23B72'}\n    for idx, metric_info in enumerate(metrics_for_plotting):\n        metric_key = metric_info['key']\n        display_name = metric_info['name']\n        \n        ax = fig.add_subplot(subplot_definitions[idx])\n        val_metric_key = f'val_{metric_key}'\n\n        if metric_key in history_df.columns:\n            ax.plot(history_df['epoch'], history_df[metric_key],\n                    color=colors['train'], marker='o', markersize=4,\n                    label='Training', linewidth=2)\n        if val_metric_key in history_df.columns:\n            ax.plot(history_df['epoch'], history_df[val_metric_key],\n                    color=colors['val'], marker='s', markersize=4,\n                    label='Validation', linewidth=2, linestyle='--')\n        \n        ax.set_title(display_name, fontweight='bold', pad=10)\n        ax.set_xlabel('Epoch')\n        ax.set_ylabel('Value')\n        ax.grid(True, alpha=0.3)\n        ax.legend(frameon=True, fancybox=True, shadow=True)\n        \n        if metric_key not in ['loss']:\n            ax.set_ylim(-0.05, 1.05) # Allow slight dip below 0 for visual\n        else: \n            min_val_data = history_df[metric_key].dropna()\n            max_val_data = history_df[metric_key].dropna()\n            if val_metric_key in history_df and not history_df[val_metric_key].dropna().empty:\n                min_val_data = pd.concat([min_val_data, history_df[val_metric_key].dropna()])\n                max_val_data = pd.concat([max_val_data, history_df[val_metric_key].dropna()])\n            \n            min_loss = min_val_data.min() if not min_val_data.empty else 0\n            max_loss = max_val_data.max() if not max_val_data.empty else 1.0 # Ensure max_loss is float\n            \n            padding_abs = 0.1 \n            if pd.notna(min_loss) and pd.notna(max_loss) and (max_loss - min_loss) > 1e-5 :\n                 padding = 0.1 * (max_loss - min_loss)\n                 padding = max(padding, 0.05) # ensure some minimal padding\n            else:\n                 padding = padding_abs\n            \n            y_min_plot = float(min_loss - padding) if pd.isna(min_loss) else float(max(0, min_loss - padding))\n            y_max_plot = float(max_loss + padding) if pd.notna(max_loss) else float(y_min_plot + 2*padding_abs)\n            if y_max_plot <= y_min_plot: y_max_plot = y_min_plot + padding_abs # Ensure max > min\n\n            ax.set_ylim(y_min_plot, y_max_plot)\n\n    fig.suptitle(f'Training History {title_suffix}', fontsize=16, fontweight='bold')\n    plt.tight_layout(rect=[0, 0.03, 1, 0.95]) \n\n    if save_dir:\n        os.makedirs(save_dir, exist_ok=True)\n        filename = f\"training_history_{config_name}.pdf\"\n        filepath = os.path.join(save_dir, filename)\n        plt.savefig(filepath, dpi=300, bbox_inches='tight', format='pdf')\n        print(f\"Training history saved: {filepath}\")\n    plt.show()\n\ndef plot_dual_confusion_matrices(y_true, y_pred_proba, optimal_threshold, inv_label_map,\n                                 title_suffix=\"\", save_dir=None, config_name=\"model\"):\n    if y_true is None or y_pred_proba is None:\n        print(\"Cannot plot confusion matrices: Missing data\")\n        return\n    y_true = np.array(y_true).astype(int)\n    y_pred_proba = np.array(y_pred_proba)\n\n    y_pred_default = (y_pred_proba >= 0.5).astype(int)\n    y_pred_optimal = (y_pred_proba >= optimal_threshold).astype(int)\n    \n    cm_default = confusion_matrix(y_true, y_pred_default)\n    cm_optimal = confusion_matrix(y_true, y_pred_optimal)\n    \n    # Determine class names robustly\n    unique_labels = sorted(np.unique(y_true))\n    if not inv_label_map or not all(lbl in inv_label_map for lbl in unique_labels) :\n        class_names = [f\"Class {i}\" for i in unique_labels]\n    else:\n        class_names = [inv_label_map.get(lbl, f\"Class {lbl}\") for lbl in unique_labels]\n    if not class_names: class_names = [\"Class 0\", \"Class 1\"] # Fallback\n\n    fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(14, 6.5)) # Slightly taller\n    sns.heatmap(cm_default, annot=True, fmt='d', cmap='Blues',\n                xticklabels=class_names, yticklabels=class_names,\n                ax=ax1, cbar_kws={'shrink': 0.8}, square=True,\n                annot_kws={'size': 14, 'weight': 'bold'})\n    ax1.set_title('Confusion Matrix\\n(Threshold = 0.5)', fontweight='bold', pad=15)\n    ax1.set_xlabel('Predicted Label', fontweight='bold'); ax1.set_ylabel('True Label', fontweight='bold')\n\n    sns.heatmap(cm_optimal, annot=True, fmt='d', cmap='Greens',\n                xticklabels=class_names, yticklabels=class_names,\n                ax=ax2, cbar_kws={'shrink': 0.8}, square=True,\n                annot_kws={'size': 14, 'weight': 'bold'})\n    ax2.set_title(f'Confusion Matrix\\n(Optimal Threshold = {optimal_threshold:.3f})',\n                  fontweight='bold', pad=15)\n    ax2.set_xlabel('Predicted Label', fontweight='bold'); ax2.set_ylabel('True Label', fontweight='bold')\n    \n    plt.suptitle(f'Confusion Matrix Comparison {title_suffix}',\n                 fontsize=16, fontweight='bold', y=1.0) # Adjusted y\n    plt.tight_layout(rect=[0, 0, 1, 0.95]) # Make space for suptitle\n    if save_dir:\n        os.makedirs(save_dir, exist_ok=True)\n        filename = f\"confusion_matrices_{config_name}.pdf\"\n        filepath = os.path.join(save_dir, filename)\n        plt.savefig(filepath, dpi=300, bbox_inches='tight', format='pdf')\n        print(f\"Confusion matrices saved: {filepath}\")\n    plt.show()\n\ndef plot_performance_curves(y_true, y_pred_proba, optimal_threshold, inv_label_map,\n                            title_suffix=\"\", save_dir=None, config_name=\"model\"):\n    if y_true is None or y_pred_proba is None:\n        print(\"Cannot plot performance curves: Missing data\")\n        return\n    y_true = np.array(y_true).astype(int)\n    y_pred_proba = np.array(y_pred_proba)\n\n    if len(np.unique(y_true)) < 2:\n        print(\"Cannot plot ROC/PR curves: Only one class present in true labels.\")\n        return\n\n    fpr, tpr, roc_thresholds = roc_curve(y_true, y_pred_proba)\n    roc_auc = roc_auc_score(y_true, y_pred_proba)\n    precision_vals, recall_vals, pr_thresholds = precision_recall_curve(y_true, y_pred_proba)\n    avg_precision = average_precision_score(y_true, y_pred_proba)\n    \n    positive_class_label = 1 # Assuming positive class is 1 for binary\n    positive_class_name = inv_label_map.get(positive_class_label, \"Positive Class\") if isinstance(inv_label_map, dict) else \"Positive Class\"\n\n\n    fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(14, 6.5)) # Slightly taller\n    # ROC Curve\n    ax1.plot(fpr, tpr, color='#2E86AB', linewidth=3, label=f'ROC Curve (AUC = {roc_auc:.4f})')\n    ax1.plot([0, 1], [0, 1], color='gray', linewidth=2, linestyle='--', alpha=0.7, label='Random Classifier')\n    \n    # Find point for optimal threshold on ROC curve\n    # roc_thresholds also includes some high values like inf, so be careful with argmin\n    if len(roc_thresholds) > 0:\n        optimal_idx_roc = np.argmin(np.abs(roc_thresholds - optimal_threshold))\n        # Ensure index is valid for fpr and tpr arrays\n        if optimal_idx_roc < len(fpr) and optimal_idx_roc < len(tpr):\n             ax1.scatter(fpr[optimal_idx_roc], tpr[optimal_idx_roc], color='red', s=100,\n                        zorder=5, label=f'Optimal Threshold ({optimal_threshold:.3f})')\n\n    ax1.set_xlim([-0.02, 1.0]); ax1.set_ylim([0.0, 1.05]) # Start x slightly before 0\n    ax1.set_xlabel('False Positive Rate', fontweight='bold'); ax1.set_ylabel('True Positive Rate', fontweight='bold')\n    ax1.set_title(f'ROC Curve\\n({positive_class_name} Detection)', fontweight='bold', pad=15)\n    ax1.legend(loc=\"lower right\", frameon=True, fancybox=True, shadow=True); ax1.grid(True, alpha=0.3)\n\n    # Precision-Recall Curve\n    ax2.plot(recall_vals, precision_vals, color='#A23B72', linewidth=3, label=f'PR Curve (AP = {avg_precision:.4f})')\n    no_skill = len(y_true[y_true==positive_class_label]) / len(y_true) if len(y_true) > 0 else 0\n    ax2.axhline(y=no_skill, color='gray', linewidth=2, linestyle='--', alpha=0.7,\n                label=f'No-Skill Classifier (AP = {no_skill:.4f})')\n\n    # Find point for optimal threshold on PR curve\n    # pr_thresholds is shorter by 1 than precision and recall.\n    # It corresponds to decisions made *between* points on the curve.\n    # So, pr_thresholds[i] is the threshold used to get recall_vals[i+1] and precision_vals[i+1]\n    if len(pr_thresholds) > 0:\n        # Find index in pr_thresholds closest to optimal_threshold\n        optimal_idx_pr_thresh = np.argmin(np.abs(pr_thresholds - optimal_threshold))\n        # The corresponding point on PR curve is at index optimal_idx_pr_thresh + 1 for recall and precision\n        # Ensure this index is valid for recall_vals and precision_vals\n        point_idx_pr = optimal_idx_pr_thresh +1\n        if point_idx_pr < len(recall_vals) and point_idx_pr < len(precision_vals):\n            ax2.scatter(recall_vals[point_idx_pr], precision_vals[point_idx_pr],\n                        color='red', s=100, zorder=5,\n                        label=f'Optimal Threshold ({optimal_threshold:.3f})')\n\n    ax2.set_xlim([0.0, 1.02]); ax2.set_ylim([0.0, 1.05]) # End x slightly after 1\n    ax2.set_xlabel('Recall', fontweight='bold'); ax2.set_ylabel('Precision', fontweight='bold')\n    ax2.set_title(f'Precision-Recall Curve\\n({positive_class_name} Detection)', fontweight='bold', pad=15)\n    ax2.legend(loc=\"lower left\", frameon=True, fancybox=True, shadow=True); ax2.grid(True, alpha=0.3)\n\n    plt.suptitle(f'Performance Curves {title_suffix}', fontsize=16, fontweight='bold', y=1.0) # Adjusted y\n    plt.tight_layout(rect=[0, 0, 1, 0.95])\n    if save_dir:\n        os.makedirs(save_dir, exist_ok=True)\n        filename = f\"performance_curves_{config_name}.pdf\"\n        filepath = os.path.join(save_dir, filename)\n        plt.savefig(filepath, dpi=300, bbox_inches='tight', format='pdf')\n        print(f\"Performance curves saved: {filepath}\")\n    plt.show()\n\n# --- Evaluation Function (Fixed model.predict calls) ---\n# In your consolidated Cell 6:\n# Ensure these imports are at the top of Cell 6 if not already:\n# from sklearn.metrics import average_precision_score, roc_auc_score, ... (other metrics)\n# import numpy as np\n\ndef evaluate_model_optimized_with_viz(model, val_ds, test_ds, strategy, inv_label_map,\n                                      target_metric='f1', dataset_name=\"Test\",\n                                      save_dir=None, config_name=\"model\"):\n    \"\"\"\n    Evaluates the model using logic from old 'evaluate_model_optimized'\n    and then calls new enhanced visualization functions. NOW INCLUDES PR AUC.\n    \"\"\"\n    results_eval = {}\n    y_true_val_np, y_pred_proba_val_np = None, None\n    y_true_eval_np, y_pred_proba_eval_np = None, None\n    # cm_eval_np = None # Not returned by this version as run_experiment doesn't use it directly\n    optimal_threshold = 0.5 \n\n    print(f\"\\n--- Evaluating Model on {dataset_name} Set ---\")\n    print(f\"  Using target metric '{target_metric}' for threshold optimization on validation set.\")\n\n    # 1. Get Compiled Metrics on Evaluation Set (using model.evaluate @ 0.5 threshold)\n    print(f\"Running model.evaluate() on {dataset_name} set...\")\n    if test_ds is not None:\n        try:\n            eval_results_tf = model.evaluate(test_ds, verbose=0, return_dict=True) # Renamed to avoid clash\n            print(\"  Compiled metrics (@ 0.5 Threshold from model.evaluate):\")\n            for k, v in eval_results_tf.items():\n                metric_val = float(v) if isinstance(v, (np.float32, np.float64)) else v\n                print(f\"    {k}: {metric_val:.4f}\")\n                results_eval[k] = metric_val # Stores loss, accuracy, precision, recall, auc\n        except Exception as e:\n            print(f\"  Error during model.evaluate() on {dataset_name} set: {e}\")\n            print(\"  Skipping compiled metrics evaluation from model.evaluate.\")\n    else:\n        print(f\"  {dataset_name} dataset not provided. Skipping compiled metrics evaluation from model.evaluate.\")\n\n    # 2. Get Predictions on Validation Set for Threshold Optimization\n    # ... (this part remains the same - it finds optimal_threshold) ...\n    print(f\"\\nRunning model.predict() on validation set for threshold optimization...\")\n    if val_ds is not None:\n        try:\n            # Efficiently get all labels and predictions\n            y_true_val_list_batches = []\n            y_pred_proba_val_list_batches = []\n            print(\"  Extracting true labels and making predictions on validation set...\")\n            for images_batch_val, labels_batch_val in val_ds.as_numpy_iterator(): # Iterate once\n                y_true_val_list_batches.append(labels_batch_val)\n                y_pred_proba_val_list_batches.append(model.predict(images_batch_val, verbose=0))\n\n            if y_true_val_list_batches:\n                y_true_val_np = np.concatenate([item.flatten() for item in y_true_val_list_batches])\n                y_pred_proba_val_np = np.concatenate([item.flatten() for item in y_pred_proba_val_list_batches])\n                print(f\"  Extracted {len(y_true_val_np)} validation labels and made {len(y_pred_proba_val_np)} predictions.\")\n\n                if len(y_true_val_np) == len(y_pred_proba_val_np) and len(y_true_val_np) > 0:\n                    optimal_threshold = find_optimal_threshold(y_true_val_np, y_pred_proba_val_np, target_metric)\n                    print(f\"  Optimal threshold determined from validation set: {optimal_threshold:.3f}\")\n                else:\n                    print(f\"  Warning: Mismatch or empty validation labels/predictions ({len(y_true_val_np)} vs {len(y_pred_proba_val_np)}). Using default threshold 0.5.\")\n                    optimal_threshold = 0.5\n            else:\n                print(\"  Warning: No labels/data extracted from validation dataset. Using default threshold 0.5.\")\n                optimal_threshold = 0.5\n        except Exception as e:\n            print(f\"  Error during validation prediction/threshold optimization: {e}\")\n            # import traceback\n            # traceback.print_exc()\n            print(\"  Using default threshold 0.5.\")\n            optimal_threshold = 0.5\n    else:\n        print(\"  Validation dataset not provided. Using default threshold 0.5 for evaluation set metrics.\")\n        optimal_threshold = 0.5\n    results_eval['optimal_threshold'] = optimal_threshold\n    results_eval['threshold_target_metric'] = target_metric if val_ds else 'N/A'\n\n\n    # 3. Get Predictions on Evaluation Set (e.g., test_ds) for Detailed Metrics\n    print(f\"\\nRunning model.predict() on {dataset_name} set for detailed metrics...\")\n    if test_ds is not None:\n        try:\n            y_true_eval_list_batches = []\n            y_pred_proba_eval_list_batches = []\n            print(f\"  Extracting true labels and making predictions on {dataset_name} set...\")\n            for images_batch_eval, labels_batch_eval in test_ds.as_numpy_iterator(): # Iterate once\n                y_true_eval_list_batches.append(labels_batch_eval)\n                y_pred_proba_eval_list_batches.append(model.predict(images_batch_eval, verbose=0))\n\n            if y_true_eval_list_batches:\n                y_true_eval_np = np.concatenate([item.flatten() for item in y_true_eval_list_batches])\n                y_pred_proba_eval_np = np.concatenate([item.flatten() for item in y_pred_proba_eval_list_batches])\n                print(f\"  Extracted {len(y_true_eval_np)} {dataset_name} labels and made {len(y_pred_proba_eval_np)} predictions.\")\n\n                if len(y_true_eval_np) == len(y_pred_proba_eval_np) and len(y_true_eval_np) > 0:\n                    y_pred_eval_optimized = (y_pred_proba_eval_np >= optimal_threshold).astype(int)\n                    print(f\"\\nDetailed Metrics ({dataset_name} Set @ Optimal Threshold {optimal_threshold:.3f}):\")\n                    try:\n                        results_eval['accuracy_opt'] = accuracy_score(y_true_eval_np, y_pred_eval_optimized)\n                        results_eval['precision_opt'] = precision_score(y_true_eval_np, y_pred_eval_optimized, zero_division=0)\n                        results_eval['recall_opt'] = recall_score(y_true_eval_np, y_pred_eval_optimized, zero_division=0)\n                        results_eval['f1_opt'] = f1_score(y_true_eval_np, y_pred_eval_optimized, zero_division=0)\n\n                        if len(np.unique(y_true_eval_np)) > 1: # Need at least two classes for AUCs\n                            try:\n                                results_eval['roc_auc_proba'] = roc_auc_score(y_true_eval_np, y_pred_proba_eval_np)\n                                # --- ADDED PR AUC (Average Precision) ---\n                                results_eval['pr_auc'] = average_precision_score(y_true_eval_np, y_pred_proba_eval_np)\n                                # --------------------------------------\n                            except ValueError as auc_e:\n                                print(f\"  Warning: Could not calculate ROC AUC or PR AUC scores: {auc_e}\")\n                                results_eval['roc_auc_proba'] = np.nan\n                                results_eval['pr_auc'] = np.nan # Also set PR AUC to NaN\n                        else:\n                            results_eval['roc_auc_proba'] = np.nan\n                            results_eval['pr_auc'] = np.nan # Also set PR AUC to NaN for single class\n                            print(\"  Warning: Only one class present in test labels, ROC AUC and PR AUC are undefined.\")\n\n                        print(f\"  Accuracy (opt):  {results_eval.get('accuracy_opt', np.nan):.4f}\")\n                        print(f\"  Precision (opt): {results_eval.get('precision_opt', np.nan):.4f}\")\n                        print(f\"  Recall (opt):    {results_eval.get('recall_opt', np.nan):.4f}\")\n                        print(f\"  F1 Score (opt):  {results_eval.get('f1_opt', np.nan):.4f}\")\n                        print(f\"  ROC AUC (proba): {results_eval.get('roc_auc_proba', np.nan):.4f}\")\n                        # --- ADDED PR AUC PRINT ---\n                        print(f\"  PR AUC (AvgPrec):{results_eval.get('pr_auc', np.nan):.4f}\")\n                        # -------------------------\n\n                        cm_eval_np = confusion_matrix(y_true_eval_np, y_pred_eval_optimized)\n                        print(f\"\\nClassification Report ({dataset_name} Set @ Optimal Threshold {optimal_threshold:.3f}):\")\n                        num_classes_eval = len(np.unique(y_true_eval_np))\n                        target_names_report = [inv_label_map.get(i, f\"Class {i}\") for i in range(num_classes_eval)] if num_classes_eval == 2 else [inv_label_map.get(c, f\"Class {c}\") for c in sorted(np.unique(y_true_eval_np))]\n                        if not target_names_report: target_names_report = [\"Unknown\"]\n                        print(classification_report(y_true_eval_np, y_pred_eval_optimized, target_names=target_names_report, labels=np.unique(y_true_eval_np), zero_division=0))\n                    except Exception as metric_e:\n                        print(f\"  Error calculating detailed sklearn metrics: {metric_e}\")\n                else:\n                    print(f\"  Warning: Mismatch or empty {dataset_name} labels/predictions. Cannot calculate detailed sklearn metrics.\")\n            else:\n                print(f\"  Warning: No labels/data extracted from {dataset_name} dataset. Cannot calculate detailed sklearn metrics.\")\n        except Exception as e:\n            print(f\"  Error during {dataset_name} prediction or detailed metrics calculation: {e}\")\n            # import traceback\n            # traceback.print_exc()\n    else:\n        print(f\"  {dataset_name} dataset not provided. Cannot calculate detailed sklearn metrics.\")\n\n    # --- Call Enhanced Visualizations ---\n    if y_true_eval_np is not None and y_pred_proba_eval_np is not None:\n        print(\"\\n--- Generating Enhanced Visualizations ---\")\n        # These plotting functions already calculate ROC AUC and Avg Precision internally for their plots\n        plot_dual_confusion_matrices( \n            y_true_eval_np, y_pred_proba_eval_np, optimal_threshold, inv_label_map,\n            title_suffix=f\"({dataset_name} Set, Config: {config_name})\", save_dir=save_dir, config_name=config_name\n        )\n        plot_performance_curves(\n            y_true_eval_np, y_pred_proba_eval_np, optimal_threshold, inv_label_map,\n            title_suffix=f\"({dataset_name} Set, Config: {config_name})\", save_dir=save_dir, config_name=config_name\n        )\n    else:\n        print(\"\\n--- Skipping Enhanced Visualizations due to missing evaluation data ---\")\n\n    print(\"--- Evaluation Complete ---\")\n    return results_eval, y_true_eval_np, y_pred_proba_eval_np, None # cm_eval_np is not directly needed by run_experiment\n\n# In your consolidated Cell 6:\ndef plot_comparison_bars_enhanced(config_keys_to_plot, metrics_dir, title, save_dir=None,\n                                  metrics_to_display=['f1_opt', 'accuracy_opt', 'roc_auc_proba', 'pr_auc']):\n    \"\"\"\n    Creates publication-quality comparison bar charts by reading metrics from saved JSON files.\n    Removes top and right spines from subplots for a cleaner look.\n\n    Args:\n        config_keys_to_plot (list): List of configuration keys (strings) to load and plot.\n        metrics_dir (str): Path to the directory containing the \"evaluation_metrics_{key}.json\" files.\n        title (str): Main title for the plot.\n        save_dir (str, optional): Directory to save the plot PDF. Defaults to None (no save).\n        metrics_to_display (list, optional): List of metric keys (strings) from the JSON files\n                                             to extract and plot.\n    Returns:\n        pandas.DataFrame: DataFrame containing the plotted data, or None if plotting failed.\n    \"\"\"\n    if not config_keys_to_plot:\n        print(f\"No configuration keys provided for comparison plot: {title}\")\n        return None\n    if not os.path.isdir(metrics_dir):\n        print(f\"Metrics directory not found: {metrics_dir}\")\n        return None\n\n    data_for_plot = {}\n    for config_key in config_keys_to_plot:\n        json_filename = f\"evaluation_metrics_{config_key}.json\"\n        json_path = os.path.join(metrics_dir, json_filename)\n        if os.path.exists(json_path):\n            try:\n                with open(json_path, 'r') as f:\n                    metrics_from_json = json.load(f)\n                temp_metrics_for_this_config = {}\n                has_at_least_one_valid_value = False\n                for metric_name in metrics_to_display:\n                    value = metrics_from_json.get(metric_name, np.nan)\n                    temp_metrics_for_this_config[metric_name] = value\n                    if not pd.isna(value):\n                        has_at_least_one_valid_value = True\n                if has_at_least_one_valid_value:\n                    data_for_plot[config_key] = temp_metrics_for_this_config\n            except Exception as e:\n                print(f\"Error loading or parsing JSON for config '{config_key}' from {json_path}: {e}\")\n\n    if not data_for_plot:\n        print(f\"No valid data could be loaded from JSON files for the specified configurations and metrics ({metrics_to_display}) for plot: {title}\")\n        return None\n\n    df = pd.DataFrame(data_for_plot).T.reset_index().rename(columns={'index': 'Configuration'})\n    if df.empty:\n        print(f\"DataFrame is empty after processing JSON files for plot: {title}\")\n        return None\n\n    plottable_metric_keys = [mk for mk in metrics_to_display if mk in df.columns and not df[mk].isnull().all()]\n    if not plottable_metric_keys:\n        print(f\"None of the specified metrics_to_display ({metrics_to_display}) have any valid data in the loaded JSONs for plotting: {title}\")\n        return df\n\n    sort_metric = None\n    if 'f1_opt' in plottable_metric_keys: sort_metric = 'f1_opt'\n    elif plottable_metric_keys: sort_metric = plottable_metric_keys[0]\n    if sort_metric: df = df.sort_values(by=sort_metric, ascending=False, na_position='last')\n    else: df = df.sort_values(by='Configuration', ascending=True)\n\n    num_metrics_to_plot = len(plottable_metric_keys)\n    ncols = min(num_metrics_to_plot, 3)\n    nrows = ceil(num_metrics_to_plot / ncols)\n    fig, axes = plt.subplots(nrows, ncols, figsize=(5 * ncols, max(4, 0.6 * len(df) + 1.5)), squeeze=False)\n    axes = axes.flatten()\n    colors = ['#2E86AB', '#A23B72', '#F18F01', '#C73E1D', '#593E2A', '#3A7D44', '#B565A7']\n    \n    plot_idx = 0\n    for metric in plottable_metric_keys:\n        ax = axes[plot_idx]\n        \n        y_pos = np.arange(len(df))\n        bar_values = df[metric].fillna(0) \n        bars = ax.barh(y_pos, bar_values, color=colors[plot_idx % len(colors)],\n                       alpha=0.85, edgecolor='black', linewidth=0.7)\n        \n        if metric == 'pr_auc': metric_name_display = \"PR AUC (AvgPrec)\"\n        elif metric == 'roc_auc_proba': metric_name_display = \"ROC AUC\"\n        else: metric_name_display = (metric.replace('_', ' ').replace(' opt', ' (Opt)').title())\n            \n        ax.set_title(metric_name_display, fontweight='bold', pad=12, fontsize=12)\n        ax.set_xlabel('Score', fontweight='bold', fontsize=10)\n        ax.set_yticks(y_pos)\n        ax.set_yticklabels(df['Configuration'], fontsize=9)\n        \n        max_val_metric = df[metric].max()\n        if pd.isna(max_val_metric) or max_val_metric == 0 : upper_limit = 0.1 \n        elif max_val_metric <= 1.0: upper_limit = 1.05\n        else: upper_limit = max_val_metric * 1.15\n        ax.set_xlim(0, upper_limit)\n        ax.tick_params(axis='x', labelsize=8)\n        ax.grid(axis='x', linestyle=':', alpha=0.6)\n\n        # --- MODIFICATION: Remove top and right spines ---\n        ax.spines['top'].set_visible(False)\n        ax.spines['right'].set_visible(False)\n        # You could also use sns.despine(ax=ax, top=True, right=True, left=False, bottom=False)\n        # If you also want to remove left/bottom, set them to True in despine or use ax.spines['left'].set_visible(False) etc.\n        # -------------------------------------------------\n\n        for bar_idx, bar_obj in enumerate(bars):\n            original_value = df[metric].iloc[bar_idx]\n            width_for_text = bar_obj.get_width()\n            text_label = f'{original_value:.3f}' if not pd.isna(original_value) else 'N/A'\n            \n            # Adjust text position slightly if needed, especially after removing spines\n            padding_from_bar = ax.get_xlim()[1] * 0.015 # Slightly increased padding\n            ax.text(width_for_text + padding_from_bar, \n                    bar_obj.get_y() + bar_obj.get_height() / 2,\n                    text_label, ha='left', va='center',\n                    fontweight='normal', fontsize=8.5, color='dimgray')\n        plot_idx += 1\n\n    for k_ax in range(plot_idx, len(axes)): axes[k_ax].axis('off')\n    fig.suptitle(title, fontsize=16, fontweight='bold', y=0.99 if nrows > 1 else 1.02)\n    plt.tight_layout(rect=[0, 0.03, 1, 0.95 if nrows > 1 else 0.92])\n\n    if save_dir:\n        os.makedirs(save_dir, exist_ok=True)\n        safe_title = \"\".join(c if c.isalnum() else \"_\" for c in title.lower())\n        filename = f\"comparison_bars_{safe_title}.pdf\"\n        filepath = os.path.join(save_dir, filename)\n        try:\n            plt.savefig(filepath, dpi=300, bbox_inches='tight', format='pdf')\n            print(f\"Comparison bar plot saved: {filepath}\")\n        except Exception as e:\n            print(f\"Error saving comparison bar plot to {filepath}: {e}\")\n    \n    plt.show()\n    return df\n    \nprint(\"Cell 6: All utility functions (training, evaluation, enhanced plotting) are defined.\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-03T16:19:05.345607Z","iopub.execute_input":"2025-06-03T16:19:05.345938Z","iopub.status.idle":"2025-06-03T16:19:05.448361Z","shell.execute_reply.started":"2025-06-03T16:19:05.345905Z","shell.execute_reply":"2025-06-03T16:19:05.447539Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"\n# CELL 7: Experiment Runner Function\nprint(\"\\n--- Cell 7: Experiment Runner Function ---\")\n\n# REMOVED Grad-CAM specific args: test_paths_global, test_labels_global, label_dict_global\n# def run_experiment(config, test_paths_global, test_labels_global, label_dict_global):\ndef run_experiment(config): # Modified signature\n    \"\"\"\n    Runs a complete experiment stage: builds datasets, builds model,\n    trains, evaluates, stores results, and cleans up. (Grad-CAM part removed)\n    \"\"\"\n    key = config['key']\n    print(\"\\n\" + \"=\"*70)\n    print(f\" Starting Experiment: {key} \")\n    print(\"=\"*70)\n    # ... (Config printing logic remains the same) ...\n    print(\"-\"*70)\n\n    start_time_total = time.time()\n    model = None\n    history = None\n    eval_metrics = None # This will hold the dictionary of evaluation metrics\n    training_duration = 0\n    model_to_eval = None\n    checkpoint_filepath = os.path.join(CHECKPOINT_DIR, f\"{key}_best.keras\")\n\n    try:\n        # --- 1. Build Datasets ---\n        print(\"\\n[1. Building Datasets...]\")\n        if 'img_size' not in config['parse_args']:\n            config['parse_args']['img_size'] = IMG_SIZE\n        train_ds = build_dataset(\n            train_paths, train_labels, preprocess_image, config['parse_args'],\n            GLOBAL_BATCH_SIZE, f\"Train ({key})\", shuffle=True,\n            augment_in_map=config['parse_args'].get('apply_augment', False),\n            oversample=config.get('oversample_train', False), cache=True\n        )\n        val_parse_args = config['parse_args'].copy(); val_parse_args['apply_augment'] = False\n        val_ds = build_dataset(\n            val_paths, val_labels, preprocess_image, val_parse_args,\n            GLOBAL_BATCH_SIZE, f\"Validation ({key})\", shuffle=False, cache=True\n        )\n        test_parse_args = config['parse_args'].copy(); test_parse_args['apply_augment'] = False\n        test_ds = build_dataset(\n            test_paths, test_labels, preprocess_image, test_parse_args,\n            GLOBAL_BATCH_SIZE, f\"Test ({key})\", shuffle=False, cache=True\n        )\n        if not all([train_ds, val_ds, test_ds]): raise RuntimeError(\"Dataset build failed.\")\n        print(\"Datasets built successfully.\")\n\n        # --- 2. Build Model ---\n        print(\"\\n[2. Building Model...]\")\n        with strategy.scope():\n            if 'num_classes' not in config['model_args']: config['model_args']['num_classes'] = 1\n            model = build_full_model(IMG_SHAPE, **config['model_args'])\n        model.summary(line_length=100)\n        print(f\"Model '{model.name}' built.\")\n\n        # --- 3. Setup Callbacks ---\n        print(\"\\n[3. Setting up Callbacks...]\")\n        callbacks_list = [\n            EarlyStopping(monitor='val_loss', patience=PATIENCE_EARLY_STOPPING, verbose=1, restore_best_weights=False),\n            ReduceLROnPlateau(monitor='val_loss', factor=0.2, patience=PATIENCE_REDUCE_LR, min_lr=MIN_LR, verbose=1),\n            ModelCheckpoint(filepath=checkpoint_filepath, monitor='val_loss', save_best_only=True, save_weights_only=False, verbose=1)\n        ]\n        print(f\"Model checkpoint path: {checkpoint_filepath}\")\n\n        # --- 4. Determine Class Weights ---\n        train_class_weights = None\n        if config['train_args'].get('class_weights_setting') == 'balanced':\n            train_class_weights = class_weights_dict\n            print(\"Using 'balanced' class weights for training.\")\n        else: print(\"No class weights applied for training.\")\n\n        # --- 5. Train Model ---\n        print(\"\\n[5. Training Model...]\")\n        history, training_duration = train_model(\n            model, train_ds, val_ds,\n            epochs=config['train_args']['epochs'], class_weights=train_class_weights,\n            strategy=strategy, learning_rate=config['train_args']['learning_rate'],\n            callbacks=callbacks_list, stage_name=f\"Training ({key})\"\n        )\n        if history is None: raise RuntimeError(\"Model training failed.\")\n\n        # --- 6. Load Best Weights ---\n        print(\"\\n[6. Loading Best Weights from Checkpoint...]\")\n        if os.path.exists(checkpoint_filepath):\n            with strategy.scope():\n                model_to_eval = keras.models.load_model(checkpoint_filepath, custom_objects=custom_objects_map)\n            print(f\"Successfully loaded best model weights from {checkpoint_filepath}\")\n        else:\n            print(f\"WARNING: Checkpoint file not found at {checkpoint_filepath}. Evaluating with the last epoch's weights.\")\n            model_to_eval = model\n\n        # --- 7. Evaluate Model ---\n        print(\"\\n[7. Evaluating Model...]\")\n        eval_metrics, y_true_test, y_pred_proba_test, _ = evaluate_model_optimized_with_viz(\n            model=model_to_eval, val_ds=val_ds, test_ds=test_ds, strategy=strategy,\n            inv_label_map=inv_label_dict, target_metric=TARGET_METRIC,\n            dataset_name=f\"Test ({key})\",\n            save_dir=PLOTS_DIR,\n            config_name=key\n        )\n        if not eval_metrics: print(\"Warning: Evaluation failed or returned no metrics.\"); eval_metrics = {}\n\n        \n\n        # --- 9. Store Results ---\n        print(\"\\n[9. Storing Results...]\") # Step number remains for consistency, though Grad-CAM (step 8) is out\n        total_duration = time.time() - start_time_total\n        \n        metrics_json_save_path = None\n        if eval_metrics:\n            metrics_json_save_path = os.path.join(METRICS_DIR, f\"evaluation_metrics_{key}.json\")\n            try:\n                serializable_metrics = {}\n                for m_key, m_value in eval_metrics.items():\n                    if isinstance(m_value, np.generic):\n                        serializable_metrics[m_key] = m_value.item()\n                    elif isinstance(m_value, np.ndarray):\n                        serializable_metrics[m_key] = m_value.tolist()\n                    else:\n                        serializable_metrics[m_key] = m_value\n                \n                with open(metrics_json_save_path, 'w') as f:\n                    json.dump(serializable_metrics, f, indent=4)\n                print(f\"Evaluation metrics saved to {metrics_json_save_path}\")\n            except Exception as e:\n                print(f\"Error saving evaluation metrics to {metrics_json_save_path}: {e}\")\n                metrics_json_save_path = None\n        \n        results[key] = {\n            'config': config,\n            'metrics': eval_metrics,\n            'training_duration_sec': training_duration,\n            'total_duration_sec': total_duration,\n            'checkpoint_path': checkpoint_filepath if os.path.exists(checkpoint_filepath) else None,\n            'metrics_json_path': metrics_json_save_path\n        }\n        print(f\"Results for '{key}' stored (including path to JSON metrics).\")\n\n        # --- 10. Plotting ---\n        print(\"\\n[10. Plotting Results...]\")\n        if history:\n            plot_training_history_enhanced(\n                history,\n                title_suffix=f\" ({key})\",\n                save_dir=PLOTS_DIR,\n                config_name=key\n            )\n\n        print(f\"--- Experiment {key} Complete ---\")\n        return eval_metrics\n\n    except Exception as e:\n        print(f\"\\n\\n ****** ERROR during experiment {key} ****** \")\n        print(f\"Error Type: {type(e).__name__}\")\n        print(f\"Error Details: {e}\")\n        print(\"Traceback:\")\n        traceback.print_exc()\n        results[key] = {'status': 'failed', 'error': str(e), 'config': config}\n        return None\n\n   \n\n# --- End of run_experiment definition ---","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-03T16:19:12.44707Z","iopub.execute_input":"2025-06-03T16:19:12.447324Z","iopub.status.idle":"2025-06-03T16:19:12.473195Z","shell.execute_reply.started":"2025-06-03T16:19:12.447296Z","shell.execute_reply":"2025-06-03T16:19:12.472213Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# CELL 8: Stage 1 - Run Baseline (Transfer Learning + Standard Augmentation)\nprint(\"\\n--- Cell 8: Stage 1 - Baseline (Transfer Learning + Standard Augmentation) ---\")\n\n\nconfig_baseline = {\n    'key': \"Baseline_StdAug\",\n    'parse_args': {\n        'apply_augment': True,\n        'augment_layer': standard_augmentation, # Defined in Cell 4\n        'apply_clahe': False,\n        'clahe_clip_limit': 2.0, # Default, not used\n        'img_size': IMG_SIZE\n    },\n    'model_args': {\n        'pooling': 'avg',\n        'attention': None,\n        'dropout': DROPOUT_RATE,\n        'base_trainable': False, # Base frozen for initial training\n        'num_classes': 1 # Binary\n    },\n    'train_args': {\n        'epochs': EPOCHS_HEAD,\n        'learning_rate': LEARNING_RATE,\n        'class_weights_setting': None # Start without explicit imbalance handling\n    },\n    'oversample_train': False # Not oversampling baseline\n}\n\n# Run the baseline experiment\nbaseline_metrics = run_experiment(\n    config_baseline,\n)\n\n# Initialize the variable to track the best configuration key\n# It starts with the baseline, assuming it ran successfully\ncurrent_best_config_key = config_baseline['key'] if baseline_metrics else None\nif current_best_config_key:\n     print(f\"\\nBaseline run complete. Current best configuration key: '{current_best_config_key}'\")\nelse:\n     print(\"\\nERROR: Baseline run failed. Cannot proceed with subsequent experiments.\")\n     # Optionally raise an error here\n\n\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-03T15:32:50.040205Z","iopub.execute_input":"2025-06-03T15:32:50.040575Z","iopub.status.idle":"2025-06-03T15:35:19.552744Z","shell.execute_reply.started":"2025-06-03T15:32:50.040537Z","shell.execute_reply":"2025-06-03T15:35:19.551878Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# CELL 9: Stage 2 - CLAHE Experiments\n\nprint(\"\\n--- Cell 9: Stage 2 - CLAHE Experiments ---\")\n\nif current_best_config_key is None:\n    print(\"Skipping CLAHE experiments because baseline failed.\")\nelse:\n    clahe_clip_limits_to_test = [1.0, 2.0, 3.0]\n    clahe_stage_keys = [current_best_config_key] # Start comparison list with the baseline key\n\n    # Get the baseline config to modify\n    base_config_for_clahe = results[current_best_config_key]['config']\n\n    for clip_limit in clahe_clip_limits_to_test:\n        clahe_key = f\"{current_best_config_key}_CLAHE{clip_limit:.1f}\" # Build key based on baseline\n\n        config_clahe = {\n            'key': clahe_key,\n            'parse_args': base_config_for_clahe['parse_args'].copy(), # Copy baseline parse args\n            'model_args': base_config_for_clahe['model_args'].copy(), # Copy baseline model args\n            'train_args': base_config_for_clahe['train_args'].copy(), # Copy baseline train args\n            'oversample_train': base_config_for_clahe['oversample_train'] # Copy baseline oversample flag\n        }\n\n        # Modify only the CLAHE settings in parse_args\n        config_clahe['parse_args']['apply_clahe'] = True\n        config_clahe['parse_args']['clahe_clip_limit'] = clip_limit\n\n        # Run the experiment for this CLAHE variation\n        # Inside the loop:\n        # Run the experiment for this CLAHE variation\n        clahe_metrics = run_experiment(\n            config_clahe,\n        )\n        if clahe_metrics: # Add key only if run succeeded\n            clahe_stage_keys.append(clahe_key)\n\n    print(\"\\nCompleted CLAHE experiments.\")\n\n\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-03T15:35:19.554081Z","iopub.execute_input":"2025-06-03T15:35:19.554375Z","iopub.status.idle":"2025-06-03T15:45:18.793535Z","shell.execute_reply.started":"2025-06-03T15:35:19.554333Z","shell.execute_reply":"2025-06-03T15:45:18.792712Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# CELL 10: Stage 2 - CLAHE Comparison & Selection\nprint(\"\\n--- Cell 10: Stage 2 - CLAHE Comparison & Selection ---\")\n\nimport os\nimport json\nimport numpy as np # Required if directly handling np.nan, though pd.isna is often used\n\n# --- Configuration for this Stage ---\n# These global variables should be defined in previous cells:\n# METRICS_DIR (str): Path to the directory where evaluation_metrics_{key}.json files are stored.\n# PLOTS_DIR (str): Path to the directory where plots will be saved.\n# TARGET_METRIC (str): The primary metric for selecting the best model (e.g., 'f1_opt').\n# results (dict): The global dictionary populated by run_experiment, holding all results.\n# current_best_config_key (str): Key of the best model from the PREVIOUS stage.\n\n# Explicitly list the EXACT configuration keys for this CLAHE comparison stage.\n# These names MUST match the {key} part of your \"evaluation_metrics_{key}.json\" filenames.\n# This list includes the baseline that CLAHE is being compared against/applied to.\n# We assume \"Baseline_StdAug\" is the relevant baseline for these CLAHE variants.\nkeys_for_clahe_stage_evaluation = [\n    \"Baseline_StdAug\",\n    \"Baseline_StdAug_CLAHE1.0\",\n    \"Baseline_StdAug_CLAHE2.0\",\n    \"Baseline_StdAug_CLAHE3.0\"\n    # Add any other specific CLAHE variant keys if you ran them based on \"Baseline_StdAug\"\n]\n\n# This is the specific baseline configuration key within this stage's comparison list.\n# It's used for context in titles or for fetching its score for comparison messages.\n# It should be one of the keys from keys_for_clahe_stage_evaluation.\n# If current_best_config_key (from previous stage) is this baseline, that's good,\n# otherwise, ensure this key is correct for what you consider the 'base' in this comparison.\nreference_baseline_key_in_this_stage = \"Baseline_StdAug\"\n\nprint(f\"DEBUG: Initial current_best_config_key (from previous stage): '{current_best_config_key}'\")\nprint(f\"DEBUG: Reference baseline for this CLAHE stage comparison: '{reference_baseline_key_in_this_stage}'\")\nprint(f\"DEBUG: TARGET_METRIC for selection is: '{TARGET_METRIC}'\")\nprint(f\"DEBUG: METRICS_DIR is: '{METRICS_DIR}'\")\n\n# Filter the defined keys to only include those for which a metrics JSON file actually exists\nvalid_keys_for_comparison = [\n    key for key in keys_for_clahe_stage_evaluation\n    if os.path.exists(os.path.join(METRICS_DIR, f\"evaluation_metrics_{key}.json\"))\n]\n\nprint(f\"Found metric files for and will compare: {valid_keys_for_comparison}\")\n\nif not valid_keys_for_comparison:\n    print(\"ERROR: No metric files found for any of the specified CLAHE stage configurations. Cannot proceed with comparison.\")\n    # current_best_config_key remains unchanged\nelif len(valid_keys_for_comparison) == 1 and reference_baseline_key_in_this_stage in valid_keys_for_comparison:\n    print(f\"Warning: Only the reference baseline '{reference_baseline_key_in_this_stage}' metric file was found. \"\n          \"No other CLAHE variants to compare against in the provided list for this stage.\")\n    # current_best_config_key remains unchanged if this is the only valid key\n    # Or it becomes this key if it wasn't already.\n    if current_best_config_key != reference_baseline_key_in_this_stage :\n         print(f\"Setting current_best_config_key to '{reference_baseline_key_in_this_stage}' as it's the only valid one found for this stage.\")\n         current_best_config_key = reference_baseline_key_in_this_stage\nelse:\n    # --- Plotting ---\n    metrics_to_request_for_plot = [\n        'f1_opt',\n        'accuracy_opt',\n        'precision_opt',\n        'recall_opt',\n        'roc_auc_proba',\n        'pr_auc'  # Added PR AUC\n    ]\n    # Ensure TARGET_METRIC is in the list to be plotted, preferably first\n    if TARGET_METRIC not in metrics_to_request_for_plot:\n        metrics_to_request_for_plot.insert(0, TARGET_METRIC)\n    metrics_to_request_for_plot = list(dict.fromkeys(metrics_to_request_for_plot)) # Remove duplicates, keep order\n\n    df_clahe_comparison = plot_comparison_bars_enhanced(\n        config_keys_to_plot=valid_keys_for_comparison,\n        metrics_dir=METRICS_DIR,\n        title=f\"CLAHE Stage Comparison (Ref: {reference_baseline_key_in_this_stage})\",\n        save_dir=PLOTS_DIR,\n        metrics_to_display=metrics_to_request_for_plot\n    )\n\n    # --- Selection Logic (based on JSON files) ---\n    best_score_this_stage = -1.0  # Initialize with a value lower than any possible valid score\n    winner_key_this_stage = None\n    winner_metrics_this_stage = None # To store metrics dict of the winner\n\n    # Get the score of the reference baseline for this stage's comparison message\n    reference_baseline_score = -1.0\n    if reference_baseline_key_in_this_stage in valid_keys_for_comparison:\n        ref_baseline_json_path = os.path.join(METRICS_DIR, f\"evaluation_metrics_{reference_baseline_key_in_this_stage}.json\")\n        try:\n            with open(ref_baseline_json_path, 'r') as f:\n                ref_baseline_metrics_data = json.load(f)\n            reference_baseline_score = ref_baseline_metrics_data.get(TARGET_METRIC, -1.0)\n        except Exception as e:\n            print(f\"Warning: Error loading metrics for reference baseline '{reference_baseline_key_in_this_stage}': {e}\")\n\n    print(f\"\\nSelecting best configuration from this CLAHE stage using '{TARGET_METRIC}' \"\n          f\"(Score of '{reference_baseline_key_in_this_stage}' for reference: {reference_baseline_score:.4f}):\")\n\n    for config_key in valid_keys_for_comparison:\n        json_path = os.path.join(METRICS_DIR, f\"evaluation_metrics_{config_key}.json\")\n        current_score_from_json = -1.0 # Default for this iteration\n        current_metrics_from_json = None\n\n        try:\n            with open(json_path, 'r') as f:\n                current_metrics_from_json = json.load(f)\n            current_score_from_json = current_metrics_from_json.get(TARGET_METRIC, -1.0)\n            \n            metric_display_value = f\"{current_score_from_json:.4f}\" if current_score_from_json != -1.0 or TARGET_METRIC in current_metrics_from_json else \"Not Found\"\n            print(f\"  - Config '{config_key}': {TARGET_METRIC} = {metric_display_value}\")\n\n            # Assumes higher is better for TARGET_METRIC\n            if current_score_from_json > best_score_this_stage:\n                best_score_this_stage = current_score_from_json\n                winner_key_this_stage = config_key\n                winner_metrics_this_stage = current_metrics_from_json\n        except Exception as e:\n            print(f\"  - Config '{config_key}': Error loading/processing metrics from JSON - {e}. Skipping for winner selection.\")\n            continue # Skip to the next config_key\n\n    # --- Announce Winner of this Stage and Update the GLOBAL current_best_config_key ---\n    if winner_key_this_stage and best_score_this_stage > -1.0: # Check if a valid positive score was found\n        print(f\"\\n🏆 Winner of CLAHE Stage: '{winner_key_this_stage}' ({TARGET_METRIC}: {best_score_this_stage:.4f})\")\n        print(\"This configuration will be used as the new 'current_best_config_key' for subsequent stages.\")\n        current_best_config_key = winner_key_this_stage # Update the global best key\n\n        if winner_key_this_stage in results and 'config' in results[winner_key_this_stage]:\n            print(\"\\nWinning Configuration Details (from in-memory 'results'):\")\n            winner_config_dict = results[winner_key_this_stage]['config']\n            # Simple print, assuming you might have a dedicated print_config_details function\n            for detail_key, detail_value in winner_config_dict.items():\n                if isinstance(detail_value, dict):\n                    print(f\"  {detail_key}:\")\n                    for sub_key, sub_value in detail_value.items():\n                        if sub_key == 'augment_layer': print(f\"    {sub_key}: <Keras Layer Object>\")\n                        else: print(f\"    {sub_key}: {sub_value}\")\n                else: print(f\"  {detail_key}: {detail_value}\")\n        else:\n            print(f\"Full configuration details for winner '{winner_key_this_stage}' not found in in-memory 'results'.\")\n\n        if winner_metrics_this_stage:\n            print(\"\\nWinning Metrics (from JSON):\")\n            for m_key, m_val in winner_metrics_this_stage.items():\n                if isinstance(m_val, (float, np.floating)): print(f\"  {m_key}: {m_val:.4f}\")\n                else: print(f\"  {m_key}: {m_val}\")\n    else:\n        print(\"\\nCould not determine a new winner for the CLAHE stage (e.g., all scores were -1.0 or no valid positive scores found).\")\n        if reference_baseline_key_in_this_stage in valid_keys_for_comparison and reference_baseline_score >= best_score_this_stage:\n            print(f\"The configuration '{reference_baseline_key_in_this_stage}' (Score: {reference_baseline_score:.4f}) \"\n                  \"remains the best among those evaluated in this stage, or no improvement was found.\")\n            # If the baseline for this stage is better than any other in this stage, it becomes the new overall best.\n            if current_best_config_key != reference_baseline_key_in_this_stage and reference_baseline_score != -1.0 :\n                 current_best_config_key = reference_baseline_key_in_this_stage\n                 print(f\"Updating overall best configuration to '{current_best_config_key}'.\")\n            elif current_best_config_key == reference_baseline_key_in_this_stage:\n                 print(f\"Overall best configuration remains '{current_best_config_key}'.\")\n\n        else: # No clear winner and baseline itself wasn't valid or wasn't best\n             print(f\"The overall best configuration '{current_best_config_key}' (from before this stage) remains unchanged.\")\n\nprint(\"-\" * 70)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-03T16:19:51.794255Z","iopub.execute_input":"2025-06-03T16:19:51.794591Z","iopub.status.idle":"2025-06-03T16:19:53.505249Z","shell.execute_reply.started":"2025-06-03T16:19:51.794556Z","shell.execute_reply":"2025-06-03T16:19:53.504081Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# LEARNING_RATE=.0001\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# CELL 11: Stage 3 - Imbalance Handling Experiments\nprint(\"\\n--- Cell 11: Stage 3 - Imbalance Handling Experiments ---\")\n\n\n# The baseline for this stage is the best configuration identified after Cell 10\nimbalance_baseline_key = current_best_config_key\nimbalance_stage_keys = [] # Keep track of keys for comparison in this stage\n\nif imbalance_baseline_key is None or imbalance_baseline_key not in results:\n    print(\"ERROR: Cannot proceed with Imbalance Handling stage. Baseline configuration key is missing or invalid.\")\n    # Optionally raise error: raise ValueError(\"Baseline configuration for Imbalance Handling stage is missing.\")\nelse:\n    print(f\"Using configuration '{imbalance_baseline_key}' as baseline for Imbalance Handling stage.\")\n    # Add baseline key to the list for comparison\n    imbalance_stage_keys.append(imbalance_baseline_key)\n\n    # Retrieve the configuration dictionary of the baseline\n    baseline_config_imbalance = results[imbalance_baseline_key]['config']\n\n    # --- Experiment 3a: Apply Class Weights ---\n    print(\"\\n--- Running Imbalance Experiment: Class Weights ---\")\n    config_weights_key = f\"{imbalance_baseline_key}_ClassWeights\"\n    config_weights = {\n        'key': config_weights_key,\n        'parse_args': baseline_config_imbalance['parse_args'].copy(),\n        'model_args': baseline_config_imbalance['model_args'].copy(),\n        'train_args': baseline_config_imbalance['train_args'].copy(),\n        'oversample_train': False # Ensure oversampling is off\n    }\n    # Modify train_args to apply balanced class weights\n    config_weights['train_args']['class_weights_setting'] = 'balanced'\n\n    # Run the experiment\n    weights_metrics = run_experiment(\n    config_weights,\n    )\n    if weights_metrics: # Add key only if run succeeded\n        imbalance_stage_keys.append(config_weights_key)\n\n\n    # --- Experiment 3b: Apply Oversampling ---\n    print(\"\\n--- Running Imbalance Experiment: Oversampling ---\")\n    config_oversample_key = f\"{imbalance_baseline_key}_Oversample\"\n    config_oversample = {\n        'key': config_oversample_key,\n        'parse_args': baseline_config_imbalance['parse_args'].copy(),\n        'model_args': baseline_config_imbalance['model_args'].copy(),\n        'train_args': baseline_config_imbalance['train_args'].copy(),\n        'oversample_train': True # Enable oversampling in build_dataset\n    }\n    # Ensure class weights are explicitly off when oversampling\n    config_oversample['train_args']['class_weights_setting'] = None\n\n    # Run the experiment\n    oversample_metrics = run_experiment(\n    config_oversample,\n    )\n    if oversample_metrics: # Add key only if run succeeded\n        imbalance_stage_keys.append(config_oversample_key)\n\n    print(\"\\nCompleted Imbalance Handling experiments.\")\n\n\n\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-03T15:46:25.400924Z","iopub.execute_input":"2025-06-03T15:46:25.401223Z","iopub.status.idle":"2025-06-03T15:54:20.772446Z","shell.execute_reply.started":"2025-06-03T15:46:25.401193Z","shell.execute_reply":"2025-06-03T15:54:20.771531Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# CELL 12: Stage 3 - Imbalance Handling Comparison & Selection\nprint(\"\\n--- Cell 12: Stage 3 - Imbalance Handling Comparison & Selection ---\")\n\nimport os\nimport json\nimport numpy as np # Though pd.isna is used in plot function, direct use might occur\n\n# --- Configuration for this Stage ---\n# These global variables should be defined in previous cells:\n# METRICS_DIR (str): Path to evaluation_metrics_{key}.json files.\n# PLOTS_DIR (str): Path to save plots.\n# TARGET_METRIC (str): Primary metric for selection (e.g., 'f1_opt').\n# results (dict): Global dictionary with all experiment results (configs, paths, etc.).\n# current_best_config_key (str): Key of the best model from the PREVIOUS stage (e.g., winner of CLAHE stage).\n\n# The baseline for THIS Imbalance Handling stage is the winner from the previous stage.\nimbalance_stage_baseline_key = current_best_config_key\n\n# IMPORTANT: Define the list of exact keys for this Imbalance Handling stage.\n# This list should include the baseline for this stage and all its imbalance variants.\n# These names MUST match the {key} part of your \"evaluation_metrics_{key}.json\" filenames.\nkeys_for_imbalance_stage = [\n    imbalance_stage_baseline_key,  # e.g., \"Baseline_StdAug_CLAHE1.0\"\n    f\"{imbalance_stage_baseline_key}_ClassWeights\",\n    f\"{imbalance_stage_baseline_key}_Oversample\"\n    # Add other specific imbalance handling experiment keys if you ran more based on this baseline\n]\n\n# --- Pre-computation & Sanity Checks ---\nprint(f\"DEBUG: Baseline for this Imbalance Stage (current_best_config_key entering Cell 12): '{imbalance_stage_baseline_key}'\")\nprint(f\"DEBUG: TARGET_METRIC for selection is: '{TARGET_METRIC}'\") # Ensure this is 'f1_opt' or similar from your JSON\nprint(f\"DEBUG: METRICS_DIR is: '{METRICS_DIR}'\")\nprint(f\"DEBUG: Intended keys for Imbalance Stage (before file check): {keys_for_imbalance_stage}\")\n\n# Filter these keys to only include those for which a metrics JSON file actually exists\nvalid_keys_for_imbalance_comparison = [\n    key for key in keys_for_imbalance_stage\n    if os.path.exists(os.path.join(METRICS_DIR, f\"evaluation_metrics_{key}.json\"))\n]\nprint(f\"Found metric files for and will compare these Imbalance Stage configurations: {valid_keys_for_imbalance_comparison}\")\n\n\nif not imbalance_stage_baseline_key or imbalance_stage_baseline_key not in valid_keys_for_imbalance_comparison:\n    print(f\"ERROR: Baseline for Imbalance Stage ('{imbalance_stage_baseline_key}') metric file not found or key is None. \"\n          \"Skipping Imbalance Handling comparison.\")\n    # current_best_config_key remains unchanged from before this cell\nelif len(valid_keys_for_imbalance_comparison) <= 1: # Needs at least baseline + 1 variant for a meaningful comparison\n    print(\"Not enough successful Imbalance Handling runs (including this stage's baseline) to perform a meaningful comparison.\")\n    print(f\"Keeping previous best configuration: '{current_best_config_key}'\")\n    # current_best_config_key remains unchanged\nelse:\n    # --- Plotting ---\n    metrics_to_request_for_plot = [\n        'f1_opt',\n        'accuracy_opt',\n        'precision_opt',\n        'recall_opt',\n        'roc_auc_proba',\n        'pr_auc'  # Make sure this key exists in your JSON files\n    ]\n    if TARGET_METRIC not in metrics_to_request_for_plot:\n        metrics_to_request_for_plot.insert(0, TARGET_METRIC)\n    metrics_to_request_for_plot = list(dict.fromkeys(metrics_to_request_for_plot)) # Remove duplicates\n\n    df_imbalance_comparison = plot_comparison_bars_enhanced(\n        config_keys_to_plot=valid_keys_for_imbalance_comparison,\n        metrics_dir=METRICS_DIR,\n        title=f\"Imbalance Handling Stage Comparison (Base: {imbalance_stage_baseline_key})\",\n        save_dir=PLOTS_DIR,\n        metrics_to_display=metrics_to_request_for_plot\n    )\n\n    # --- Selection Logic (based on JSON files) ---\n    best_score_this_stage = -1.0\n    winner_key_this_stage = None \n    winner_metrics_this_stage = None\n\n    baseline_score_for_this_stage_ref = -1.0\n    baseline_ref_json_path = os.path.join(METRICS_DIR, f\"evaluation_metrics_{imbalance_stage_baseline_key}.json\")\n    try:\n        with open(baseline_ref_json_path, 'r') as f:\n            baseline_ref_metrics_data = json.load(f)\n        baseline_score_for_this_stage_ref = baseline_ref_metrics_data.get(TARGET_METRIC, -1.0)\n    except Exception as e:\n        print(f\"Warning: Error loading metrics for this stage's baseline '{imbalance_stage_baseline_key}': {e}\")\n\n    print(f\"\\nSelecting best configuration from Imbalance Handling stage using '{TARGET_METRIC}' \"\n          f\"(Score of '{imbalance_stage_baseline_key}' for reference: {baseline_score_for_this_stage_ref:.4f}):\")\n\n    for config_key in valid_keys_for_imbalance_comparison:\n        json_path = os.path.join(METRICS_DIR, f\"evaluation_metrics_{config_key}.json\")\n        current_score_from_json = -1.0\n        current_metrics_from_json = None\n\n        try:\n            with open(json_path, 'r') as f:\n                current_metrics_from_json = json.load(f)\n            current_score_from_json = current_metrics_from_json.get(TARGET_METRIC, -1.0)\n            \n            metric_display_value = f\"{current_score_from_json:.4f}\" if current_score_from_json != -1.0 or TARGET_METRIC in current_metrics_from_json else \"Not Found\"\n            print(f\"  - Config '{config_key}': {TARGET_METRIC} = {metric_display_value}\")\n\n            if current_score_from_json > best_score_this_stage: # Assumes higher is better\n                best_score_this_stage = current_score_from_json\n                winner_key_this_stage = config_key\n                winner_metrics_this_stage = current_metrics_from_json\n        except Exception as e:\n            print(f\"  - Config '{config_key}': Error loading/processing metrics from JSON - {e}. Skipping for winner selection.\")\n            continue\n\n    # --- Announce Winner of this Stage and Update the GLOBAL current_best_config_key ---\n    if winner_key_this_stage and best_score_this_stage > -1.0: \n        print(f\"\\n🏆 Winner of Imbalance Handling Stage: '{winner_key_this_stage}' ({TARGET_METRIC}: {best_score_this_stage:.4f})\")\n        print(\"This configuration will be updated as the new 'current_best_config_key'.\")\n        current_best_config_key = winner_key_this_stage # Update the global best key\n\n        if winner_key_this_stage in results and 'config' in results[winner_key_this_stage]:\n            print(\"\\nWinning Configuration Details (from in-memory 'results'):\")\n            winner_config_dict = results[winner_key_this_stage]['config']\n            for detail_key, detail_value in winner_config_dict.items():\n                if isinstance(detail_value, dict):\n                    print(f\"  {detail_key}:\")\n                    for sub_key, sub_value in detail_value.items():\n                        if sub_key == 'augment_layer': print(f\"    {sub_key}: <Keras Layer Object>\")\n                        else: print(f\"    {sub_key}: {sub_value}\")\n                else: print(f\"  {detail_key}: {detail_value}\")\n        else:\n            print(f\"Full configuration details for winner '{winner_key_this_stage}' not found in in-memory 'results'.\")\n\n        if winner_metrics_this_stage:\n            print(\"\\nWinning Metrics (from JSON):\")\n            for m_key, m_val in winner_metrics_this_stage.items():\n                if isinstance(m_val, (float, np.floating)): print(f\"  {m_key}: {m_val:.4f}\")\n                else: print(f\"  {m_key}: {m_val}\")\n    else:\n        print(\"\\nCould not determine a new winner for the Imbalance Handling stage (e.g., all scores were -1.0 or no improvement).\")\n        if imbalance_stage_baseline_key in valid_keys_for_imbalance_comparison and \\\n           baseline_score_for_this_stage_ref >= best_score_this_stage and \\\n           baseline_score_for_this_stage_ref > -1.0 : # Ensure baseline had a valid positive score\n            print(f\"The configuration '{imbalance_stage_baseline_key}' (Score: {baseline_score_for_this_stage_ref:.4f}) \"\n                  \"remains the best among those evaluated in this stage.\")\n            if current_best_config_key != imbalance_stage_baseline_key: # If current best was somehow different\n                 current_best_config_key = imbalance_stage_baseline_key \n                 print(f\"Updating overall best configuration to '{current_best_config_key}'.\")\n            else:\n                 print(f\"Overall best configuration remains '{current_best_config_key}'.\")\n        else:\n             print(f\"The overall best configuration '{current_best_config_key}' (from before this stage started, or due to errors/no improvement) remains unchanged.\")\n\nprint(\"-\" * 70)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-03T16:19:31.967476Z","iopub.execute_input":"2025-06-03T16:19:31.96779Z","iopub.status.idle":"2025-06-03T16:19:33.544946Z","shell.execute_reply.started":"2025-06-03T16:19:31.967756Z","shell.execute_reply":"2025-06-03T16:19:33.544284Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# print(LEARNING_RATE)\n# DROPOUT_RATE = 0.25\n# print(DROPOUT_RATE)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-03T16:22:33.410539Z","iopub.execute_input":"2025-06-03T16:22:33.411522Z","iopub.status.idle":"2025-06-03T16:22:33.417341Z","shell.execute_reply.started":"2025-06-03T16:22:33.411483Z","shell.execute_reply":"2025-06-03T16:22:33.416371Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# CELL 13: Stage 4 - Pooling Experiments\nprint(\"\\n--- Cell 13: Stage 4 - Pooling Experiments ---\")\n\n# The baseline for this stage is the best configuration identified after Cell 12\npooling_baseline_key = current_best_config_key\npooling_stage_keys = [] # Keep track of keys for comparison in this stage\n\nif pooling_baseline_key is None or pooling_baseline_key not in results:\n    print(\"ERROR: Cannot proceed with Pooling stage. Baseline configuration key is missing or invalid.\")\n    # Optionally raise error: raise ValueError(\"Baseline configuration for Pooling stage is missing.\")\nelse:\n    print(f\"Using configuration '{pooling_baseline_key}' as baseline for Pooling stage.\")\n    # Add baseline key to the list for comparison\n    pooling_stage_keys.append(pooling_baseline_key)\n\n    # Retrieve the configuration dictionary of the baseline\n    baseline_config_pooling = results[pooling_baseline_key]['config']\n    baseline_pooling_type = baseline_config_pooling['model_args'].get('pooling', 'avg') # Get current pooling type\n    print(f\"Baseline pooling type for this stage: '{baseline_pooling_type}'\")\n\n    pooling_types_to_test = ['max', 'hybrid']\n\n    for test_pooling_type in pooling_types_to_test:\n        # Skip if the type to test is the same as the baseline's pooling type\n        if test_pooling_type == baseline_pooling_type:\n            print(f\"Skipping pooling type '{test_pooling_type}' as it matches the baseline.\")\n            continue\n\n        print(f\"\\n--- Running Pooling Experiment: {test_pooling_type.upper()} ---\")\n        # Construct key by appending pooling type to the baseline key for this stage\n        config_pool_key = f\"{pooling_baseline_key}_Pool{test_pooling_type.upper()}\"\n\n        config_pool = {\n            'key': config_pool_key,\n            'parse_args': baseline_config_pooling['parse_args'].copy(),\n            'model_args': baseline_config_pooling['model_args'].copy(),\n            'train_args': baseline_config_pooling['train_args'].copy(),\n            'oversample_train': baseline_config_pooling['oversample_train']\n        }\n\n        # Modify only the pooling setting in model_args\n        config_pool['model_args']['pooling'] = test_pooling_type\n        # Ensure attention is still off for this stage\n        config_pool['model_args']['attention'] = None\n\n        # Run the experiment\n        pool_metrics = run_experiment(\n        config_pool,\n        )\n        if pool_metrics: # Add key only if run succeeded\n            pooling_stage_keys.append(config_pool_key)\n\n    print(\"\\nCompleted Pooling experiments.\")\n\n\n\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-03T16:22:39.457502Z","iopub.execute_input":"2025-06-03T16:22:39.457829Z","iopub.status.idle":"2025-06-03T16:29:05.683148Z","shell.execute_reply.started":"2025-06-03T16:22:39.457794Z","shell.execute_reply":"2025-06-03T16:29:05.682421Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# CELL 14: Stage 4 - Pooling Comparison & Selection\nprint(\"\\n--- Cell 14: Stage 4 - Pooling Comparison & Selection ---\")\n\nimport os\nimport json\nimport numpy as np # For np.nan if needed, pd.isna used in plot function\n\n# --- Configuration for this Stage ---\n# These global variables should be defined from previous cells:\n# METRICS_DIR (str): Path to evaluation_metrics_{key}.json files.\n# PLOTS_DIR (str): Path to save plots.\n# TARGET_METRIC (str): Primary metric for selection (e.g., 'f1_opt').\n# results (dict): Global dictionary with all experiment results (configs, paths, etc.).\n# current_best_config_key (str): Key of the best model from the PREVIOUS stage (e.g., winner of Imbalance Handling).\n\n# The baseline for THIS Pooling stage is the winner from the previous stage.\npooling_stage_baseline_key = current_best_config_key\n\n# IMPORTANT: Define the list of exact keys for THIS Pooling stage.\n# This list should include the pooling_stage_baseline_key AND all its pooling variants.\n# These names MUST match the {key} part of your \"evaluation_metrics_{key}.json\" filenames.\n# Example: If pooling_stage_baseline_key = \"Baseline_StdAug_CLAHE1.0_OverSample\"\nkeys_for_pooling_stage = [\n    pooling_stage_baseline_key,  # e.g., \"Baseline_StdAug_CLAHE1.0_OverSample\" (represents default/current pooling)\n    f\"{pooling_stage_baseline_key}_PoolMAX\",\n    f\"{pooling_stage_baseline_key}_PoolAVG\",    # If you ran a specific 'PoolAVG' variant\n    f\"{pooling_stage_baseline_key}_PoolHYBRID\"\n    # Add other specific pooling strategy experiment keys you ran based on this baseline.\n    # If your pooling experiments ALSO include attention, add those keys here,\n    # e.g., f\"{pooling_stage_baseline_key}_PoolHYBRID_AttnCBAM\"\n    # For now, this example focuses purely on comparing pooling types.\n]\n# Remove duplicates if the baseline itself represents one of the pooling types implicitly\nkeys_for_pooling_stage = list(dict.fromkeys(keys_for_pooling_stage))\n\n\n# --- Pre-computation & Sanity Checks ---\nprint(f\"DEBUG: Baseline for this Pooling Stage (current_best_config_key entering Cell 14): '{pooling_stage_baseline_key}'\")\nprint(f\"DEBUG: TARGET_METRIC for selection is: '{TARGET_METRIC}'\")\nprint(f\"DEBUG: METRICS_DIR is: '{METRICS_DIR}'\")\nprint(f\"DEBUG: Intended keys for Pooling Stage (before file check): {keys_for_pooling_stage}\")\n\n# Filter these keys to only include those for which a metrics JSON file actually exists\nvalid_keys_for_pooling_comparison = [\n    key for key in keys_for_pooling_stage\n    if os.path.exists(os.path.join(METRICS_DIR, f\"evaluation_metrics_{key}.json\"))\n]\nprint(f\"Found metric files for and will compare these Pooling Stage configurations: {valid_keys_for_pooling_comparison}\")\n\n\nif not pooling_stage_baseline_key or pooling_stage_baseline_key not in valid_keys_for_pooling_comparison:\n    print(f\"ERROR: Baseline for Pooling Stage ('{pooling_stage_baseline_key}') metric file not found or key is None. \"\n          \"Skipping Pooling comparison.\")\nelif len(valid_keys_for_pooling_comparison) <= 1:\n    print(\"Not enough successful Pooling runs (including this stage's baseline) to perform a meaningful comparison.\")\n    print(f\"Keeping previous best configuration: '{current_best_config_key}'\")\nelse:\n    # --- Plotting ---\n    metrics_to_request_for_plot = [\n        'f1_opt', 'accuracy_opt', 'precision_opt', 'recall_opt', 'roc_auc_proba', 'pr_auc'\n    ]\n    if TARGET_METRIC not in metrics_to_request_for_plot:\n        metrics_to_request_for_plot.insert(0, TARGET_METRIC)\n    metrics_to_request_for_plot = list(dict.fromkeys(metrics_to_request_for_plot))\n\n    df_pooling_comparison = plot_comparison_bars_enhanced(\n        config_keys_to_plot=valid_keys_for_pooling_comparison,\n        metrics_dir=METRICS_DIR,\n        title=f\"Pooling Strategy Stage Comparison (Base: {pooling_stage_baseline_key})\",\n        save_dir=PLOTS_DIR,\n        metrics_to_display=metrics_to_request_for_plot\n    )\n\n    # --- Selection Logic (based on JSON files) ---\n    best_score_this_stage = -1.0\n    winner_key_this_stage = None \n    winner_metrics_this_stage = None\n\n    baseline_score_for_this_stage_ref = -1.0\n    baseline_ref_json_path = os.path.join(METRICS_DIR, f\"evaluation_metrics_{pooling_stage_baseline_key}.json\")\n    try:\n        with open(baseline_ref_json_path, 'r') as f:\n            baseline_ref_metrics_data = json.load(f)\n        baseline_score_for_this_stage_ref = baseline_ref_metrics_data.get(TARGET_METRIC, -1.0)\n    except Exception as e:\n        print(f\"Warning: Error loading metrics for this stage's baseline '{pooling_stage_baseline_key}': {e}\")\n\n    print(f\"\\nSelecting best configuration from Pooling stage using '{TARGET_METRIC}' \"\n          f\"(Score of '{pooling_stage_baseline_key}' for reference: {baseline_score_for_this_stage_ref:.4f}):\")\n\n    for config_key in valid_keys_for_pooling_comparison:\n        json_path = os.path.join(METRICS_DIR, f\"evaluation_metrics_{config_key}.json\")\n        current_score_from_json = -1.0\n        current_metrics_from_json = None\n        try:\n            with open(json_path, 'r') as f:\n                current_metrics_from_json = json.load(f)\n            current_score_from_json = current_metrics_from_json.get(TARGET_METRIC, -1.0)\n            metric_display_value = f\"{current_score_from_json:.4f}\" if current_score_from_json != -1.0 or TARGET_METRIC in current_metrics_from_json else \"Not Found\"\n            print(f\"  - Config '{config_key}': {TARGET_METRIC} = {metric_display_value}\")\n            if current_score_from_json > best_score_this_stage: # Assumes higher is better\n                best_score_this_stage = current_score_from_json\n                winner_key_this_stage = config_key\n                winner_metrics_this_stage = current_metrics_from_json\n        except Exception as e:\n            print(f\"  - Config '{config_key}': Error loading/processing metrics from JSON - {e}. Skipping.\")\n            continue\n            \n    # --- Announce Winner of this Stage and Update the GLOBAL current_best_config_key ---\n    if winner_key_this_stage and best_score_this_stage > -1.0: \n        print(f\"\\n🏆 Winner of Pooling Stage: '{winner_key_this_stage}' ({TARGET_METRIC}: {best_score_this_stage:.4f})\")\n        # The old code said: \"This configuration will be used as the baseline for the next stage (Attention).\"\n        # You might have a separate Attention stage, or combine Pooling & Attention. Adjust message as needed.\n        print(\"This configuration will be updated as the new 'current_best_config_key'.\")\n        current_best_config_key = winner_key_this_stage \n\n        if winner_key_this_stage in results and 'config' in results[winner_key_this_stage]:\n            print(\"\\nWinning Configuration Details (from in-memory 'results'):\")\n            winner_config_dict = results[winner_key_this_stage]['config']\n            winning_pooling_type = winner_config_dict.get('model_args', {}).get('pooling', 'N/A')\n            print(f\"  (Winning Pooling Type from config: {winning_pooling_type})\")\n            for detail_key, detail_value in winner_config_dict.items():\n                if isinstance(detail_value, dict):\n                    print(f\"  {detail_key}:\")\n                    for sub_key, sub_value in detail_value.items():\n                        if sub_key == 'augment_layer': print(f\"    {sub_key}: <Keras Layer Object>\")\n                        else: print(f\"    {sub_key}: {sub_value}\")\n                else: print(f\"  {detail_key}: {detail_value}\")\n        else:\n            print(f\"Full configuration details for winner '{winner_key_this_stage}' not found in 'results'.\")\n\n        if winner_metrics_this_stage:\n            print(\"\\nWinning Metrics (from JSON):\")\n            for m_key, m_val in winner_metrics_this_stage.items():\n                if isinstance(m_val, (float, np.floating)): print(f\"  {m_key}: {m_val:.4f}\")\n                else: print(f\"  {m_key}: {m_val}\")\n    else:\n        print(\"\\nCould not determine a new winner for the Pooling stage.\")\n        if pooling_stage_baseline_key in valid_keys_for_pooling_comparison and \\\n           baseline_score_for_this_stage_ref >= best_score_this_stage and \\\n           baseline_score_for_this_stage_ref > -1.0:\n            print(f\"The configuration '{pooling_stage_baseline_key}' (Score: {baseline_score_for_this_stage_ref:.4f}) remains the best.\")\n            current_best_config_key = pooling_stage_baseline_key\n        else:\n            print(f\"The overall best configuration '{current_best_config_key}' remains unchanged.\")\nprint(\"-\" * 70)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-03T16:32:10.669037Z","iopub.execute_input":"2025-06-03T16:32:10.669348Z","iopub.status.idle":"2025-06-03T16:32:12.294284Z","shell.execute_reply.started":"2025-06-03T16:32:10.669315Z","shell.execute_reply":"2025-06-03T16:32:12.293491Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# LEARNING_RATE=0.0002\n# print(LEARNING_RATE)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-03T16:32:33.249801Z","iopub.execute_input":"2025-06-03T16:32:33.250091Z","iopub.status.idle":"2025-06-03T16:32:33.255131Z","shell.execute_reply.started":"2025-06-03T16:32:33.25006Z","shell.execute_reply":"2025-06-03T16:32:33.254187Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# CELL 15: Stage 5 - Attention Mechanism Experiments\nprint(\"\\n--- Cell 15: Stage 5 - Attention Mechanism Experiments ---\")\n\n# The baseline for this stage is the best configuration identified after Cell 14\nattention_baseline_key = current_best_config_key\nattention_stage_keys = [] # Keep track of keys for comparison in this stage\n\nif attention_baseline_key is None or attention_baseline_key not in results:\n    print(\"ERROR: Cannot proceed with Attention stage. Baseline configuration key is missing or invalid.\")\n    # Optionally raise error: raise ValueError(\"Baseline configuration for Attention stage is missing.\")\nelse:\n    print(f\"Using configuration '{attention_baseline_key}' as baseline for Attention stage (No Attention).\")\n    # Add baseline key (representing No Attention) to the list for comparison\n    attention_stage_keys.append(attention_baseline_key)\n\n    # Retrieve the configuration dictionary of the baseline\n    baseline_config_attention = results[attention_baseline_key]['config']\n    # Verify baseline has no attention\n    baseline_attention_type = baseline_config_attention['model_args'].get('attention', None)\n    if baseline_attention_type is not None:\n         print(f\"WARNING: Baseline config '{attention_baseline_key}' for Attention stage unexpectedly has attention type '{baseline_attention_type}'.\")\n\n    attention_types_to_test = ['self', 'channel', 'spatial', 'cbam']\n\n    for test_attention_type in attention_types_to_test:\n        print(f\"\\n--- Running Attention Experiment: {test_attention_type.upper()} ---\")\n        # Construct key by appending attention type to the baseline key for this stage\n        config_attn_key = f\"{attention_baseline_key}_Attn{test_attention_type.upper()}\"\n\n        config_attn = {\n            'key': config_attn_key,\n            'parse_args': baseline_config_attention['parse_args'].copy(),\n            'model_args': baseline_config_attention['model_args'].copy(),\n            'train_args': baseline_config_attention['train_args'].copy(),\n            'oversample_train': baseline_config_attention['oversample_train']\n        }\n\n        # Modify only the attention setting in model_args\n        config_attn['model_args']['attention'] = test_attention_type\n\n        # Run the experiment\n        attn_metrics = run_experiment(\n        config_attn,\n        )\n        if attn_metrics: # Add key only if run succeeded\n            attention_stage_keys.append(config_attn_key)\n\n    print(\"\\nCompleted Attention experiments.\")\n\n\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-03T16:33:05.874842Z","iopub.execute_input":"2025-06-03T16:33:05.875127Z","iopub.status.idle":"2025-06-03T16:43:09.718182Z","shell.execute_reply.started":"2025-06-03T16:33:05.875098Z","shell.execute_reply":"2025-06-03T16:43:09.71729Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# CELL 16: Stage 5 - Attention Comparison & Selection (Best Overall Pre-Finetuning)\nprint(\"\\n--- Cell 16: Stage 5 - Attention Comparison & Selection (Best Overall Pre-Finetuning) ---\")\n\nimport os\nimport json\nimport numpy as np # For np.nan if needed, pd.isna used in plot function\n\n# --- Configuration for this Stage ---\n# These global variables should be defined from previous cells:\n# METRICS_DIR, PLOTS_DIR, TARGET_METRIC, results, current_best_config_key\n\n# The baseline for THIS Attention stage is the winner from the previous (Pooling) stage.\nattention_stage_baseline_key = current_best_config_key\n\n# IMPORTANT: Define the list of exact keys for THIS Attention stage.\n# This list should include the attention_stage_baseline_key (representing no new/specific attention or a default)\n# AND all its attention variants. These names MUST match your \"evaluation_metrics_{key}.json\" filenames.\n# Example: If attention_stage_baseline_key = \"Baseline_StdAug_CLAHE1.0_PoolHYBRID\"\nkeys_for_attention_stage = [\n    attention_stage_baseline_key,  # This is the model configuration before adding specific attention mechanisms for this stage\n    f\"{attention_stage_baseline_key}_AttnCBAM\",\n    f\"{attention_stage_baseline_key}_AttnCHANNEL\",\n    f\"{attention_stage_baseline_key}_AttnSPATIAL\",\n    # Add other specific attention mechanism experiment keys you ran based on this baseline.\n    # Adjust if your naming convention is different (e.g., if attention is part of the pooling key like _PoolHYBRID_AttnCBAM)\n    # In that case, your list might be more like the pooling stage list but varying the Attn part.\n    # Given your file list, if pooling_stage_baseline_key was \"Baseline_StdAug_CLAHE1.0_PoolHYBRID\", then:\n    # keys_for_attention_stage = [\n    #    \"Baseline_StdAug_CLAHE1.0_PoolHYBRID\", # No specific additional attention\n    #    \"Baseline_StdAug_CLAHE1.0_PoolHYBRID_AttnCBAM\",\n    #    \"Baseline_StdAug_CLAHE1.0_PoolHYBRID_AttnCHANNEL\",\n    #    \"Baseline_StdAug_CLAHE1.0_PoolHYBRID_AttnSPATIAL\",\n    # ]\n]\n# Remove duplicates if the baseline itself represents one of the attention types implicitly\nkeys_for_attention_stage = list(dict.fromkeys(keys_for_attention_stage))\n\n# This variable will store the ultimate winner before any fine-tuning.\nbest_overall_pre_finetune_key = None # Initialize\n\n# --- Pre-computation & Sanity Checks ---\nprint(f\"DEBUG: Baseline for this Attention Stage (current_best_config_key entering Cell 16): '{attention_stage_baseline_key}'\")\nprint(f\"DEBUG: TARGET_METRIC for selection is: '{TARGET_METRIC}'\")\nprint(f\"DEBUG: METRICS_DIR is: '{METRICS_DIR}'\")\nprint(f\"DEBUG: Intended keys for Attention Stage (before file check): {keys_for_attention_stage}\")\n\n# Filter these keys to only include those for which a metrics JSON file actually exists\nvalid_keys_for_attention_comparison = [\n    key for key in keys_for_attention_stage\n    if os.path.exists(os.path.join(METRICS_DIR, f\"evaluation_metrics_{key}.json\"))\n]\nprint(f\"Found metric files for and will compare these Attention Stage configurations: {valid_keys_for_attention_comparison}\")\n\n\nif not attention_stage_baseline_key or attention_stage_baseline_key not in valid_keys_for_attention_comparison:\n    print(f\"ERROR: Baseline for Attention Stage ('{attention_stage_baseline_key}') metric file not found or key is None. \"\n          \"Skipping Attention comparison.\")\n    best_overall_pre_finetune_key = current_best_config_key # Fallback to previous stage's winner\n    print(f\"Best overall pre-finetune key defaults to: '{best_overall_pre_finetune_key}'\")\nelif len(valid_keys_for_attention_comparison) <= 1:\n    print(\"Not enough successful Attention runs (including this stage's baseline/No Attention) to perform a meaningful comparison.\")\n    best_overall_pre_finetune_key = current_best_config_key # The baseline for this stage is effectively the winner\n    print(f\"Best overall pre-finetune key set to: '{best_overall_pre_finetune_key}' (baseline of this stage).\")\nelse:\n    # --- Plotting ---\n    metrics_to_request_for_plot = [\n        'f1_opt', 'accuracy_opt', 'precision_opt', 'recall_opt', 'roc_auc_proba', 'pr_auc'\n    ]\n    if TARGET_METRIC not in metrics_to_request_for_plot:\n        metrics_to_request_for_plot.insert(0, TARGET_METRIC)\n    metrics_to_request_for_plot = list(dict.fromkeys(metrics_to_request_for_plot))\n\n    df_attention_comparison = plot_comparison_bars_enhanced(\n        config_keys_to_plot=valid_keys_for_attention_comparison,\n        metrics_dir=METRICS_DIR,\n        title=f\"Attention Mechanism Stage Comparison (Base: {attention_stage_baseline_key})\",\n        save_dir=PLOTS_DIR,\n        metrics_to_display=metrics_to_request_for_plot\n    )\n\n    # --- Selection Logic (based on JSON files) ---\n    best_score_this_stage = -1.0\n    winner_key_this_stage = None \n    winner_metrics_this_stage = None\n\n    baseline_score_for_this_stage_ref = -1.0\n    baseline_ref_json_path = os.path.join(METRICS_DIR, f\"evaluation_metrics_{attention_stage_baseline_key}.json\")\n    try:\n        with open(baseline_ref_json_path, 'r') as f:\n            baseline_ref_metrics_data = json.load(f)\n        baseline_score_for_this_stage_ref = baseline_ref_metrics_data.get(TARGET_METRIC, -1.0)\n    except Exception as e:\n        print(f\"Warning: Error loading metrics for this stage's baseline '{attention_stage_baseline_key}': {e}\")\n\n    print(f\"\\nSelecting best configuration from Attention stage using '{TARGET_METRIC}' \"\n          f\"(Score of '{attention_stage_baseline_key}' (No new Attention) for reference: {baseline_score_for_this_stage_ref:.4f}):\")\n\n    for config_key in valid_keys_for_attention_comparison:\n        json_path = os.path.join(METRICS_DIR, f\"evaluation_metrics_{config_key}.json\")\n        current_score_from_json = -1.0\n        current_metrics_from_json = None\n        try:\n            with open(json_path, 'r') as f:\n                current_metrics_from_json = json.load(f)\n            current_score_from_json = current_metrics_from_json.get(TARGET_METRIC, -1.0)\n            metric_display_value = f\"{current_score_from_json:.4f}\" if current_score_from_json != -1.0 or TARGET_METRIC in current_metrics_from_json else \"Not Found\"\n            print(f\"  - Config '{config_key}': {TARGET_METRIC} = {metric_display_value}\")\n            if current_score_from_json > best_score_this_stage: # Assumes higher is better\n                best_score_this_stage = current_score_from_json\n                winner_key_this_stage = config_key\n                winner_metrics_this_stage = current_metrics_from_json\n        except Exception as e:\n            print(f\"  - Config '{config_key}': Error loading/processing metrics from JSON - {e}. Skipping.\")\n            continue\n            \n    # --- Announce Winner of this Stage and Update GLOBAL current_best_config_key ---\n    # This winner is also the best_overall_pre_finetune_key\n    if winner_key_this_stage and best_score_this_stage > -1.0: \n        print(f\"\\n🏆 Winner of Attention Stage (Best Overall Pre-Finetune): '{winner_key_this_stage}' ({TARGET_METRIC}: {best_score_this_stage:.4f})\")\n        current_best_config_key = winner_key_this_stage \n        best_overall_pre_finetune_key = current_best_config_key # Store specifically\n        print(\"This configuration will be used for the final Fine-tuning stage.\")\n\n\n        if winner_key_this_stage in results and 'config' in results[winner_key_this_stage]:\n            print(\"\\nBest Overall (Pre-Finetuning) Configuration Details (from in-memory 'results'):\")\n            winner_config_dict = results[winner_key_this_stage]['config']\n            winning_attention_type = winner_config_dict.get('model_args', {}).get('attention', 'N/A (or baseline)')\n            print(f\"  (Winning Attention Type from config: {winning_attention_type})\")\n            for detail_key, detail_value in winner_config_dict.items():\n                if isinstance(detail_value, dict): # More concise print for nested dicts\n                    # Check for 'model_args' specifically to print its content if desired\n                    if detail_key == 'model_args':\n                        print(f\"  {detail_key}:\")\n                        for sub_k, sub_v in detail_value.items(): print(f\"    {sub_k}: {sub_v}\")\n                    elif detail_key == 'parse_args' and 'augment_layer' in detail_value:\n                        print(f\"  {detail_key}:\")\n                        for pa_key, pa_value in detail_value.items():\n                             if pa_key == 'augment_layer': print(f\"    {pa_key}: <Keras Layer Object>\")\n                             else: print(f\"    {pa_key}: {pa_value}\")\n                    else:\n                        print(f\"  {detail_key}: {{...}}\") # Default concise print for other dicts\n                else: print(f\"  {detail_key}: {detail_value}\")\n        else:\n            print(f\"Full configuration details for winner '{winner_key_this_stage}' not found in 'results'.\")\n\n        if winner_metrics_this_stage:\n            print(\"\\nBest Overall (Pre-Finetuning) Metrics (from JSON):\")\n            for m_key, m_val in winner_metrics_this_stage.items():\n                if isinstance(m_val, (float, np.floating)): print(f\"  {m_key}: {m_val:.4f}\")\n                else: print(f\"  {m_key}: {m_val}\")\n    else:\n        print(\"\\nCould not determine a new winner for the Attention stage.\")\n        # If no new winner, the baseline for this stage (winner of previous stage) remains the best pre-finetune\n        best_overall_pre_finetune_key = attention_stage_baseline_key \n        # current_best_config_key also remains attention_stage_baseline_key\n        print(f\"The configuration '{attention_stage_baseline_key}' (Score: {baseline_score_for_this_stage_ref:.4f}) remains the best overall pre-finetune.\")\n\n# --- Final Sanity Check before potentially moving to a fine-tuning cell ---\nif best_overall_pre_finetune_key is None or \\\n   not os.path.exists(os.path.join(METRICS_DIR, f\"evaluation_metrics_{best_overall_pre_finetune_key}.json\")):\n    print(\"\\nCRITICAL WARNING: Could not determine a valid best overall configuration before fine-tuning, or its metrics file is missing. \"\n          \"The next Fine-tuning stage might not proceed correctly.\")\n    # Consider raising an error: raise RuntimeError(\"Failed to determine best configuration for fine-tuning.\")\nelse:\n    print(f\"\\nReady for Fine-Tuning using configuration: '{best_overall_pre_finetune_key}'\")\n\nprint(\"-\" * 70)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-03T16:45:02.408324Z","iopub.execute_input":"2025-06-03T16:45:02.408595Z","iopub.status.idle":"2025-06-03T16:45:04.183744Z","shell.execute_reply.started":"2025-06-03T16:45:02.408567Z","shell.execute_reply":"2025-06-03T16:45:04.182925Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# CELL 17: Stage 6 - Fine-Tuning Experiment\nprint(\"\\n--- Cell 17: Stage 6 - Fine-Tuning Experiment ---\")\n\nimport os\nimport time # Make sure time is imported if not already at the top\nimport numpy as np # Make sure numpy is imported\n# import tensorflow as tf # Already imported in Cell 6 usually\n# from tensorflow import keras # Already imported in Cell 6 usually\nimport gc # Make sure gc is imported\n\n# Assumed global variables:\n# best_overall_pre_finetune_key (from Cell 16)\n# results (global dictionary)\n# CHECKPOINT_DIR, PLOTS_DIR, METRICS_DIR\n# train_paths, train_labels, val_paths, val_labels, test_paths, test_labels\n# preprocess_image, IMG_SIZE, GLOBAL_BATCH_SIZE\n# strategy, custom_objects_map, inv_label_dict, label_dict\n# LEARNING_RATE_FINETUNE, EPOCHS_FINETUNE\n# PATIENCE_EARLY_STOPPING, PATIENCE_REDUCE_LR, MIN_LR\n# TARGET_METRIC (e.g., 'f1_opt')\n# train_model (function from Cell 6)\n# evaluate_model_optimized_with_viz (function from Cell 6)\n# plot_training_history_enhanced (function from Cell 6)\n# class_weights_dict (if class weights are used)\n\nfine_tuned_run_key = None # Use a different variable name to avoid confusion if cell is re-run\n\nif best_overall_pre_finetune_key is None or best_overall_pre_finetune_key not in results:\n    print(\"CRITICAL ERROR: Cannot proceed with Fine-tuning. Best pre-finetune configuration key is missing or invalid.\")\nelse:\n    print(f\"Starting Fine-tuning based on configuration: '{best_overall_pre_finetune_key}'\")\n\n    pre_ft_config_data = results[best_overall_pre_finetune_key].get('config')\n    pre_ft_checkpoint_path = results[best_overall_pre_finetune_key].get('checkpoint_path')\n\n    if not pre_ft_config_data or not pre_ft_checkpoint_path or not os.path.exists(pre_ft_checkpoint_path):\n        print(f\"ERROR: Cannot fine-tune. Missing config or checkpoint file ({pre_ft_checkpoint_path}) for the best pre-finetune model '{best_overall_pre_finetune_key}'.\")\n    else:\n        fine_tuned_run_key = f\"{best_overall_pre_finetune_key}_FineTuned\" # Construct a unique key\n        \n        # Define Fine-tuning specific parameters\n        unfreeze_from_block = 'conv5_block' # Example: Start unfreezing from conv block 5 for DenseNet\n        fine_tuning_lr = LEARNING_RATE_FINETUNE # Should be a very small LR\n        fine_tuning_epochs = EPOCHS_FINETUNE\n        fine_tune_checkpoint_filepath = os.path.join(CHECKPOINT_DIR, f\"{fine_tuned_run_key}_best.keras\")\n\n        print(f\"Fine-tuning Key: {fine_tuned_run_key}\")\n        print(f\"Loading model from: {pre_ft_checkpoint_path}\")\n        print(f\"Unfreezing from: '{unfreeze_from_block}' (or all if not found)\")\n        print(f\"Fine-tuning LR: {fine_tuning_lr}, Max Epochs: {fine_tuning_epochs}\")\n        print(f\"Best fine-tuned model will be saved to: {fine_tune_checkpoint_filepath}\")\n\n        model_ft = None\n        history_ft = None\n        eval_metrics_ft = None\n        training_duration_ft = 0\n        start_time_total_ft = time.time()\n\n        try:\n            # --- 1. Build Datasets (using config from best pre-ft run) ---\n            print(\"\\n[FT-1. Building Datasets for Fine-Tuning...]\")\n            ft_parse_args = pre_ft_config_data['parse_args'].copy()\n            ft_oversample = pre_ft_config_data.get('oversample_train', False)\n\n            train_ds_ft = build_dataset(\n                train_paths, train_labels, preprocess_image, ft_parse_args,\n                GLOBAL_BATCH_SIZE, f\"Train ({fine_tuned_run_key})\", shuffle=True,\n                augment_in_map=ft_parse_args.get('apply_augment', False),\n                oversample=ft_oversample, cache=True\n            )\n            val_ft_parse_args = ft_parse_args.copy(); val_ft_parse_args['apply_augment'] = False\n            test_ft_parse_args = ft_parse_args.copy(); test_ft_parse_args['apply_augment'] = False\n            val_ds_ft = build_dataset(val_paths, val_labels, preprocess_image, val_ft_parse_args, GLOBAL_BATCH_SIZE, f\"Val ({fine_tuned_run_key})\", shuffle=False, cache=True)\n            test_ds_ft = build_dataset(test_paths, test_labels, preprocess_image, test_ft_parse_args, GLOBAL_BATCH_SIZE, f\"Test ({fine_tuned_run_key})\", shuffle=False, cache=True)\n\n            if not all([train_ds_ft, val_ds_ft, test_ds_ft]):\n                raise RuntimeError(\"Failed to build fine-tuning datasets.\")\n            print(\"Fine-tuning datasets built successfully.\")\n\n            # --- 2. Load Best Pre-FT Model ---\n            print(\"\\n[FT-2. Loading Best Pre-Finetune Model...]\")\n            with strategy.scope():\n                model_ft = keras.models.load_model(pre_ft_checkpoint_path, custom_objects=custom_objects_map)\n            print(f\"Model '{model_ft.name}' loaded successfully.\")\n\n            # --- 3. Unfreeze Layers ---\n            print(f\"\\n[FT-3. Unfreezing Base Model Layers from '{unfreeze_from_block}'...]\")\n            base_model_to_unfreeze = None\n            for layer in model_ft.layers:\n                if isinstance(layer, keras.Model) and ('densenet' in layer.name or 'efficientnet' in layer.name or 'resnet' in layer.name): # More generic base model check\n                    base_model_to_unfreeze = layer\n                    break\n            if not base_model_to_unfreeze and len(model_ft.layers) > 1 and isinstance(model_ft.layers[1], keras.Model): # Fallback\n                base_model_to_unfreeze = model_ft.layers[1]\n                print(f\"Warning: Assuming base model is layer '{base_model_to_unfreeze.name}' based on position.\")\n            \n            if not base_model_to_unfreeze:\n                raise ValueError(\"Could not identify base model layer for unfreezing.\")\n            print(f\"Identified base model for unfreezing: '{base_model_to_unfreeze.name}'\")\n\n            base_model_to_unfreeze.trainable = True\n            unfreeze_from_index = -1\n            if unfreeze_from_block: # Only try to find specific block if name is given\n                for i, layer in enumerate(base_model_to_unfreeze.layers):\n                    if layer.name.startswith(unfreeze_from_block):\n                        unfreeze_from_index = i\n                        break\n                if unfreeze_from_index == -1:\n                    print(f\"Warning: Layer prefix '{unfreeze_from_block}' not found in base model '{base_model_to_unfreeze.name}'. Unfreezing ALL base layers.\")\n                    unfreeze_from_index = 0 \n            else: # If unfreeze_from_block is None or empty, unfreeze all\n                print(\"Unfreezing ALL layers in the base model.\")\n                unfreeze_from_index = 0\n            \n            num_frozen_in_base = 0\n            if unfreeze_from_index > 0:\n                print(f\"Freezing layers in '{base_model_to_unfreeze.name}' before index {unfreeze_from_index} ('{base_model_to_unfreeze.layers[unfreeze_from_index].name}')\")\n                for layer in base_model_to_unfreeze.layers[:unfreeze_from_index]:\n                    if layer.trainable: layer.trainable = False; num_frozen_in_base += 1\n                print(f\"Froze {num_frozen_in_base} layers in the base model.\")\n            else:\n                print(f\"All {len(base_model_to_unfreeze.layers)} layers in '{base_model_to_unfreeze.name}' will be trainable (or maintain their current trainable status if set individually).\")\n\n            trainable_count = np.sum([np.prod(w.shape) for w in model_ft.trainable_weights])\n            non_trainable_count = np.sum([np.prod(w.shape) for w in model_ft.non_trainable_weights])\n            print(f\"Total Trainable weights in full model: {trainable_count:,}\")\n            print(f\"Total Non-trainable weights in full model: {non_trainable_count:,}\")\n\n            # --- 4. Re-compile Model ---\n            print(\"\\n[FT-4. Re-compiling Model for Fine-tuning...]\")\n            with strategy.scope():\n                model_ft.compile(\n                    optimizer=keras.optimizers.Adam(learning_rate=fine_tuning_lr),\n                    loss='binary_crossentropy',\n                    metrics=[ 'accuracy', tf.keras.metrics.Precision(name='precision'),\n                              tf.keras.metrics.Recall(name='recall'), tf.keras.metrics.AUC(name='auc')]\n                )\n            print(f\"Model re-compiled with LR={fine_tuning_lr}.\")\n\n            # --- 5. Define Fine-Tuning Callbacks ---\n            print(\"\\n[FT-5. Setting up Fine-Tuning Callbacks...]\")\n            ft_callbacks_list = [\n                EarlyStopping(monitor='val_loss', patience=PATIENCE_EARLY_STOPPING + 2, verbose=1, restore_best_weights=False),\n                ReduceLROnPlateau(monitor='val_loss', factor=0.2, patience=PATIENCE_REDUCE_LR, min_lr=MIN_LR / 5, verbose=1), # Potentially even lower min_lr\n                ModelCheckpoint(filepath=fine_tune_checkpoint_filepath, monitor='val_loss', save_best_only=True, save_weights_only=False, verbose=1)\n            ]\n            print(f\"Best fine-tuned model checkpoint path: {fine_tune_checkpoint_filepath}\")\n\n            # --- 6. Train (Fine-tune) ---\n            print(\"\\n[FT-6. Starting Fine-tuning Training Phase...]\")\n            ft_class_weights_setting = pre_ft_config_data['train_args'].get('class_weights_setting')\n            ft_train_class_weights = class_weights_dict if ft_class_weights_setting == 'balanced' else None\n\n            history_ft, training_duration_ft = train_model(\n                model_ft, train_ds_ft, val_ds_ft,\n                epochs=fine_tuning_epochs, initial_epoch=0, \n                class_weights=ft_train_class_weights, strategy=strategy,\n                learning_rate=fine_tuning_lr, # LR is set during compile, but train_model might use it for logging\n                callbacks=ft_callbacks_list,\n                stage_name=f\"Fine-Tuning ({fine_tuned_run_key})\"\n            )\n            if history_ft is None: raise RuntimeError(\"Model fine-tuning training failed.\")\n\n            # --- 7. Load Best Fine-tuned Weights ---\n            print(\"\\n[FT-7. Loading Best Weights from Fine-Tuning Checkpoint...]\")\n            model_to_eval_ft = model_ft # Default to last epoch if checkpoint missing\n            if os.path.exists(fine_tune_checkpoint_filepath):\n                with strategy.scope():\n                    model_to_eval_ft = keras.models.load_model(fine_tune_checkpoint_filepath, custom_objects=custom_objects_map)\n                print(f\"Successfully loaded best fine-tuned model from {fine_tune_checkpoint_filepath}\")\n            else:\n                print(f\"WARNING: Fine-tuning checkpoint not found at {fine_tune_checkpoint_filepath}. Evaluating with last FT epoch's weights.\")\n\n            # --- 8. Evaluate Final Fine-tuned Model ---\n            print(\"\\n[FT-8. Evaluating Final Fine-tuned Model...]\")\n            eval_metrics_ft, y_true_test_ft, y_pred_proba_test_ft, _ = evaluate_model_optimized_with_viz( # CORRECTED\n                model=model_to_eval_ft, # Use the reloaded best model\n                val_ds=val_ds_ft, \n                test_ds=test_ds_ft,\n                strategy=strategy,\n                inv_label_map=inv_label_dict,\n                target_metric=TARGET_METRIC, # Ensure TARGET_METRIC is globally defined\n                dataset_name=f\"Test ({fine_tuned_run_key})\",\n                save_dir=PLOTS_DIR,\n                config_name=fine_tuned_run_key\n            )\n            if not eval_metrics_ft:\n                print(\"Warning: Fine-tuned evaluation failed or returned no metrics.\")\n                eval_metrics_ft = {}\n\n            # --- 9. Grad-CAM on Fine-tuned Model (REMOVED/COMMENTED as per previous request) ---\n            # print(\"\\n[FT-9. Generating Grad-CAM for Fine-tuned Model...]\")\n            # try:\n            #     # ... (Grad-CAM logic if you re-enable it, ensure it uses model_to_eval_ft) ...\n            #     print(\"Grad-CAM for fine-tuned model is currently disabled.\")\n            # except Exception as grad_e_ft:\n            #     print(f\"Error during fine-tuned Grad-CAM generation: {grad_e_ft}\")\n\n\n            # --- 10. Store Fine-Tuned Results ---\n            print(\"\\n[FT-10. Storing Fine-tuned Results...]\")\n            total_duration_ft = time.time() - start_time_total_ft\n            \n            # Save metrics to JSON for the fine-tuned model\n            metrics_json_save_path_ft = None\n            if eval_metrics_ft:\n                metrics_json_save_path_ft = os.path.join(METRICS_DIR, f\"evaluation_metrics_{fine_tuned_run_key}.json\")\n                try:\n                    serializable_metrics_ft = {}\n                    for m_key, m_value in eval_metrics_ft.items():\n                        if isinstance(m_value, np.generic): serializable_metrics_ft[m_key] = m_value.item()\n                        elif isinstance(m_value, np.ndarray): serializable_metrics_ft[m_key] = m_value.tolist()\n                        else: serializable_metrics_ft[m_key] = m_value\n                    with open(metrics_json_save_path_ft, 'w') as f:\n                        json.dump(serializable_metrics_ft, f, indent=4)\n                    print(f\"Fine-tuned evaluation metrics saved to {metrics_json_save_path_ft}\")\n                except Exception as e:\n                    print(f\"Error saving fine-tuned evaluation metrics to JSON: {e}\")\n                    metrics_json_save_path_ft = None\n            \n            results[fine_tuned_run_key] = {\n                'config': pre_ft_config_data, \n                'fine_tune_params': {\n                    'unfreeze_from_block': unfreeze_from_block, 'lr': fine_tuning_lr,\n                    'epochs_run': len(history_ft.epoch) if history_ft and hasattr(history_ft, 'epoch') else 0,\n                },\n                'metrics': eval_metrics_ft,\n                'training_duration_sec': training_duration_ft,\n                'total_duration_sec': total_duration_ft,\n                'checkpoint_path': fine_tune_checkpoint_filepath if os.path.exists(fine_tune_checkpoint_filepath) else None,\n                'metrics_json_path': metrics_json_save_path_ft\n            }\n            print(f\"Results for fine-tuned model '{fine_tuned_run_key}' stored.\")\n\n            # --- 11. Plotting Fine-Tuned Results ---\n            print(\"\\n[FT-11. Plotting Fine-tuned Results...]\")\n            if history_ft:\n                plot_training_history_enhanced( # CORRECTED\n                    history_ft,\n                    title_suffix=f\"({fine_tuned_run_key} - FT Phase)\",\n                    save_dir=PLOTS_DIR,\n                    config_name=fine_tuned_run_key\n                )\n            # Other plots (CM, ROC/PR) are now handled by evaluate_model_optimized_with_viz in FT-8.\n\n            print(f\"--- Fine-Tuning Experiment {fine_tuned_run_key} Complete ---\")\n            # Update current_best_config_key if fine-tuned model is better\n            # This comparison should ideally happen in the next cell (Final Summary Comparison)\n            # For now, we just record the fine_tuned_run_key.\n            # The next cell (Cell 18) will compare this fine_tuned_run_key with best_overall_pre_finetune_key.\n\n        except Exception as e:\n            print(f\"\\n\\n ****** ERROR during FINE-TUNING experiment {fine_tuned_run_key} ****** \")\n            print(f\"Error Type: {type(e).__name__}\")\n            print(f\"Error Details: {e}\")\n            import traceback # Moved import here for when it's actually needed\n            traceback.print_exc()\n            if fine_tuned_run_key: # Only store if key was generated\n                results[fine_tuned_run_key] = {'status': 'failed', 'error': str(e), 'config': pre_ft_config_data if 'pre_ft_config_data' in locals() else {}}\n            fine_tuned_run_key = None # Mark as failed by nullifying the key for later checks\n\n        # finally:\n        #     # --- 12. Fine-Tuning Cleanup ---\n        #     print(\"\\n[FT-12. Cleaning up fine-tuning resources...]\")\n        #     del model_ft, train_ds_ft, val_ds_ft, test_ds_ft, history_ft \n        #     if 'keras' in globals() or 'tensorflow.keras' in globals():\n        #          keras.backend.clear_session()\n        #     gc.collect()\n        #     print(\"Fine-tuning cleanup complete.\")\n            \n# --- Update current_best_config_key AFTER fine-tuning IF it was successful AND better ---\n# This comparison and update is typically done in the *next* cell (Cell 18 - Final Summary)\n# For now, Cell 17 just runs the fine-tuning experiment and records its result.\n# Cell 18 will compare fine_tuned_run_key with best_overall_pre_finetune_key.\n# So, current_best_config_key is NOT updated here. It's updated by the comparison cells.\n# best_overall_pre_finetune_key remains the best pre-finetuning key.\n# fine_tuned_run_key (if successful) is the key for the fine-tuned version.\n\nprint(\"-\" * 70)\nif fine_tuned_run_key and fine_tuned_run_key in results and results[fine_tuned_run_key].get('status') != 'failed':\n    print(f\"Fine-tuning experiment '{fine_tuned_run_key}' completed and results stored.\")\n    print(f\"The best model before this fine-tuning was: '{best_overall_pre_finetune_key}'\")\n    print(f\"Compare metrics of '{fine_tuned_run_key}' with '{best_overall_pre_finetune_key}' in the next cell (Cell 18) to determine the ultimate winner.\")\nelif fine_tuned_run_key and fine_tuned_run_key in results and results[fine_tuned_run_key].get('status') == 'failed':\n    print(f\"Fine-tuning experiment '{fine_tuned_run_key}' FAILED.\")\nelse:\n    print(\"Fine-tuning was not performed or key was not generated due to earlier errors.\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-03T16:52:33.41655Z","iopub.execute_input":"2025-06-03T16:52:33.416928Z","iopub.status.idle":"2025-06-03T16:56:07.180198Z","shell.execute_reply.started":"2025-06-03T16:52:33.416889Z","shell.execute_reply":"2025-06-03T16:56:07.179388Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"","metadata":{"trusted":true,"execution":{"execution_failed":"2025-06-03T13:33:32.214Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# CELL 18: Stage 7 - Final Summary Comparison\nprint(\"\\n--- Cell 18: Stage 7 - Final Summary Comparison ---\")\n\nimport os\nimport json\nimport numpy as np # For isinstance checks in JSON serialization, and pd.isna in plot func\nimport pandas as pd # For pd.isna in plot func\n\n# These global variables should be defined from previous cells:\n# METRICS_DIR (str): Path to evaluation_metrics_{key}.json files.\n# PLOTS_DIR (str): Path to save plots.\n# results (dict): Global dictionary holding all experiment results.\n# plot_comparison_bars_enhanced (function from Cell 6 that reads from JSON)\n\n# Variables holding keys from previous stages (ensure these are correctly set by Cells 8, 10, 12, 14, 16, 17)\n# Example:\n# key1 = \"Baseline_StdAug\" # From Cell 8 (or your actual baseline key)\n# imbalance_baseline_key # Winner of CLAHE, input to Imbalance stage (Cell 12)\n# pooling_baseline_key # Winner of Imbalance, input to Pooling stage (Cell 14)\n# attention_baseline_key # Winner of Pooling, input to Attention stage (Cell 16)\n# best_overall_pre_finetune_key # Winner of Attention stage (Cell 16)\n# fine_tuned_run_key # Key of the fine-tuned model from Cell 17 (use the variable holding the actual key)\n\n\nprint(\"Gathering results for final milestone comparison...\")\n# Store original keys that correspond to milestones\nmilestone_original_keys = []\n# Map descriptive labels to original keys for sorting or direct access if needed\nmilestone_label_to_original_key = {}\n\n# 1. Baseline (StdAug only)\n# Ensure 'key1' is the actual key for your initial baseline experiment.\n# If you have a variable holding this, use it. For example:\n# initial_baseline_key = \"Baseline_StdAug\" # Or whatever it was named\nkey1 = \"Baseline_StdAug\" # Assuming this is your absolute first baseline key\nmilestone_label = \"1. Baseline (Aug)\"\nif key1 and os.path.exists(os.path.join(METRICS_DIR, f\"evaluation_metrics_{key1}.json\")):\n    milestone_original_keys.append(key1)\n    milestone_label_to_original_key[milestone_label] = key1\n    print(f\"- Found Milestone 1: '{key1}' -> '{milestone_label}'\")\nelse:\n    print(f\"- WARNING: Milestone 1 key '{key1}' or its metrics JSON not found.\")\n\n# 2. Best CLAHE\n# This should be the key that won the CLAHE stage (was 'current_best_config_key' after CLAHE cell,\n# and became 'imbalance_stage_baseline_key' at the start of Imbalance cell)\nkey2 = imbalance_baseline_key # Key after CLAHE stage\nmilestone_label = \"2. +CLAHE\" if key2 and \"_CLAHE\" in key2 else \"2. Baseline (No CLAHE)\"\nif key2 and os.path.exists(os.path.join(METRICS_DIR, f\"evaluation_metrics_{key2}.json\")):\n    if key2 not in milestone_original_keys : milestone_original_keys.append(key2) # Avoid duplicates if same as key1\n    milestone_label_to_original_key[milestone_label] = key2\n    print(f\"- Found Milestone 2: '{key2}' -> '{milestone_label}'\")\nelse:\n    print(f\"- WARNING: Milestone 2 key '{key2}' or its metrics JSON not found.\")\n\n# 3. Best Imbalance Handling\nkey3 = pooling_baseline_key # Key after Imbalance stage\nmilestone_label = \"3. +Imbalance\"\nif key3:\n    if \"_ClassWeights\" in key3: milestone_label += \" (Weights)\"\n    elif \"_Oversample\" in key3: milestone_label += \" (Oversample)\"\n    # Add else if neither, assume baseline for imbalance stage didn't change or no specific imbalance won\nif key3 and os.path.exists(os.path.join(METRICS_DIR, f\"evaluation_metrics_{key3}.json\")):\n    if key3 not in milestone_original_keys : milestone_original_keys.append(key3)\n    milestone_label_to_original_key[milestone_label] = key3\n    print(f\"- Found Milestone 3: '{key3}' -> '{milestone_label}'\")\nelse:\n    print(f\"- WARNING: Milestone 3 key '{key3}' or its metrics JSON not found.\")\n\n# 4. Best Pooling\nkey4 = attention_baseline_key # Key after Pooling stage\nmilestone_label = \"4. +Pooling\"\nif key4:\n    if \"_PoolMAX\" in key4: milestone_label += \" (Max)\"\n    elif \"_PoolHYBRID\" in key4: milestone_label += \" (Hybrid)\"\n    elif \"_PoolAVG\" in key4: milestone_label += \" (Avg)\"\nif key4 and os.path.exists(os.path.join(METRICS_DIR, f\"evaluation_metrics_{key4}.json\")):\n    if key4 not in milestone_original_keys : milestone_original_keys.append(key4)\n    milestone_label_to_original_key[milestone_label] = key4\n    print(f\"- Found Milestone 4: '{key4}' -> '{milestone_label}'\")\nelse:\n    print(f\"- WARNING: Milestone 4 key '{key4}' or its metrics JSON not found.\")\n\n# 5. Best Attention (Best Overall Pre-FT)\nkey5 = best_overall_pre_finetune_key # Key after Attention stage\nmilestone_label = \"5. +Attention\"\nif key5:\n    if \"_AttnCBAM\" in key5: milestone_label += \" (CBAM)\"\n    elif \"_AttnCHANNEL\" in key5: milestone_label += \" (Channel)\"\n    elif \"_AttnSPATIAL\" in key5: milestone_label += \" (Spatial)\"\n    # Add other attention types if used\nif key5 and os.path.exists(os.path.join(METRICS_DIR, f\"evaluation_metrics_{key5}.json\")):\n    if key5 not in milestone_original_keys : milestone_original_keys.append(key5)\n    milestone_label_to_original_key[milestone_label] = key5\n    print(f\"- Found Milestone 5: '{key5}' -> '{milestone_label}'\")\nelse:\n    print(f\"- WARNING: Milestone 5 key '{key5}' or its metrics JSON not found.\")\n\n# 6. Fine-Tuned (Result from Stage 6 - Fine-tuning)\nkey6 = fine_tuned_run_key # Use the specific variable from Cell 17 that holds the fine-tuned key\nmilestone_label = \"6. Fine-Tuned\"\nif key6 and os.path.exists(os.path.join(METRICS_DIR, f\"evaluation_metrics_{key6}.json\")):\n    if key6 not in milestone_original_keys : milestone_original_keys.append(key6)\n    milestone_label_to_original_key[milestone_label] = key6\n    print(f\"- Found Milestone 6: '{key6}' -> '{milestone_label}'\")\nelse:\n    print(f\"- WARNING: Milestone 6 key '{key6}' (Fine-Tuned) or its metrics JSON not found.\")\n\n# Ensure milestone_original_keys only contains unique keys that were actually found\nmilestone_original_keys = list(dict.fromkeys(milestone_original_keys)) # Preserves order, removes duplicates\n\n# Plot Final Comparison using the original keys if data available\nif milestone_original_keys:\n    print(\"\\nPlotting Final Milestone Comparison...\")\n    \n    # Define the metrics you want to plot from the JSON files\n    metrics_to_plot_final = ['f1_opt', 'accuracy_opt', 'precision_opt', 'recall_opt', 'roc_auc_proba', 'pr_auc']\n    if TARGET_METRIC not in metrics_to_plot_final: # Ensure target metric is plotted\n        metrics_to_plot_final.insert(0, TARGET_METRIC)\n    metrics_to_plot_final = list(dict.fromkeys(metrics_to_plot_final))\n\n\n    # The plot function now expects original keys.\n    # If you want the bars to be labeled with descriptive names (like \"1. Baseline (Aug)\"),\n    # you would need to modify plot_comparison_bars_enhanced to accept a mapping for y-tick labels,\n    # or ensure the 'Configuration' column in its internal DataFrame is set to these descriptive labels.\n    # For now, it will use the original keys as labels. We can sort milestone_original_keys\n    # based on the milestone order for plotting if the map `milestone_label_to_original_key` is correctly populated.\n\n    # To sort the keys for plotting in milestone order:\n    sorted_descriptive_labels = sorted(milestone_label_to_original_key.keys())\n    ordered_original_keys_for_plot = [milestone_label_to_original_key[label] for label in sorted_descriptive_labels if milestone_label_to_original_key[label] in milestone_original_keys]\n    \n    # Make sure all keys in ordered_original_keys_for_plot are valid and unique\n    ordered_original_keys_for_plot = [k for k in ordered_original_keys_for_plot if k in milestone_original_keys]\n    ordered_original_keys_for_plot = list(dict.fromkeys(ordered_original_keys_for_plot))\n\n\n    if ordered_original_keys_for_plot:\n        plot_comparison_bars_enhanced(\n            config_keys_to_plot=ordered_original_keys_for_plot, # Pass the original keys in desired order\n            metrics_dir=METRICS_DIR,\n            title=\"Final Model Performance Milestones\",\n            save_dir=PLOTS_DIR, # Make sure PLOTS_DIR is defined\n            metrics_to_display=metrics_to_plot_final\n        )\n    else:\n        print(\"\\nNo valid, ordered keys found to generate the final comparison plot from JSONs.\")\n\nelse:\n    print(\"\\nNot enough valid milestone results found to generate the final comparison plot.\")\n\nprint(\"\\n\" + \"=\"*70)\nprint(\"                      End of Experiment Pipeline                      \")\nprint(\"=\"*70)\n\n# --- Optional: Save final results dictionary (the big 'results' dict) to JSON ---\n# This part needs a robust serializer if 'results' contains complex objects like Keras layers.\n# The existing serialization logic you had was a good start.\n# For simplicity, I'm focusing on the plotting part from JSONs.\n# The 'final_experiment_results.json' in your provided file list suggests you do save this.\n\nresults_summary_json_path = os.path.join(PLOTS_DIR, \"pipeline_summary_results.json\") # Save in PLOTS_DIR for outputs\ntry:\n    serializable_summary = {}\n    print(f\"\\nAttempting to create a serializable summary for JSON export to {results_summary_json_path}...\")\n    for milestone_label, original_key in milestone_label_to_original_key.items():\n        if original_key and os.path.exists(os.path.join(METRICS_DIR, f\"evaluation_metrics_{original_key}.json\")):\n            with open(os.path.join(METRICS_DIR, f\"evaluation_metrics_{original_key}.json\"), 'r') as f:\n                metrics = json.load(f)\n            serializable_summary[milestone_label] = {\n                'original_key': original_key,\n                'metrics': metrics\n            }\n            if original_key in results and 'config' in results[original_key]:\n                 # Attempt to serialize basic config parts\n                serializable_summary[milestone_label]['config_summary'] = {\n                    k: str(v) for k, v in results[original_key]['config'].items() if not callable(v) and not isinstance(v, keras.Model) and not isinstance(v, keras.layers.Layer)\n                }\n\n\n    if serializable_summary:\n        with open(results_summary_json_path, 'w') as f:\n            json.dump(serializable_summary, f, indent=4)\n        print(f\"Final milestone metrics summary saved to: {results_summary_json_path}\")\n    else:\n        print(\"No milestone data to save in summary JSON.\")\n\nexcept Exception as json_e:\n    print(f\"\\nWarning: Could not save final milestone summary to JSON. Error: {json_e}\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-03T16:58:01.904402Z","iopub.execute_input":"2025-06-03T16:58:01.904724Z","iopub.status.idle":"2025-06-03T16:58:03.333888Z","shell.execute_reply.started":"2025-06-03T16:58:01.904692Z","shell.execute_reply":"2025-06-03T16:58:03.332971Z"}},"outputs":[],"execution_count":null}]}