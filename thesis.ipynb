{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.7.10","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[{"sourceId":23812,"sourceType":"datasetVersion","datasetId":17810}],"dockerImageVersionId":30146,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# CELL 1: Setup, Imports, and Configuration\nprint(\"--- Cell 1: Setup, Imports, and Configuration ---\")\n\n# Core Libraries\nimport os\nimport numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nimport cv2\nimport random\nimport glob\nfrom math import ceil\nimport gc\nimport inspect\nfrom functools import partial\nimport json\nimport traceback\nimport time  # For timing experiments\n\n# Scikit-learn\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.metrics import (\n    classification_report, confusion_matrix, roc_auc_score, roc_curve,\n    accuracy_score, precision_score, recall_score, f1_score, precision_recall_curve\n)\nfrom sklearn.utils import class_weight\n\n# TensorFlow / Keras\nimport tensorflow as tf\nfrom tensorflow import keras\nfrom tensorflow.keras import layers, models, applications\nfrom tensorflow.keras.preprocessing.image import load_img, img_to_array\nfrom tensorflow.keras.layers import (\n    Dense, Dropout, BatchNormalization, GlobalAveragePooling2D, GlobalMaxPooling2D,\n    Input, Conv2D, Add, Multiply, Activation, Concatenate, Reshape, Layer, Softmax\n)\nfrom tensorflow.keras.layers.experimental import preprocessing as keras_preprocessing\nfrom tensorflow.keras.callbacks import EarlyStopping, ReduceLROnPlateau, ModelCheckpoint\nfrom tensorflow.keras.mixed_precision import Policy, set_global_policy\nimport tensorflow_addons as tfa\n# print(f\"TensorFlow Version: {tf.__version__}\")\n# print(f\"Keras Version: {keras.__version__}\")\n\n# --- Configuration ---\nSEED = 143\nIMG_SIZE = 224 # Using a single dimension for square images\nIMG_SHAPE = (IMG_SIZE, IMG_SIZE, 3)\nBATCH_SIZE_PER_REPLICA = 32 # Adjust based on GPU memory\nEPOCHS_HEAD = 50       # Epochs for initial training (head only)\nEPOCHS_FINETUNE = 40     # Epochs for fine-tuning\nLEARNING_RATE = .0001\nLEARNING_RATE_FINETUNE = .00001\nDROPOUT_RATE = 0.35\nPATIENCE_EARLY_STOPPING = 10\nPATIENCE_REDUCE_LR = 5\nMIN_LR = 1e-7\nTARGET_METRIC = 'f1_opt' \n\n# Experiment Tracking - Simple dictionary for results\nresults = {}\n# Directory for saving model checkpoints\nCHECKPOINT_DIR = \"/kaggle/working/checkpoints\"\nGRADCAM_DIR = \"/kaggle/working/gradcam_outputs\"\nMETRICS_DIR = \"/kaggle/working/saved\"\nPLOTS_DIR = \"/kaggle/working/plots\"\n# Create directories if they don't exist\nos.makedirs(CHECKPOINT_DIR, exist_ok=True)\nos.makedirs(GRADCAM_DIR, exist_ok=True)\nos.makedirs(METRICS_DIR, exist_ok=True)\nos.makedirs(PLOTS_DIR, exist_ok=True)\n\n\nprint(f\"Checkpoint directory: {CHECKPOINT_DIR}\")\nprint(f\"Grad-CAM output directory: {GRADCAM_DIR}\")\n\n# --- Hardware Setup ---\n# Mixed Precision (Optional but recommended for speed/memory on compatible GPUs)\ntry:\n    policy = Policy('mixed_float16')\n    set_global_policy(policy)\n    print('Mixed precision enabled: Compute dtype=%s, Variable dtype=%s' % (\n          policy.compute_dtype, policy.variable_dtype))\nexcept Exception as e:\n    print(f\"Could not enable mixed precision: {e}\")\n\n# GPU Configuration\ngpus = tf.config.list_physical_devices('GPU')\nif gpus:\n    try:\n        for gpu in gpus:\n            tf.config.experimental.set_memory_growth(gpu, True)\n        print(f\"Found {len(gpus)} GPUs. Memory growth enabled.\")\n        # Set up distribution strategy\n        if len(gpus) > 1:\n            strategy = tf.distribute.MirroredStrategy()\n            print(f\"Using MirroredStrategy with {strategy.num_replicas_in_sync} devices.\")\n        else:\n            strategy = tf.distribute.get_strategy() # Default strategy for single GPU\n            print(\"Using default strategy for single GPU.\")\n    except RuntimeError as e:\n        print(f\"GPU setup error: {e}. Falling back to default strategy.\")\n        strategy = tf.distribute.get_strategy()\nelse:\n    print(\"No GPUs found. Using default strategy (CPU).\")\n    strategy = tf.distribute.get_strategy()\n\n# Calculate Global Batch Size\nGLOBAL_BATCH_SIZE = BATCH_SIZE_PER_REPLICA * strategy.num_replicas_in_sync\nprint(f\"Global Batch Size: {GLOBAL_BATCH_SIZE}\")\n\n# AUTOTUNE for tf.data pipelines\nAUTOTUNE = tf.data.AUTOTUNE\n\n# --- Reproducibility ---\n# os.environ['TF_DETERMINISTIC_OPS'] = '1' # Commented out: Caused UnimplementedError on GPU\n# os.environ['TF_CUDNN_DETERMINISTIC'] = '1' # Also comment out or set to '0' if needed\nprint(\"Note: TF_DETERMINISTIC_OPS disabled for GPU compatibility. Minor non-determinism may occur.\")\nrandom.seed(SEED)\nnp.random.seed(SEED)\ntf.random.set_seed(SEED)\n# Set Python hash seed (for certain operations)\nos.environ['PYTHONHASHSEED'] = str(SEED)\n\nprint(\"--- Setup Complete ---\")\n\n\n","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","_kg_hide-input":true,"trusted":true,"execution":{"iopub.status.busy":"2025-06-05T08:53:25.211164Z","iopub.execute_input":"2025-06-05T08:53:25.211570Z","iopub.status.idle":"2025-06-05T08:53:25.233110Z","shell.execute_reply.started":"2025-06-05T08:53:25.211520Z","shell.execute_reply":"2025-06-05T08:53:25.232381Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# CELL 2: Data Loading and Path Definition\nprint(\"\\n--- Cell 2: Data Loading and Path Definition ---\")\n\n# --- Dataset Paths ---\n# Adjust BASE_PATH if your dataset is located elsewhere\ntry:\n    if os.path.exists(\"/kaggle/input/chest-xray-pneumonia/chest_xray/\"):\n        BASE_PATH = \"/kaggle/input/chest-xray-pneumonia/chest_xray/\"\n        print(\"Using Kaggle dataset path.\")\n    else:\n        # Example for local structure - MODIFY AS NEEDED\n        local_path = \"./chest_xray/\"\n        if os.path.exists(local_path):\n             BASE_PATH = local_path\n             print(f\"Using local dataset path: {BASE_PATH}\")\n        else:\n             raise FileNotFoundError(\"Dataset base path not found locally or on Kaggle.\")\n\n    TRAIN_PATH = os.path.join(BASE_PATH, \"train\")\n    VAL_PATH = os.path.join(BASE_PATH, \"val\")\n    TEST_PATH = os.path.join(BASE_PATH, \"test\")\n\n    # Basic check for subdirectories\n    for p in [TRAIN_PATH, VAL_PATH, TEST_PATH]:\n        if not os.path.exists(p):\n            print(f\"WARNING: Dataset directory does not exist: {p}\")\n        elif not os.listdir(p):\n             print(f\"WARNING: Dataset directory is empty: {p}\")\n\nexcept FileNotFoundError as e:\n    print(f\"ERROR: {e}\")\n    print(\"Please ensure the dataset is available and BASE_PATH is set correctly.\")\n    # Optionally, raise the error again to stop execution\n    # raise\n\n# --- Load All Image Paths and Labels ---\ndef load_image_paths_and_labels(base_dir, label_map=None):\n    \"\"\"Loads image paths and numeric labels from subdirectories.\"\"\"\n    paths = []\n    labels = []\n    new_label_map = {}\n    is_new_map = False\n    if label_map is None:\n        label_map = {}\n        is_new_map = True\n\n    print(f\"Loading data from: {base_dir}\")\n    try:\n        categories = sorted([d for d in os.listdir(base_dir) if os.path.isdir(os.path.join(base_dir, d))])\n        if not categories:\n            print(f\"ERROR: No category subdirectories found in {base_dir}\")\n            return [], [], {}\n\n        for i, category in enumerate(categories):\n            if is_new_map:\n                label_map[category.upper()] = i # Use uppercase for consistency\n            label = label_map.get(category.upper())\n            if label is None:\n                 print(f\"Warning: Category '{category}' not found in provided label map. Skipping.\")\n                 continue\n\n            category_path = os.path.join(base_dir, category)\n            image_files = []\n            # Common image extensions (consider adding .png if needed)\n            for ext in ['*.jpeg', '*.jpg', '*.png']:\n                image_files.extend(glob.glob(os.path.join(category_path, ext)))\n\n            print(f\"  Found {len(image_files)} images for '{category}' (Label: {label})\")\n            paths.extend(image_files)\n            labels.extend([label] * len(image_files))\n\n    except FileNotFoundError:\n        print(f\"ERROR: Directory not found: {base_dir}\")\n    except Exception as e:\n        print(f\"ERROR loading from {base_dir}: {e}\")\n\n    if not paths:\n        print(f\"WARNING: No images found in {base_dir}\")\n\n    return paths, labels, label_map if is_new_map else {}\n\n\n# Load paths and create label map from the training directory initially\nprint(\"Loading initial paths from TRAIN directory to define labels...\")\ntry:\n    train_paths_orig, train_labels_orig, label_dict = load_image_paths_and_labels(TRAIN_PATH)\n    if not label_dict:\n        raise ValueError(\"Could not determine label mapping from training data.\")\n\n    print(\"\\nLoading paths from VAL directory...\")\n    val_paths_orig, val_labels_orig, _ = load_image_paths_and_labels(VAL_PATH, label_dict)\n    print(\"\\nLoading paths from TEST directory...\")\n    test_paths_orig, test_labels_orig, _ = load_image_paths_and_labels(TEST_PATH, label_dict)\n\n    # Create inverse mapping for display purposes\n    inv_label_dict = {v: k for k, v in label_dict.items()}\n    print(f\"\\nLabel Mapping: {label_dict}\")\n    print(f\"Inverse Label Mapping: {inv_label_dict}\")\n\n    # Combine all paths and labels\n    all_paths = train_paths_orig + val_paths_orig + test_paths_orig\n    all_labels = train_labels_orig + val_labels_orig + test_labels_orig\n    print(f\"\\nTotal images found across all sets: {len(all_paths)}\")\n\n    if len(all_paths) == 0:\n        raise ValueError(\"No images loaded. Check dataset paths and structure.\")\n    if len(all_paths) != len(all_labels):\n         raise ValueError(\"Mismatch between loaded image paths and labels count.\")\n\n    # Clean up original lists to save memory\n    del train_paths_orig, train_labels_orig, val_paths_orig, val_labels_orig\n    del test_paths_orig, test_labels_orig\n    gc.collect()\n\nexcept Exception as e:\n    print(f\"ERROR during data loading: {e}\")\n    # Consider raising the error to halt if loading is critical\n    # raise\n\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-05T08:53:25.234773Z","iopub.execute_input":"2025-06-05T08:53:25.235022Z","iopub.status.idle":"2025-06-05T08:53:25.451812Z","shell.execute_reply.started":"2025-06-05T08:53:25.234988Z","shell.execute_reply":"2025-06-05T08:53:25.451047Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# --- NEW CELL X.1: Overall Dataset Distribution (PDF Output) ---\nprint(\"\\n--- Cell X.1: Overall Dataset Distribution ---\")\n\nif 'all_labels' in locals() and 'inv_label_dict' in locals() and all_labels is not None and inv_label_dict is not None:\n    plt.figure(figsize=(8, 6)) # Adjust figsize as needed for PDF layout\n    class_names_overall = [inv_label_dict[label] for label in sorted(inv_label_dict.keys())]\n    \n    # Calculate counts and percentages\n    counts_series_overall = pd.Series(all_labels).map(inv_label_dict).value_counts().reindex(class_names_overall, fill_value=0)\n    percentages_overall = (counts_series_overall / len(all_labels)) * 100\n    \n    ax = sns.barplot(x=counts_series_overall.index, y=percentages_overall.values, palette=\"viridis\", order=class_names_overall)\n    plt.title('Overall Dataset Class Distribution', fontsize=16)\n    plt.ylabel('Percentage (%)', fontsize=12)\n    plt.xlabel('Class', fontsize=12)\n    plt.ylim(0, 105) # Set y-limit to 105 to give space for labels\n    \n    # Add labels on top of bars\n    if ax.containers:\n        try:\n            # Create labels with both percentage and count\n            labels_for_bars = [f'{p:.1f}%\\n({c})' for p, c in zip(percentages_overall, counts_series_overall)]\n            ax.bar_label(ax.containers[0], labels=labels_for_bars, padding=3, fontsize=10)\n        except IndexError:\n            pass \n        except Exception as e:\n            print(f\"Error creating bar labels: {e}\")\n\n\n    plt.grid(axis='y', linestyle='--', alpha=0.7)\n    plt.tight_layout() # Adjust layout to prevent clipping in PDF\n    \n    # Save the plot as a high-quality PDF\n    overall_dist_plot_path_pdf = os.path.join(PLOTS_DIR, \"overall_dataset_distribution.pdf\")\n    try:\n        plt.savefig(overall_dist_plot_path_pdf, format='pdf', dpi=300, bbox_inches='tight')\n        print(f\"Overall distribution plot saved as PDF: {overall_dist_plot_path_pdf}\")\n    except Exception as e:\n        print(f\"Error saving overall distribution plot as PDF: {e}\")\n        # Fallback to PNG if PDF fails for some reason\n        overall_dist_plot_path_png = os.path.join(PLOTS_DIR, \"overall_dataset_distribution.png\")\n        plt.savefig(overall_dist_plot_path_png, dpi=300, bbox_inches='tight')\n        print(f\"Overall distribution plot saved as PNG instead: {overall_dist_plot_path_png}\")\n\n    plt.show()\n    \nelse:\n    print(\"ERROR: 'all_labels' or 'inv_label_dict' not available. Cannot plot overall distribution. Check Cell 2 output.\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-05T08:53:25.453424Z","iopub.execute_input":"2025-06-05T08:53:25.453791Z","iopub.status.idle":"2025-06-05T08:53:25.857584Z","shell.execute_reply.started":"2025-06-05T08:53:25.453763Z","shell.execute_reply":"2025-06-05T08:53:25.856852Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# CELL 3: Stratified Splitting, Class Weights, and Visualization\nprint(\"\\n--- Cell 3: Stratified Splitting, Class Weights, and Visualization ---\")\n\nif 'all_paths' not in locals() or not all_paths:\n    print(\"ERROR: 'all_paths' not available. Cannot perform splitting. Check Cell 2.\")\nelse:\n    print(f\"Performing 70/10/20 stratified split on {len(all_paths)} images...\") # Updated print message\n\n    # --- Perform Stratified Split ---\n    # Ensure labels are numpy array for stratification\n    all_labels_np = np.array(all_labels)\n\n    # Split 1: Separate Test set (20%)\n    try:\n        train_val_paths, test_paths, train_val_labels, test_labels = train_test_split(\n            all_paths, all_labels_np,\n            test_size=0.20,  # 20% for test (remains the same)\n            random_state=SEED,\n            stratify=all_labels_np,\n            shuffle=True\n        )\n\n        # Split 2: Separate Validation set from the remaining Train/Val (target 10% of original total)\n        # Calculate validation size relative to the train_val set\n        original_total = len(all_paths)\n        # --- MODIFIED LINE HERE ---\n        val_target_size = 0.10  # Target 10% of original total for validation\n        # --------------------------\n        test_actual_size_fraction = len(test_paths) / original_total # Corrected variable name for clarity\n        # Relative size: val_target / (1 - test_actual_size_fraction)\n        # e.g., 0.10 / (1.0 - 0.20) = 0.10 / 0.80 = 0.125\n        val_relative_size = val_target_size / (1.0 - test_actual_size_fraction)\n\n        if val_relative_size >= 1.0 or val_relative_size <= 0:\n            print(f\"Warning: Calculated relative validation size ({val_relative_size:.3f}) is invalid. Setting validation set to empty.\")\n            train_paths, val_paths, train_labels, val_labels = train_val_paths, [], train_val_labels, []\n        else:\n            train_paths, val_paths, train_labels, val_labels = train_test_split(\n                train_val_paths, train_val_labels,\n                test_size=val_relative_size, # This will take 12.5% of the train_val_paths (which is 80% of total)\n                                             # resulting in 10% of original total for validation\n                random_state=SEED,\n                stratify=train_val_labels,\n                shuffle=True\n            )\n\n        print(\"\\nSplit complete:\")\n        print(f\"  Train Set:      {len(train_paths):>6} images ({len(train_paths)/original_total:7.1%})\")\n        print(f\"  Validation Set: {len(val_paths):>6} images ({len(val_paths)/original_total:7.1%})\")\n        print(f\"  Test Set:       {len(test_paths):>6} images ({len(test_paths)/original_total:7.1%})\")\n        print(f\"  Total Verified: {len(train_paths) + len(val_paths) + len(test_paths):>6} images\")\n\n        # Convert labels back to lists (optional, but consistent with input)\n        train_labels = list(train_labels)\n        val_labels = list(val_labels)\n        test_labels = list(test_labels)\n\n    except ValueError as e:\n        print(f\"ERROR during train/test split: {e}\")\n        print(\"This might happen if a class has only 1 sample. Check dataset balance.\")\n        # Consider stopping execution if split fails\n        raise\n    except Exception as e:\n        print(f\"An unexpected error occurred during splitting: {e}\")\n        raise\n\n    # Clean up intermediate variables\n    del all_paths, all_labels, all_labels_np, train_val_paths, train_val_labels\n    gc.collect()\n\n    # --- Calculate Class Weights (using the new train_labels) ---\n    print(\"\\nCalculating Class Weights for Training Set...\")\n    unique_classes, class_counts = np.unique(train_labels, return_counts=True)\n\n    if len(unique_classes) < 2:\n        print(\"WARNING: Only one class found in the training set. Class weights set to None.\")\n        class_weights_dict = None\n    else:\n        # Calculate balanced weights\n        total_samples_train = len(train_labels) # Corrected variable name\n        num_unique_classes = len(unique_classes) # Corrected variable name\n        weights = total_samples_train / (num_unique_classes * class_counts)\n        class_weights_dict = dict(zip(unique_classes, weights))\n        print(\"Calculated Class Weights:\")\n        for cls, weight in class_weights_dict.items():\n            print(f\"  Class {cls} ({inv_label_dict.get(cls, 'Unknown')}): {weight:.4f}\") # Added .get for safety\n\n    # --- Visualize Split Distributions ---\n    def plot_split_distributions(split_data, label_map_inv):\n        \"\"\"Generates bar plots showing class counts and percentages within each split.\"\"\"\n        num_splits = len(split_data)\n        if num_splits == 0: return\n\n        fig, axes = plt.subplots(1, num_splits, figsize=(6 * num_splits, 5), sharey=False, constrained_layout=True) # Added constrained_layout\n        if num_splits == 1: axes = [axes] # Ensure iterable\n\n        fig.suptitle(f'Dataset Split Class Distribution', fontsize=16) # y adjusted by constrained_layout\n        class_names_plot = sorted(list(label_map_inv.values())) # Corrected variable name\n        palette = sns.color_palette('viridis', n_colors=len(class_names_plot)) # Corrected variable name\n\n        for i, (name, (paths_list, labels_list)) in enumerate(split_data.items()): # Corrected variable names\n            ax = axes[i]\n            count_items = len(labels_list) # Corrected variable name\n            ax.set_title(f\"{name} Set ({count_items} images)\")\n\n            if count_items > 0:\n                counts_series = pd.Series(labels_list).map(label_map_inv).value_counts().reindex(class_names_plot, fill_value=0)\n                percentages = (counts_series / count_items) * 100\n\n                sns.barplot(x=counts_series.index, y=percentages.values, ax=ax, palette=palette, order=class_names_plot)\n                ax.set_ylabel(\"Percentage (%)\" if i == 0 else \"\")\n                ax.set_ylim(0, 105)\n                ax.tick_params(axis='x', rotation=0)\n\n                if ax.containers:\n                    try:\n                        ax.bar_label(ax.containers[0], fmt='%.1f%%', padding=3, fontsize=9)\n                    except IndexError: pass \n            else:\n                ax.text(0.5, 0.5, 'No Data', ha='center', va='center', transform=ax.transAxes)\n                ax.set_ylim(0, 105)\n            ax.grid(axis='y', linestyle='--', alpha=0.7)\n\n        # plt.tight_layout(rect=[0, 0, 1, 0.97]) # Removed as constrained_layout is used\n        plt.show()\n\n    split_summary_data = {\n        \"Train\": (train_paths, train_labels),\n        \"Validation\": (val_paths, val_labels),\n        \"Test\": (test_paths, test_labels)\n    }\n    # Ensure inv_label_dict is defined before this call\n    if 'inv_label_dict' in locals() or 'inv_label_dict' in globals():\n        plot_split_distributions(split_summary_data, inv_label_dict)\n    else:\n        print(\"Warning: 'inv_label_dict' not defined. Cannot plot split distributions.\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-05T08:53:25.859029Z","iopub.execute_input":"2025-06-05T08:53:25.859285Z","iopub.status.idle":"2025-06-05T08:53:26.528420Z","shell.execute_reply.started":"2025-06-05T08:53:25.859250Z","shell.execute_reply":"2025-06-05T08:53:26.527753Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import matplotlib.pyplot as plt\nimport matplotlib.patches as patches\nimport cv2\nimport numpy as np\nimport os\nimport random\nfrom typing import List, Dict, Tuple, Optional\n\n# Set beautiful default styling\nplt.style.use('default')\nplt.rcParams.update({\n    'font.family': 'serif',\n    'font.serif': ['Times New Roman', 'serif'],\n    'font.size': 11,\n    'axes.linewidth': 0.8,\n    'axes.spines.top': False,\n    'axes.spines.right': False,\n    'figure.facecolor': 'white'\n})\n\ndef create_beautiful_xray_comparison(\n    image_paths: List[str],\n    image_labels: List[int],\n    label_map: Dict[str, int],\n    output_dir: Optional[str] = None,\n    samples_per_class: int = 6,\n    figsize: Tuple[float, float] = (16, 10)\n) -> None:\n    \"\"\"\n    Creates a stunning comparison visualization of chest X-ray samples.\n\n    Features:\n    - Modern, clean design with beautiful typography\n    - Elegant spacing and professional color scheme\n    - High-quality output perfect for thesis inclusion\n    - Sophisticated visual hierarchy\n    \"\"\"\n\n    # Validate inputs\n    normal_label = label_map.get(\"NORMAL\")\n    pneumonia_label = label_map.get(\"PNEUMONIA\")\n\n    if normal_label is None or pneumonia_label is None:\n        raise KeyError(f\"Classes not found. Available: {list(label_map.keys())}\")\n\n    # Sample images\n    normal_imgs = [p for i, p in enumerate(image_paths) if image_labels[i] == normal_label]\n    pneumonia_imgs = [p for i, p in enumerate(image_paths) if image_labels[i] == pneumonia_label]\n\n    sample_normal = random.sample(normal_imgs, min(samples_per_class, len(normal_imgs)))\n    sample_pneumonia = random.sample(pneumonia_imgs, min(samples_per_class, len(pneumonia_imgs)))\n\n    # Create figure with beautiful design\n    fig = plt.figure(figsize=figsize, facecolor='white')\n\n    # Add main title with elegant typography\n    # Adjusted y parameter for less space, e.g., y=0.96 or 0.97\n    fig.suptitle('Sample Images From Dataset',\n                 fontsize=24, fontweight='200', color='#2c3e50', y=0.96) # MODIFIED: y value changed\n\n    # Create grid layout - 2 rows, samples_per_class columns\n    rows, cols = 2, samples_per_class\n\n    # Beautiful color scheme - changed label colors to a neutral dark gray\n    text_label_color = '#34495e' # MODIFIED: Neutral color for text labels\n    normal_placeholder_color = '#27ae60'  # Professional green for placeholder border/text if needed\n    pneumonia_placeholder_color = '#e74c3c'  # Professional red for placeholder border/text if needed\n\n\n    for i in range(samples_per_class):\n        # Normal samples (top row)\n        ax1 = plt.subplot(rows, cols, i + 1)\n        if i < len(sample_normal):\n            _display_beautiful_image(ax1, sample_normal[i])\n        else:\n            _create_elegant_placeholder(ax1, \"Normal\", normal_placeholder_color)\n\n        # Add class label for first image\n        if i == 0:\n            ax1.text(-0.1, 0.5, 'NORMAL', rotation=90, fontsize=18,\n                     fontweight='bold', color=text_label_color, ha='center', va='center', # MODIFIED: color\n                     transform=ax1.transAxes)\n\n        # Pneumonia samples (bottom row)\n        ax2 = plt.subplot(rows, cols, i + cols + 1)\n        if i < len(sample_pneumonia):\n            _display_beautiful_image(ax2, sample_pneumonia[i])\n        else:\n            _create_elegant_placeholder(ax2, \"Pneumonia\", pneumonia_placeholder_color)\n\n        # Add class label for first image\n        if i == 0:\n            ax2.text(-0.1, 0.5, 'PNEUMONIA', rotation=90, fontsize=18,\n                     fontweight='bold', color=text_label_color, ha='center', va='center', # MODIFIED: color\n                     transform=ax2.transAxes)\n\n    # Add subtle separating line\n    # line_y = 0.5 # REMOVED\n    # line = plt.Line2D([0.1, 0.9], [line_y, line_y], # REMOVED\n    # transform=fig.transFigure, # REMOVED\n    # color='#bdc3c7', linestyle='-', linewidth=2, alpha=0.7) # REMOVED\n    # fig.add_artist(line) # REMOVED\n\n    # Elegant layout adjustment\n    # Adjusted rect to potentially reduce top space further if suptitle y wasn't enough\n    plt.tight_layout(rect=[0.08, 0.05, 0.98, 0.92]) # MODIFIED: rect top value changed\n    plt.subplots_adjust(hspace=0.15, wspace=0.05)\n\n    # Save with high quality\n    if output_dir:\n        _save_beautiful_figure(fig, output_dir)\n\n    plt.show()\n\n\ndef _display_beautiful_image(ax: plt.Axes, image_path: str) -> None:\n    \"\"\"Display X-ray image with beautiful styling.\"\"\"\n    try:\n        # Load and process image\n        img = cv2.imread(image_path)\n        if img is None:\n            raise ValueError(f\"Cannot load: {image_path}\")\n\n        # Resize to square for consistency\n        img = cv2.resize(img, (300, 300))\n        img_rgb = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n\n        # Display with beautiful frame\n        ax.imshow(img_rgb, cmap='bone', aspect='equal')\n        ax.set_xticks([])\n        ax.set_yticks([])\n\n        # Add subtle border\n        for spine in ax.spines.values():\n            spine.set_color('#ecf0f1')\n            spine.set_linewidth(2)\n\n    except Exception as e:\n        print(f\"Error loading {image_path}: {e}\")\n        _create_elegant_placeholder(ax, \"Error\", '#95a5a6')\n\n\ndef _create_elegant_placeholder(ax: plt.Axes, text: str, color: str) -> None:\n    \"\"\"Create beautiful placeholder for missing images.\"\"\"\n    ax.set_xlim(0, 1)\n    ax.set_ylim(0, 1)\n\n    # Create gradient background\n    gradient = np.linspace(0.9, 1.0, 100).reshape(1, -1)\n    ax.imshow(gradient, extent=[0, 1, 0, 1], cmap='gray', alpha=0.3)\n\n    # Add elegant text\n    ax.text(0.5, 0.5, f'{text}\\nSample\\nUnavailable',\n            ha='center', va='center', fontsize=12,\n            color=color, fontweight='300', alpha=0.8)\n\n    ax.set_xticks([])\n    ax.set_yticks([])\n\n    # Subtle border\n    for spine in ax.spines.values():\n        spine.set_color(color) # Uses the passed color, which could be green/red for placeholders\n        spine.set_linewidth(1.5)\n        spine.set_alpha(0.5)\n\n\ndef _save_beautiful_figure(fig: plt.Figure, output_dir: str) -> None:\n    \"\"\"Save figure with publication quality.\"\"\"\n    os.makedirs(output_dir, exist_ok=True)\n\n    # Save as high-quality PDF (best for thesis)\n    pdf_path = os.path.join(output_dir, \"xray_comparison_thesis.pdf\")\n    try:\n        fig.savefig(pdf_path, format='pdf', dpi=300, bbox_inches='tight',\n                    facecolor='white', edgecolor='none',\n                    metadata={'Title': 'X-Ray Dataset Comparison'})\n        print(f\"✨ Beautiful PDF saved: {pdf_path}\")\n    except Exception:\n        # High-quality PNG fallback\n        png_path = os.path.join(output_dir, \"xray_comparison_thesis.png\")\n        fig.savefig(png_path, format='png', dpi=300, bbox_inches='tight',\n                    facecolor='white', edgecolor='none')\n        print(f\"✨ High-quality PNG saved: {png_path}\")\n\n\n# Enhanced usage function\ndef generate_thesis_visualization():\n    \"\"\"Generate the beautiful X-ray comparison for thesis.\"\"\"\n    # --- Mock data for testing if run directly without external variables ---\n    # This part is for demonstration.\n    # In your actual use, ensure train_paths, train_labels, label_dict, and PLOTS_DIR are defined.\n    global train_paths, train_labels, label_dict, PLOTS_DIR\n    if 'train_paths' not in globals():\n        print(\"⚠️ Mock data is being used as global variables are not set.\")\n        # Create dummy image files for testing\n        os.makedirs(\"dummy_images/normal\", exist_ok=True)\n        os.makedirs(\"dummy_images/pneumonia\", exist_ok=True)\n        train_paths = []\n        train_labels = []\n        label_dict = {\"NORMAL\": 0, \"PNEUMONIA\": 1}\n        PLOTS_DIR = \"plots_output\"\n\n        for i in range(10):\n            # Create a simple black image for normal\n            img_n = np.zeros((100, 100, 3), dtype=np.uint8)\n            cv2.putText(img_n, f\"N{i+1}\", (10, 50), cv2.FONT_HERSHEY_SIMPLEX, 1, (255, 255, 255), 2)\n            path_n = f\"dummy_images/normal/normal_{i}.png\"\n            cv2.imwrite(path_n, img_n)\n            train_paths.append(path_n)\n            train_labels.append(label_dict[\"NORMAL\"])\n\n            # Create a simple gray image for pneumonia\n            img_p = np.full((100, 100, 3), 128, dtype=np.uint8)\n            cv2.putText(img_p, f\"P{i+1}\", (10, 50), cv2.FONT_HERSHEY_SIMPLEX, 1, (0,0,0), 2)\n            path_p = f\"dummy_images/pneumonia/pneumonia_{i}.png\"\n            cv2.imwrite(path_p, img_p)\n            train_paths.append(path_p)\n            train_labels.append(label_dict[\"PNEUMONIA\"])\n    # --- End of mock data ---\n\n\n    required_vars = [\"train_paths\", \"train_labels\", \"label_dict\", \"PLOTS_DIR\"]\n    missing = [var for var in required_vars if var not in globals()]\n\n    if not missing:\n        print(\"🎨 Creating beautiful X-ray visualization...\")\n\n        create_beautiful_xray_comparison(\n            image_paths=train_paths,\n            image_labels=train_labels,\n            label_map=label_dict,\n            output_dir=PLOTS_DIR,\n            samples_per_class=6,  # 6 samples looks cleaner\n            figsize=(16, 8) # MODIFIED: Reduced height slightly as line is removed\n        )\n\n        print(\"✅ Stunning visualization ready for your thesis!\")\n    else:\n        print(f\"❌ Missing variables: {', '.join(missing)}\")\n\n\n# Execute if run as main\nif __name__ == \"__main__\":\n    generate_thesis_visualization()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-05T09:27:18.442119Z","iopub.execute_input":"2025-06-05T09:27:18.442419Z","iopub.status.idle":"2025-06-05T09:27:21.730262Z","shell.execute_reply.started":"2025-06-05T09:27:18.442390Z","shell.execute_reply":"2025-06-05T09:27:21.729574Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# CELL 4: Augmentation Layer and Preprocessing Functions\nprint(\"\\n--- Cell 4: Augmentation Layer and Preprocessing Functions ---\")\n\n# --- Standard Augmentation Layer ---\n# Using Keras preprocessing layers for GPU acceleration\nstandard_augmentation = tf.keras.Sequential([\n    layers.Input(shape=IMG_SHAPE),\n    keras_preprocessing.RandomFlip(\"horizontal\", seed=SEED),\n    keras_preprocessing.RandomRotation(0.1, seed=SEED), # Slight rotation\n    keras_preprocessing.RandomZoom(height_factor=0.1, width_factor=0.1, seed=SEED), # Slight zoom\n    # keras_preprocessing.RandomContrast(0.1, seed=SEED), # Optional: slight contrast change\n    # keras_preprocessing.RandomBrightness(0.1, seed=SEED), # Optional: slight brightness change\n], name='standard_augmentation')\n\n# --- Image Decoding ---\n@tf.function\ndef decode_image(image_bytes):\n    \"\"\"Decodes JPEG/PNG, converts to float32, ensures 1 channel.\"\"\"\n    # Try JPEG first, then PNG\n    img = tf.io.decode_image(image_bytes, channels=1, expand_animations=False) # Ensure single channel\n    img = tf.cast(img, tf.float32)\n    return img\n\n# --- CLAHE Application (using tf.py_function) ---\n# --- CLAHE Application (using tf.py_function) ---\ndef apply_cv_clahe_np(image_np, clip_limit, grid_size): # grid_size might arrive as tensor tuple\n    \"\"\"Applies CLAHE using OpenCV to a NumPy array, handling potential tensor inputs.\"\"\"\n    # Input image is expected float32, convert to uint8 for OpenCV\n    if image_np.shape[-1] != 1: # Ensure single channel\n        print(\"Warning: Image for CLAHE is not single channel, attempting grayscale conversion.\")\n        if len(image_np.shape) == 3 and image_np.shape[-1] == 3:\n             image_uint8 = cv2.cvtColor(image_np.astype(np.uint8), cv2.COLOR_RGB2GRAY)\n        else:\n             print(\"Error: Cannot convert image to grayscale for CLAHE.\")\n             return image_np.astype(np.float32) # Return original as float\n    else:\n        # Squeeze channel dim if present, ensure uint8\n        image_uint8 = np.squeeze(image_np).astype(np.uint8)\n\n    # --- ADD EXPLICIT CONVERSION FOR grid_size ---\n    try:\n        # Check if grid_size items are TF Tensors (have .numpy()) and convert, otherwise assume Python type\n        tile_h = int(grid_size[0].numpy()) if hasattr(grid_size[0], 'numpy') else int(grid_size[0])\n        tile_w = int(grid_size[1].numpy()) if hasattr(grid_size[1], 'numpy') else int(grid_size[1])\n        cv2_grid_size = (tile_h, tile_w)\n        # print(f\"Debug: Converted grid_size to {cv2_grid_size}\") # Optional debug print\n    except Exception as e:\n        # Fallback to default if conversion fails for any reason\n        print(f\"Warning: Could not parse grid_size ({grid_size}). Using default (8, 8). Error: {e}\")\n        cv2_grid_size = (8, 8)\n    # --- END CONVERSION ---\n\n    try:\n        # Use the converted tuple of Python ints\n        clahe = cv2.createCLAHE(clipLimit=float(clip_limit), tileGridSize=cv2_grid_size)\n        clahe_img = clahe.apply(image_uint8)\n        # Add channel dimension back and cast to float32 for TF\n        processed_image = np.expand_dims(clahe_img, axis=-1).astype(np.float32)\n    except Exception as cv_e:\n         print(f\"Error during cv2.createCLAHE or apply: {cv_e}\")\n         # Return original image (uint8 converted back to float32 with channel) if CLAHE fails\n         processed_image = np.expand_dims(image_uint8, axis=-1).astype(np.float32)\n\n    return processed_image\ndef tf_apply_clahe(image, clip_limit, grid_size=(8, 8)):\n    \"\"\"TensorFlow wrapper for applying CLAHE using py_function.\"\"\"\n    # Input image is expected to be float32, shape [H, W, 1]\n    # Need float32 output from py_func\n    processed_image = tf.py_function(\n        func=apply_cv_clahe_np,\n        inp=[image, clip_limit, grid_size], # Pass clip_limit and grid_size\n        Tout=tf.float32\n    )\n    # Ensure shape is set after py_function\n    processed_image.set_shape([None, None, 1]) # Keep channel dim\n    return processed_image\n\n# --- Unified Preprocessing Function ---\n@tf.function\ndef preprocess_image(image_path, label, img_size=IMG_SIZE, apply_augment=False, augment_layer=None,\n                     apply_clahe=False, clahe_clip_limit=2.0, clahe_grid_size=(8, 8)):\n    \"\"\"\n    Loads, decodes, optionally applies CLAHE, resizes, optionally applies augmentation,\n    and preprocesses the image for DenseNet121.\n    \"\"\"\n    image_bytes = tf.io.read_file(image_path)\n    image = decode_image(image_bytes) # Decodes to grayscale float32 [H, W, 1]\n\n    # 1. Apply CLAHE (if enabled) - BEFORE resizing for better effect\n    if apply_clahe:\n        image = tf_apply_clahe(image, clip_limit=clahe_clip_limit, grid_size=clahe_grid_size)\n\n    # 2. Resize\n    image = tf.image.resize(image, [img_size, img_size], method=tf.image.ResizeMethod.BILINEAR) # Use bilinear for float images\n\n    # 3. Convert Grayscale to RGB (Required by DenseNet)\n    image = tf.image.grayscale_to_rgb(image) # Converts [H, W, 1] to [H, W, 3]\n\n    # 4. Apply Augmentation (if enabled) - AFTER resizing and RGB conversion\n    if apply_augment and augment_layer is not None:\n        # Keras layers expect batch dimension\n        image = tf.expand_dims(image, axis=0)\n        image = augment_layer(image, training=True) # Apply augmentation\n        image = tf.squeeze(image, axis=0) # Remove batch dimension\n        image = tf.cast(image, tf.float32) # Ensure float32 after augmentation\n\n    # 5. Preprocess for DenseNet121\n    image = tf.keras.applications.densenet.preprocess_input(image)\n\n    return image, label\n\n# --- Dataset Building Function ---\ndef build_dataset(paths, labels, preprocess_fn_base, preprocess_args,\n                  batch_size, dataset_name=\"Dataset\",\n                  shuffle=False, augment_in_map=False, oversample=False,\n                  cache=True):\n    \"\"\"\n    Builds a tf.data.Dataset with preprocessing, optional shuffling,\n    optional oversampling, batching, and prefetching.\n\n    Args:\n        paths (list): List of image file paths.\n        labels (list): List of corresponding labels.\n        preprocess_fn_base (function): The base preprocessing function (e.g., preprocess_image).\n        preprocess_args (dict): Dictionary of arguments for the preprocessing function.\n        batch_size (int): Global batch size.\n        dataset_name (str): Name for printing messages.\n        shuffle (bool): Whether to shuffle the dataset (typically True for train).\n        augment_in_map (bool): Whether to apply augmentation within the map function.\n                               (Passed via preprocess_args['apply_augment']).\n        oversample (bool): Whether to oversample the minority class (typically True for train).\n        cache (bool or str): Whether to cache the dataset (True for memory, file path for disk).\n    \"\"\"\n    if not paths:\n        print(f\"WARNING [{dataset_name}]: Empty paths list provided. Returning None.\")\n        return None\n    if len(paths) != len(labels):\n        print(f\"ERROR [{dataset_name}]: Mismatch paths ({len(paths)}) vs labels ({len(labels)}).\")\n        return None\n\n    AUTO = tf.data.AUTOTUNE\n    num_classes = len(np.unique(labels))\n\n    # Create the partial function for mapping BEFORE creating the dataset slices\n    # This ensures all necessary arguments are bound.\n    map_fn = partial(preprocess_fn_base, **preprocess_args)\n\n    ds = tf.data.Dataset.from_tensor_slices((paths, labels))\n\n    # Apply mapping function (preprocessing)\n    ds = ds.map(map_fn, num_parallel_calls=AUTO)\n\n    # Apply caching (after mapping, before repeating/shuffling/sampling)\n    if cache:\n        if isinstance(cache, str): # Disk caching\n            safe_suffix = \"\".join(c if c.isalnum() else \"_\" for c in dataset_name)\n            cache_file = os.path.join(CHECKPOINT_DIR, f\"tf_cache_{safe_suffix}\") # Use checkpoint dir\n            ds = ds.cache(cache_file)\n            print(f\"[{dataset_name}] Caching to disk: {cache_file}\")\n        else: # Memory caching\n            ds = ds.cache()\n            print(f\"[{dataset_name}] Caching to memory.\")\n\n    # --- Oversampling (if enabled, typically only for training set) ---\n    if oversample and num_classes > 1 and shuffle: # Only makes sense for training\n        print(f\"[{dataset_name}] Applying oversampling...\")\n        unique_cls, _ = np.unique(labels, return_counts=True)\n        datasets_by_class = []\n        # Filter dataset for each class\n        for cls_index in unique_cls:\n            datasets_by_class.append(ds.filter(lambda img, lbl: lbl == cls_index))\n\n        # Define desired distribution (equal probability for each class)\n        target_dist = [1.0 / num_classes] * num_classes\n\n        # Use sample_from_datasets for resampling\n        # Note: This samples indefinitely, so take() is needed if used without repeat()\n        # Since we usually repeat() for training, this should be fine.\n        ds = tf.data.experimental.sample_from_datasets(\n            datasets_by_class, weights=target_dist, seed=SEED\n        )\n        print(f\"[{dataset_name}] Oversampling applied.\")\n\n    # Apply shuffling (if enabled, typically for training)\n    if shuffle:\n        buffer_size = min(len(paths), 5000) # Adjust buffer size based on dataset size/memory\n        ds = ds.shuffle(buffer_size=buffer_size, seed=SEED, reshuffle_each_iteration=True)\n        print(f\"[{dataset_name}] Shuffling applied (buffer={buffer_size}).\")\n\n    # Apply batching\n    ds = ds.batch(batch_size)\n\n    # Apply prefetching\n    ds = ds.prefetch(buffer_size=AUTO)\n\n    # Apply distribution options\n    options = tf.data.Options()\n    options.experimental_distribute.auto_shard_policy = tf.data.experimental.AutoShardPolicy.DATA\n    ds = ds.with_options(options)\n\n    args_str = \", \".join(f\"{k}={v}\" for k,v in preprocess_args.items())\n    print(f\"-> [{dataset_name}] Built: items={len(paths)}, shuffle={shuffle}, oversample={oversample}, map_args=({args_str})\")\n    return ds\n\n\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-03T15:32:49.544178Z","iopub.execute_input":"2025-06-03T15:32:49.544844Z","iopub.status.idle":"2025-06-03T15:32:49.77526Z","shell.execute_reply.started":"2025-06-03T15:32:49.544799Z","shell.execute_reply":"2025-06-03T15:32:49.774357Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# CELL 5: Model Building Functions (DenseNet121, Attention Layers)\nprint(\"\\n--- Cell 5: Model Building Functions ---\")\n\n# --- Attention Layer Implementations ---\n# Using the provided implementations, ensuring they are Layers\nclass SelfAttention(Layer):\n    def __init__(self, units=64, **kwargs):\n        super().__init__(**kwargs)\n        self.units = units\n        self.query_layer = Dense(units, name='query')\n        self.key_layer = Dense(units, name='key')\n        self.value_layer = Dense(None, name='value') # Output channels inferred in build\n        self.softmax = Softmax(axis=-1)\n        self.add = Add()\n\n    def build(self, input_shape):\n        channels = input_shape[-1]\n        if channels is None:\n            raise ValueError(\"Channel dimension must be known for SelfAttention.\")\n        # Correctly set output units for value layer\n        self.value_layer.units = channels\n        super().build(input_shape)\n\n    def call(self, inputs):\n        # B = Batch size, H = Height, W = Width, C = Channels\n        input_shape = tf.shape(inputs)\n        B, H, W = input_shape[0], input_shape[1], input_shape[2]\n        C = tf.compat.dimension_value(inputs.shape[-1]) # Static preferred\n\n        flattened = Reshape((H * W, C))(inputs) # Shape: (B, H*W, C)\n\n        q = self.query_layer(flattened)  # Shape: (B, H*W, units)\n        k = self.key_layer(flattened)  # Shape: (B, H*W, units)\n        v = self.value_layer(flattened)  # Shape: (B, H*W, C)\n\n        # Attention Scores\n        scores = tf.matmul(q, k, transpose_b=True)  # Shape: (B, H*W, H*W)\n\n        # Scaling\n        dk = tf.cast(tf.shape(k)[-1], tf.float32)\n        scaled_scores = scores / tf.math.sqrt(dk)\n\n        # Weights\n        weights = self.softmax(scaled_scores) # Shape: (B, H*W, H*W)\n\n        # Weighted Values\n        attention_output = tf.matmul(weights, v)  # Shape: (B, H*W, C)\n\n        # Reshape and Residual Connection\n        output_reshaped = Reshape((H, W, C))(attention_output)\n        output = self.add([inputs, output_reshaped])\n        return output\n\n    def get_config(self):\n        config = super().get_config()\n        config.update({\"units\": self.units})\n        return config\n\nclass ChannelAttention(Layer):\n    def __init__(self, ratio=8, **kwargs):\n        super().__init__(**kwargs)\n        self.ratio = ratio\n        self.avg_pool = GlobalAveragePooling2D(keepdims=True)\n        self.max_pool = GlobalMaxPooling2D(keepdims=True)\n        # Dense layers will be built in build()\n\n    def build(self, input_shape):\n        channels = input_shape[-1]\n        if channels is None: raise ValueError(\"Channel dimension required.\")\n        self.shared_dense_1 = Dense(channels // self.ratio, activation='relu', kernel_initializer='he_normal', use_bias=True, name='ca_dense_1')\n        self.shared_dense_2 = Dense(channels, kernel_initializer='he_normal', use_bias=True, name='ca_dense_2')\n        super().build(input_shape)\n\n    def call(self, inputs):\n        avg_pooled = self.avg_pool(inputs)\n        max_pooled = self.max_pool(inputs)\n\n        avg_out = self.shared_dense_2(self.shared_dense_1(avg_pooled))\n        max_out = self.shared_dense_2(self.shared_dense_1(max_pooled))\n\n        attention = Activation('sigmoid')(Add()([avg_out, max_out]))\n        return Multiply()([inputs, attention])\n\n    def get_config(self):\n        config = super().get_config()\n        config.update({\"ratio\": self.ratio})\n        return config\n\nclass SpatialAttention(Layer):\n    def __init__(self, kernel_size=7, **kwargs):\n        super().__init__(**kwargs)\n        self.kernel_size = kernel_size\n        self.concat = Concatenate(axis=-1)\n        # Conv2D layer built in build()\n\n    def build(self, input_shape):\n         self.conv2d = Conv2D(1, kernel_size=self.kernel_size, padding='same', activation='sigmoid', kernel_initializer='he_normal', use_bias=False, name='sa_conv')\n         super().build(input_shape)\n\n    def call(self, inputs):\n        avg_pooled = tf.reduce_mean(inputs, axis=-1, keepdims=True) # Avg across channels\n        max_pooled = tf.reduce_max(inputs, axis=-1, keepdims=True)  # Max across channels\n\n        concat = self.concat([avg_pooled, max_pooled]) # Shape: (B, H, W, 2)\n        attention = self.conv2d(concat) # Shape: (B, H, W, 1)\n\n        return Multiply()([inputs, attention])\n\n    def get_config(self):\n        config = super().get_config()\n        config.update({\"kernel_size\": self.kernel_size})\n        return config\n\nclass CBAM(Layer):\n    def __init__(self, ratio=8, kernel_size=7, **kwargs):\n        super().__init__(**kwargs)\n        self.ratio = ratio\n        self.kernel_size = kernel_size\n        self.channel_attn = ChannelAttention(ratio=ratio, name='cbam_channel')\n        self.spatial_attn = SpatialAttention(kernel_size=kernel_size, name='cbam_spatial')\n\n    def call(self, inputs):\n        x = self.channel_attn(inputs)\n        x = self.spatial_attn(x)\n        return x\n\n    def get_config(self):\n        config = super().get_config()\n        config.update({\n            \"ratio\": self.ratio,\n            \"kernel_size\": self.kernel_size\n        })\n        return config\n\n# Store custom layers for potential loading later\ncustom_objects_map = {\n    'SelfAttention': SelfAttention,\n    'ChannelAttention': ChannelAttention,\n    'SpatialAttention': SpatialAttention,\n    'CBAM': CBAM,\n}\n\n\n# --- Model Creation Functions ---\ndef create_base_model(input_shape, trainable=False):\n    \"\"\"Creates the DenseNet121 base model.\"\"\"\n    base = applications.DenseNet121(weights='imagenet', include_top=False, input_shape=input_shape)\n    base.trainable = trainable\n    return base\n\ndef add_classification_head(inputs, num_classes, pooling_type='avg', attention_type=None, dropout_rate=0.3):\n    \"\"\"Adds attention (optional), pooling, and classification layers.\"\"\"\n    x = inputs\n\n    # 1. Attention (Optional)\n    if attention_type:\n        attn_name = f'attn_{attention_type}'\n        if attention_type == 'self': x = SelfAttention(units=64, name=attn_name)(x)\n        elif attention_type == 'channel': x = ChannelAttention(ratio=8, name=attn_name)(x)\n        elif attention_type == 'spatial': x = SpatialAttention(kernel_size=7, name=attn_name)(x)\n        elif attention_type == 'cbam': x = CBAM(ratio=8, kernel_size=7, name=attn_name)(x)\n        else: raise ValueError(f\"Unknown attention type: {attention_type}\")\n\n    # 2. Pooling\n    pool_name = f'pool_{pooling_type}'\n    if pooling_type == 'avg': x = GlobalAveragePooling2D(name=pool_name)(x)\n    elif pooling_type == 'max': x = GlobalMaxPooling2D(name=pool_name)(x)\n    elif pooling_type == 'hybrid':\n        avg_pool = GlobalAveragePooling2D(name='pool_hybrid_avg')(x)\n        max_pool = GlobalMaxPooling2D(name='pool_hybrid_max')(x)\n        x = Concatenate(name='pool_hybrid_concat')([avg_pool, max_pool])\n    else: raise ValueError(f\"Unknown pooling type: {pooling_type}\")\n\n    # 3. Classification Head Layers\n    x = BatchNormalization(name='head_bn_1')(x)\n    x = Dropout(dropout_rate, seed=SEED, name='head_dropout_1')(x)\n    x = Dense(128, activation='relu', name='head_dense_1', kernel_initializer='he_normal')(x)\n    x = BatchNormalization(name='head_bn_2')(x)\n    x = Dropout(dropout_rate, seed=SEED, name='head_dropout_2')(x)\n\n    # Final Output Layer\n    if num_classes == 1: # Binary classification\n        activation = 'sigmoid'\n        units = 1\n    else: # Multi-class\n        activation = 'softmax'\n        units = num_classes\n\n    outputs = Dense(units, activation=activation, name='classifier_output')(x)\n    # If using mixed precision, ensure output is float32\n    if tf.keras.mixed_precision.global_policy().compute_dtype == 'float16':\n         outputs = Activation('linear', dtype='float32')(outputs)\n\n    return outputs\n\n# def build_full_model(input_shape, num_classes, pooling='avg', attention=None, dropout=DROPOUT_RATE, base_trainable=False):\n#     \"\"\"Builds the complete model with base and head.\"\"\"\n#     inputs = Input(shape=input_shape, name='input_image')\n#     base_model = create_base_model(input_shape, trainable=base_trainable)\n#     base_output = base_model(inputs, training=base_trainable) # Control training mode\n\n#     outputs = add_classification_head(\n#         base_output,\n#         num_classes=num_classes,\n#         pooling_type=pooling,\n#         attention_type=attention,\n#         dropout_rate=dropout\n#     )\n\n#     model_name = f'DenseNet121_P-{pooling or \"none\"}_A-{attention or \"none\"}'\n#     model = keras.Model(inputs=inputs, outputs=outputs, name=model_name)\n#     return model\ndef build_full_model(input_shape, num_classes, pooling='avg', attention=None, dropout=DROPOUT_RATE, base_trainable=False):\n    \"\"\"\n    Builds the complete model with base and head.\n    THIS VERSION NOW ALWAYS INCLUDES AN INITIAL SPATIAL ATTENTION LAYER.\n    The 'attention' parameter refers to attention applied AFTER the base model.\n    Model name generation remains IDENTICAL to the original.\n    \"\"\"\n    inputs = Input(shape=input_shape, name='input_image')\n\n    # --- ALWAYS ADD INITIAL SPATIAL ATTENTION ---\n    # Using a common default kernel size of 7 for the initial SpatialAttention\n    initial_sa_kernel_size = 7\n    # Apply SpatialAttention to the input tensor\n    current_tensor = SpatialAttention(kernel_size=initial_sa_kernel_size,\n                                      name='initial_spatial_attention')(inputs) # Name this layer for clarity if inspecting model structure\n    # --- END OF INITIAL SPATIAL ATTENTION ADDITION ---\n\n    base_model = create_base_model(input_shape, trainable=base_trainable)\n    # Feed the output of the initial_spatial_attention to the base_model\n    base_output = base_model(current_tensor, training=base_trainable) # Control training mode\n\n    outputs = add_classification_head(\n        base_output,\n        num_classes=num_classes,\n        pooling_type=pooling,\n        attention_type=attention, # This 'attention' is for post-base model\n        dropout_rate=dropout\n    )\n\n    # Model name generation is IDENTICAL to your original code\n    model_name = f'DenseNet121_P-{pooling or \"none\"}_A-{attention or \"none\"}'\n    model = keras.Model(inputs=inputs, outputs=outputs, name=model_name)\n    return model\n    \nprint(\"Model building functions defined.\")\n\n\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-03T15:32:49.777194Z","iopub.execute_input":"2025-06-03T15:32:49.777422Z","iopub.status.idle":"2025-06-03T15:32:49.818955Z","shell.execute_reply.started":"2025-06-03T15:32:49.777395Z","shell.execute_reply":"2025-06-03T15:32:49.818103Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# # CELL 6: Training, Evaluation, Thresholding, Grad-CAM, and Plotting Utilities\n# print(\"\\n--- Cell 6: Training, Evaluation, Thresholding, Grad-CAM, Plotting ---\")\n\n# # --- Necessary Imports (assuming these are imported earlier or needed here) ---\n# import time\n# import numpy as np\n# import pandas as pd\n# import tensorflow as tf\n# from tensorflow import keras\n# import matplotlib.pyplot as plt\n# import seaborn as sns\n# from sklearn.metrics import (\n#     accuracy_score, precision_score, recall_score, f1_score,\n#     roc_auc_score, confusion_matrix, classification_report, roc_curve\n# )\n# import cv2\n# import os\n# import traceback # Included for potential debugging in the original code\n# from math import ceil\n\n# # --- Training Function ---\n# def train_model(model, train_ds, val_ds, epochs, class_weights, strategy, learning_rate,\n#                 initial_epoch=0, callbacks=None, stage_name=\"Training\"):\n#     \"\"\"Compiles and trains the model within the strategy scope.\"\"\"\n#     if train_ds is None:\n#         print(f\"ERROR [{stage_name}]: Training dataset is None. Cannot train.\")\n#         return None, None\n\n#     with strategy.scope():\n#         # Define metrics - ensure they handle potential division by zero gracefully\n#         metrics_list = [\n#             'accuracy',\n#             tf.keras.metrics.Precision(name='precision'), # Default threshold 0.5 during training\n#             tf.keras.metrics.Recall(name='recall'),       # Default threshold 0.5 during training\n#             tf.keras.metrics.AUC(name='auc')\n#         ]\n#         model.compile(\n#             optimizer=keras.optimizers.Adam(learning_rate=learning_rate),\n#             loss='binary_crossentropy', # Assuming binary classification\n#             metrics=metrics_list\n#         )\n#         # print(f\"[{stage_name}] Model compiled successfully with LR={learning_rate}.\")\n\n#     print(f\"\\n--- Starting {stage_name} ---\")\n#     # print(f\"Epochs: {epochs}, Initial Epoch: {initial_epoch}, LR: {learning_rate}\")\n#     print(f\"Epochs: {epochs}, Initial Epoch: {initial_epoch}\")\n#     print(f\"Class Weights: {'Applied' if class_weights else 'None'}\")\n\n#     start_time = time.time()\n#     history = model.fit(\n#         train_ds,\n#         validation_data=val_ds,\n#         epochs=epochs,\n#         initial_epoch=initial_epoch, # For resuming training/fine-tuning\n#         class_weight=class_weights,\n#         callbacks=callbacks or [],\n#         verbose=1 # Show progress bar per epoch\n#     )\n#     end_time = time.time()\n#     training_duration = end_time - start_time\n#     print(f\"--- {stage_name} Finished (Duration: {training_duration:.2f} seconds) ---\")\n\n#     # Return history and duration\n#     return history, training_duration\n\n\n# # --- Threshold Optimization ---\n# def find_optimal_threshold(y_true, y_pred_proba, target_metric='f1'):\n#     \"\"\"Finds the threshold maximizing the target metric on validation predictions.\"\"\"\n#     best_threshold = 0.5\n#     best_score = -1.0\n\n#     # Check if probabilities are valid\n#     if y_true is None or y_pred_proba is None or len(y_true) == 0 or len(y_true) != len(y_pred_proba):\n#         print(\"Warning: Invalid inputs for threshold optimization. Returning default 0.5.\")\n#         return best_threshold\n\n#     # Ensure y_true is integer type\n#     y_true = y_true.astype(int)\n\n#     # Generate potential thresholds\n#     thresholds = np.arange(0.01, 1.0, 0.01)\n\n#     scores = []\n#     for thresh in thresholds:\n#         y_pred_binary = (y_pred_proba >= thresh).astype(int)\n#         if target_metric == 'f1':\n#             score = f1_score(y_true, y_pred_binary, zero_division=0)\n#         elif target_metric == 'accuracy':\n#              score = accuracy_score(y_true, y_pred_binary)\n#         # Add other metrics like recall/precision if needed\n#         else: # Default to F1 if metric not recognized\n#             score = f1_score(y_true, y_pred_binary, zero_division=0)\n#         scores.append(score)\n\n#     if scores: # Check if scores list is not empty\n#          best_idx = np.argmax(scores)\n#          best_score = scores[best_idx]\n#          best_threshold = thresholds[best_idx]\n#          # print(f\"Optimal threshold based on validation {target_metric.upper()}: {best_threshold:.2f} (Score: {best_score:.4f})\")\n#     else:\n#         print(\"Warning: Could not compute scores for threshold optimization. Using default 0.5.\")\n\n#     return best_threshold\n\n\n# # --- Evaluation Function (Optimized - Replaced with ex2 version) ---\n# def evaluate_model_optimized(model, val_ds, test_ds, strategy, inv_label_map, target_metric='f1', dataset_name=\"Test\"):\n#     \"\"\"\n#     Evaluates the model:\n#     1. Gets compiled metrics (loss, acc, prec, rec, auc @ 0.5) using model.evaluate on the specified dataset.\n#     2. Gets predictions on the validation set to find the optimal threshold based on `target_metric`.\n#     3. Gets predictions on the specified dataset (e.g., test set).\n#     4. Calculates detailed metrics (acc, prec, rec, f1, auc) and confusion matrix on the specified dataset\n#        using the optimal threshold found from the validation set.\n\n#     Args:\n#         model (keras.Model): The trained Keras model to evaluate.\n#         val_ds (tf.data.Dataset): Validation dataset (used for threshold optimization).\n#         test_ds (tf.data.Dataset): The dataset to evaluate (e.g., Test set).\n#         strategy (tf.distribute.Strategy): Distribution strategy (used for scope if needed, though evaluate/predict often work outside).\n#         inv_label_map (dict): Inverse mapping from numeric label to class name.\n#         target_metric (str): Metric to optimize threshold ('f1', 'accuracy', etc.).\n#         dataset_name (str): Name of the dataset being evaluated (for logging).\n\n#     Returns:\n#         tuple: (results_dict, y_true_eval, y_pred_proba_eval, confusion_matrix_eval)\n#                Returns None values for preds/cm if errors occur.\n#     \"\"\"\n#     results_eval = {}\n#     y_true_val, y_pred_proba_val = None, None\n#     y_true_eval, y_pred_proba_eval = None, None # Use 'eval' suffix for the set being evaluated\n#     cm_eval = None\n#     optimal_threshold = 0.5 # Default\n\n#     print(f\"\\n--- Evaluating Model on {dataset_name} Set ---\")\n#     print(f\"  Using target metric '{target_metric}' for threshold optimization on validation set.\")\n\n#     # --- 1. Get Compiled Metrics on Evaluation Set (using model.evaluate @ 0.5 threshold) ---\n#     print(f\"Running model.evaluate() on {dataset_name} set...\")\n#     if test_ds is not None:\n#         try:\n#             # Note: model.evaluate can sometimes be slow if iterating sample by sample.\n#             eval_results = model.evaluate(test_ds, verbose=0, return_dict=True)\n#             print(\"  Compiled metrics (@ 0.5 Threshold):\")\n#             for k, v in eval_results.items():\n#                 metric_val = float(v) if isinstance(v, (np.float32, np.float64)) else v\n#                 print(f\"    {k}: {metric_val:.4f}\")\n#                 results_eval[k] = metric_val # Store loss, accuracy, precision, recall, auc\n#         except (tf.errors.NotFoundError, tf.errors.OutOfRangeError) as e: # Catch common dataset/iterator errors\n#              print(f\"  Error during model.evaluate on {dataset_name} set (possibly dataset/cache/iterator issue): {e}\")\n#              print(\"  Skipping compiled metrics evaluation.\")\n#         except ValueError as e: # Catch potential empty dataset or other value errors\n#              print(f\"  Error during model.evaluate on {dataset_name} set: {e}\")\n#              print(\"  Skipping compiled metrics evaluation.\")\n#         except Exception as e:\n#             print(f\"  Unexpected error during model.evaluate() on {dataset_name} set: {e}\")\n#             # traceback.print_exc() # Uncomment for detailed debugging\n#             print(\"  Skipping compiled metrics evaluation.\")\n#     else:\n#         print(f\"  {dataset_name} dataset not provided. Skipping compiled metrics evaluation.\")\n\n\n#     # --- 2. Get Predictions on Validation Set for Threshold Optimization ---\n#     print(f\"\\nRunning model.predict() on validation set for threshold optimization...\")\n#     if val_ds is not None:\n#         try:\n#             y_true_val_list = []\n#             # Iterate through validation dataset to get all true labels\n#             print(\"  Extracting true labels from validation set...\")\n#             for _, labels_batch in val_ds.as_numpy_iterator():\n#                 y_true_val_list.extend(labels_batch)\n#             print(f\"  Extracted {len(y_true_val_list)} validation labels.\")\n\n#             if not y_true_val_list:\n#                  print(\"  Warning: No labels extracted from validation dataset. Cannot optimize threshold.\")\n#             else:\n#                 # Predict on the validation set\n#                 print(\"  Running predictions on validation set...\")\n#                 y_pred_proba_val = model.predict(val_ds, verbose=0)\n#                 print(f\"  Generated {len(y_pred_proba_val)} validation predictions.\")\n\n\n#                 if len(y_true_val_list) == len(y_pred_proba_val):\n#                     y_true_val = np.array(y_true_val_list)\n#                     # Ensure predictions are 1D for binary classification thresholding\n#                     if y_pred_proba_val.ndim > 1 and y_pred_proba_val.shape[1] == 1:\n#                          y_pred_proba_val = y_pred_proba_val.flatten()\n\n#                     # Find optimal threshold\n#                     optimal_threshold = find_optimal_threshold(y_true_val, y_pred_proba_val, target_metric)\n#                     results_eval['optimal_threshold'] = optimal_threshold\n#                     results_eval['threshold_target_metric'] = target_metric\n#                     print(f\"  Optimal threshold determined from validation set: {optimal_threshold:.2f}\")\n#                 else:\n#                     print(f\"  Warning: Mismatch between validation labels ({len(y_true_val_list)}) and predictions ({len(y_pred_proba_val)}). Using default threshold 0.5.\")\n#                     optimal_threshold = 0.5 # Fallback\n\n#         except (tf.errors.NotFoundError, tf.errors.OutOfRangeError) as e: # Catch common dataset/iterator errors\n#              print(f\"  Error during validation predict (possibly dataset/cache/iterator issue): {e}\")\n#              print(\"  Using default threshold 0.5.\")\n#              optimal_threshold = 0.5\n#         except Exception as e:\n#             print(f\"  Error during validation prediction/threshold optimization: {e}\")\n#             # traceback.print_exc() # Uncomment for detailed debugging\n#             print(\"  Using default threshold 0.5.\")\n#             optimal_threshold = 0.5\n#     else:\n#         print(\"  Validation dataset not provided. Using default threshold 0.5 for evaluation set metrics.\")\n#         optimal_threshold = 0.5\n\n#     # Ensure optimal_threshold exists in results dict, even if defaults were used\n#     results_eval.setdefault('optimal_threshold', optimal_threshold)\n#     results_eval.setdefault('threshold_target_metric', target_metric if val_ds else 'N/A')\n\n\n#     # --- 3. Get Predictions on Evaluation Set for Detailed Metrics ---\n#     print(f\"\\nRunning model.predict() on {dataset_name} set for detailed metrics...\")\n#     if test_ds is not None:\n#         try:\n#             y_true_eval_list = []\n#             # Iterate through evaluation dataset to get all true labels\n#             print(f\"  Extracting true labels from {dataset_name} set...\")\n#             for _, labels_batch in test_ds.as_numpy_iterator():\n#                  y_true_eval_list.extend(labels_batch)\n#             print(f\"  Extracted {len(y_true_eval_list)} {dataset_name} labels.\")\n\n#             if not y_true_eval_list:\n#                  print(f\"  Warning: No labels extracted from {dataset_name} dataset. Cannot calculate detailed metrics.\")\n#             else:\n#                 # Predict on the evaluation set\n#                 print(f\"  Running predictions on {dataset_name} set...\")\n#                 y_pred_proba_eval = model.predict(test_ds, verbose=0)\n#                 print(f\"  Generated {len(y_pred_proba_eval)} {dataset_name} predictions.\")\n\n\n#                 if len(y_true_eval_list) == len(y_pred_proba_eval):\n#                     y_true_eval = np.array(y_true_eval_list)\n#                     if y_pred_proba_eval.ndim > 1 and y_pred_proba_eval.shape[1] == 1:\n#                          y_pred_proba_eval = y_pred_proba_eval.flatten() # Ensure 1D for binary\n\n#                     # Apply the optimal threshold found from validation set\n#                     y_pred_eval_optimized = (y_pred_proba_eval >= optimal_threshold).astype(int)\n\n#                     # Calculate metrics using the OPTIMIZED threshold\n#                     print(f\"\\nDetailed Metrics ({dataset_name} Set @ Optimal Threshold {optimal_threshold:.2f}):\")\n#                     try:\n#                          results_eval[f'accuracy_opt'] = accuracy_score(y_true_eval, y_pred_eval_optimized)\n#                          results_eval[f'precision_opt'] = precision_score(y_true_eval, y_pred_eval_optimized, zero_division=0)\n#                          results_eval[f'recall_opt'] = recall_score(y_true_eval, y_pred_eval_optimized, zero_division=0)\n#                          results_eval[f'f1_opt'] = f1_score(y_true_eval, y_pred_eval_optimized, zero_division=0)\n\n#                          # AUC is calculated on probabilities, doesn't depend on threshold\n#                          # Recalculate here using sklearn for consistency and if model.evaluate failed\n#                          if len(np.unique(y_true_eval)) > 1:\n#                              try:\n#                                  results_eval['roc_auc_proba'] = roc_auc_score(y_true_eval, y_pred_proba_eval)\n#                              except ValueError as auc_e:\n#                                  print(f\"  Warning: Could not calculate ROC AUC score: {auc_e}\")\n#                                  results_eval['roc_auc_proba'] = np.nan # Indicate calculation failure\n#                          else:\n#                              results_eval['roc_auc_proba'] = np.nan # AUC not defined for single class\n#                              print(\"  Warning: Only one class present in test labels, ROC AUC is undefined.\")\n\n#                          # Print calculated metrics (use .get with nan fallback)\n#                          print(f\"  Accuracy (opt):  {results_eval.get('accuracy_opt', np.nan):.4f}\")\n#                          print(f\"  Precision (opt): {results_eval.get('precision_opt', np.nan):.4f}\")\n#                          print(f\"  Recall (opt):    {results_eval.get('recall_opt', np.nan):.4f}\")\n#                          print(f\"  F1 Score (opt):  {results_eval.get('f1_opt', np.nan):.4f}\")\n#                          print(f\"  ROC AUC (proba): {results_eval.get('roc_auc_proba', np.nan):.4f}\")\n\n#                          # Confusion Matrix with optimized threshold\n#                          cm_eval = confusion_matrix(y_true_eval, y_pred_eval_optimized)\n\n#                          print(f\"\\nClassification Report ({dataset_name} Set @ Optimal Threshold {optimal_threshold:.2f}):\")\n#                          # Ensure class names match the confusion matrix dimensions\n#                          num_classes_eval = len(np.unique(y_true_eval))\n#                          if num_classes_eval == 2: # Binary case\n#                              class_names = [inv_label_map.get(0, \"Class 0\"), inv_label_map.get(1, \"Class 1\")]\n#                          else: # Fallback for multi-class (if applicable later) or single class\n#                              # Attempt to get names for present classes, ensuring order if possible\n#                              present_classes = sorted(np.unique(y_true_eval))\n#                              class_names = [inv_label_map.get(c, f\"Class {c}\") for c in present_classes]\n#                              if not class_names: # Handle case where y_true_eval was empty\n#                                 class_names = [\"Unknown\"]\n\n#                          # Ensure y_pred_eval_optimized has same unique classes or handle appropriately\n#                          # Sklearn classification_report handles labels parameter for known classes\n#                          print(classification_report(y_true_eval, y_pred_eval_optimized, target_names=class_names, labels=np.unique(y_true_eval), zero_division=0))\n\n#                     except Exception as metric_e:\n#                          print(f\"  Error calculating detailed metrics: {metric_e}\")\n#                          # traceback.print_exc() # Uncomment for detailed debugging\n\n#                 else:\n#                     print(f\"  Warning: Mismatch between {dataset_name} labels ({len(y_true_eval_list)}) and predictions ({len(y_pred_proba_eval)}). Cannot calculate detailed metrics.\")\n\n#         except (tf.errors.NotFoundError, tf.errors.OutOfRangeError) as e: # Catch common dataset/iterator errors\n#              print(f\"  Error during {dataset_name} predict (possibly dataset/cache/iterator issue): {e}\")\n#         except Exception as e:\n#             print(f\"  Error during {dataset_name} prediction or detailed metrics calculation: {e}\")\n#             # traceback.print_exc() # Uncomment for detailed debugging\n#     else:\n#          print(f\"  {dataset_name} dataset not provided. Cannot calculate detailed metrics.\")\n\n\n#     print(\"--- Evaluation Complete ---\")\n#     # Return metrics dict, true labels, predicted probabilities from the evaluation set, and its CM\n#     return results_eval, y_true_eval, y_pred_proba_eval, cm_eval\n\n\n# # --- Grad-CAM Functions ---\n# # Helper to process image for Grad-CAM input\n# def get_img_array(img_path, size):\n#     \"\"\"Loads and preprocesses image for model prediction (no DenseNet preprocessing).\"\"\"\n#     try:\n#         img = tf.keras.utils.load_img(img_path, target_size=size, color_mode='grayscale') # Load grayscale\n#         array = tf.keras.utils.img_to_array(img)\n#         array = np.expand_dims(array, axis=0) # Add batch dimension\n#         # Convert grayscale to RGB *before* passing to model if model expects 3 channels\n#         array_rgb = tf.image.grayscale_to_rgb(tf.convert_to_tensor(array)).numpy()\n#         return array_rgb\n#     except Exception as e:\n#          print(f\"Error loading image {img_path} for Grad-CAM: {e}\")\n#          return None\n\n# # Modify this function definition in Batch 1, Cell 6\n# # Note: Renamed arguments for clarity (model -> full_model, target_layer object is now passed directly)\n# def make_gradcam_heatmap(img_array_rgb, full_model, target_layer, pred_index=None):\n#     \"\"\"\n#     Generates the Grad-CAM heatmap targeting the output of a specific layer object.\n\n#     Args:\n#         img_array_rgb: Processed input image array (RGB, with batch dim).\n#         full_model: The complete Keras model.\n#         target_layer: The Keras Layer object whose output tensor to use for heatmap generation\n#                       (e.g., the base_model layer object itself).\n#         pred_index: Index of the class activation to compute gradients for (optional).\n#     \"\"\"\n#     # target_layer is now the layer OBJECT itself (e.g., base_model_for_gradcam)\n#     if not isinstance(target_layer, keras.layers.Layer):\n#          raise TypeError(f\"Expected target_layer to be a Keras Layer object, got {type(target_layer)}\")\n\n#     print(f\"--- Making Grad-CAM: Targeting output of layer '{target_layer.name}' ---\")\n\n#     # Create the gradient model. Inputs are the full model's inputs.\n#     # Outputs are the target layer's output AND the full model's final output.\n#     try:\n#         grad_model = tf.keras.models.Model(\n#             inputs=full_model.inputs, # Use inputs from the full model\n#             outputs=[target_layer.output, full_model.output] # Use the output tensor of the target layer object\n#         )\n#         print(\"   Grad-model created successfully.\")\n#     except Exception as e:\n#         print(f\"   ERROR creating grad_model: {e}\")\n#         # Provide more debug info if model creation fails\n#         print(f\"   Full model inputs: {full_model.inputs}\")\n#         print(f\"   Target layer output: {target_layer.output}\")\n#         print(f\"   Full model output: {full_model.output}\")\n#         raise e # Re-raise the error\n\n#     # Compute gradients with GradientTape\n#     with tf.GradientTape() as tape:\n#         # Preprocess input specifically for the model (assuming DenseNet preprocessing)\n#         # Check if preprocessing function exists before calling\n#         if hasattr(tf.keras.applications.densenet, 'preprocess_input'):\n#             processed_img_array = tf.keras.applications.densenet.preprocess_input(tf.cast(img_array_rgb, tf.float32))\n#         else: # Fallback if DenseNet preprocessing isn't directly available (or use model-specific)\n#             processed_img_array = tf.cast(img_array_rgb, tf.float32) / 255.0 # Simple normalization\n#             print(\"   Warning: tf.keras.applications.densenet.preprocess_input not found. Using simple normalization.\")\n\n#         # Get outputs from the grad_model\n#         target_layer_output, preds = grad_model(processed_img_array)\n\n#         # Determine the class index\n#         if pred_index is None:\n#             if preds.shape[-1] == 1: class_channel = preds[:, 0] # Binary sigmoid output\n#             else: pred_index = tf.argmax(preds[0]); class_channel = preds[:, pred_index] # Multiclass softmax\n#         else: class_channel = preds[:, pred_index] # Use provided index\n\n#     # Get gradients\n#     grads = tape.gradient(class_channel, target_layer_output)\n\n#     if grads is None:\n#          raise ValueError(f\"Could not compute gradients w.r.t. output of layer '{target_layer.name}'. Check model graph connectivity.\")\n\n#     # Pool gradients and compute heatmap (same logic as before)\n#     pooled_grads = tf.reduce_mean(grads, axis=(0, 1, 2)) # Pool spatially H, W\n#     target_layer_output = target_layer_output[0] # Remove batch dim -> (H, W, C)\n#     heatmap = target_layer_output @ pooled_grads[..., tf.newaxis] # Weighted combination -> (H, W, 1)\n#     heatmap = tf.squeeze(heatmap) # -> (H, W)\n#     heatmap = tf.maximum(heatmap, 0) # ReLU\n#     max_val = tf.math.reduce_max(heatmap)\n#     if max_val > 1e-6: heatmap = heatmap / max_val # Normalize\n#     else: heatmap = tf.zeros_like(heatmap)\n#     print(f\"   Heatmap generated successfully for layer '{target_layer.name}'.\")\n#     return heatmap.numpy()\n\n# def display_gradcam(img_path, heatmap, alpha=0.6, save_path=None):\n#     \"\"\"Superimposes heatmap on original image and displays/saves.\"\"\"\n#     try:\n#         img = tf.keras.utils.load_img(img_path) # Load original color/size\n#         img = tf.keras.utils.img_to_array(img)\n\n#         # Resize heatmap to match image\n#         heatmap = cv2.resize(heatmap, (img.shape[1], img.shape[0]))\n\n#         # Convert heatmap to RGB colormap\n#         heatmap = np.uint8(255 * heatmap)\n#         jet = cv2.applyColorMap(heatmap, cv2.COLORMAP_JET) # Use JET colormap\n\n#         # Superimpose\n#         superimposed_img = jet * alpha + img * (1 - alpha)\n#         superimposed_img = np.clip(superimposed_img, 0, 255).astype(np.uint8) # Clip values\n\n#         # Display inline\n#         plt.figure(figsize=(8, 8))\n#         plt.imshow(superimposed_img)\n#         plt.axis('off')\n#         title = f\"Grad-CAM: {os.path.basename(img_path)}\"\n#         if save_path:\n#             try:\n#                  # Ensure save directory exists\n#                  os.makedirs(os.path.dirname(save_path), exist_ok=True)\n#                  # Save before showing\n#                  plt.savefig(save_path, bbox_inches='tight', pad_inches=0.1)\n#                  title += f\"\\n(Saved to {os.path.basename(save_path)})\"\n#                  plt.title(title)\n#                  plt.show()\n#                  print(f\"Grad-CAM saved to {save_path}\")\n#             except Exception as e:\n#                  print(f\"Error saving Grad-CAM image to {save_path}: {e}\")\n#                  plt.title(title + \"\\n(Save failed)\")\n#                  plt.show()\n#         else:\n#             plt.title(title)\n#             plt.show()\n\n#     except Exception as e:\n#         print(f\"Error displaying/saving Grad-CAM for {img_path}: {e}\")\n#         plt.close() # Close the plot if error occurs\n\n\n# # Note: Renamed model -> full_model, added target_layer object argument\n# def run_gradcam_on_samples(full_model, target_layer, img_paths, img_size, output_dir, config_key):\n#      \"\"\"\n#      Runs Grad-CAM on a list of image paths using the specified target layer object.\n\n#      Args:\n#          full_model: The complete Keras model.\n#          target_layer: The Keras Layer object to target for Grad-CAM (e.g., base_model layer).\n#          img_paths: List of image file paths.\n#          img_size: Target size for preprocessing image (int).\n#          output_dir: Directory to save Grad-CAM images.\n#          config_key: String identifier for the current experiment configuration.\n#      \"\"\"\n#      print(f\"\\n--- Generating Grad-CAM Visualizations (Target: {target_layer.name}) ---\")\n#      if not img_paths: print(\"No image paths provided.\"); return\n#      if target_layer is None: print(\"Target layer object not provided.\"); return\n\n#      for img_path in img_paths:\n#           if not os.path.exists(img_path): print(f\"Skipping non-existent file: {img_path}\"); continue\n\n#           # Load and preprocess image for Grad-CAM input (expects RGB usually)\n#           img_array = get_img_array(img_path, size=(img_size, img_size))\n#           if img_array is None: continue # Skip if loading failed\n\n#           try:\n#                # Call the modified heatmap function, passing the target layer object\n#                heatmap = make_gradcam_heatmap(\n#                    img_array_rgb=img_array,\n#                    full_model=full_model,\n#                    target_layer=target_layer # Pass the layer object\n#                )\n\n#                # Create save path (same logic)\n#                base_filename = os.path.splitext(os.path.basename(img_path))[0]\n#                # Sanitize config_key for filename\n#                safe_config_key = \"\".join(c if c.isalnum() else \"_\" for c in config_key)\n#                save_filename = f\"gradcam_{safe_config_key}_{base_filename}.png\"\n#                save_path = os.path.join(output_dir, save_filename)\n\n#                # Display and save the superimposed image\n#                display_gradcam(img_path, heatmap, save_path=save_path)\n\n#           except ValueError as ve: # Catch potential layer name issues during heatmap generation\n#                print(f\"ERROR generating Grad-CAM heatmap for {img_path}: {ve}\")\n#                # Break if it's likely a layer name issue affecting all images\n#                if \"Could not compute gradients\" in str(ve):\n#                     print(\"Stopping further Grad-CAM attempts due to gradient computation issue.\")\n#                     break\n#           except Exception as e:\n#                print(f\"Unexpected error generating Grad-CAM for {img_path}: {e}\")\n#                # traceback.print_exc() # Uncomment for detailed traceback\n\n\n# # --- Plotting Functions ---\n# def plot_history(history, title_suffix=\"\", metrics=['loss', 'accuracy', 'precision', 'recall', 'auc']):\n#     \"\"\"Plots training/validation metrics from Keras History.\"\"\"\n#     if history is None or not history.history:\n#         print(\"No history data found to plot.\")\n#         return\n\n#     history_df = pd.DataFrame(history.history)\n#     # Add epoch numbers, starting from initial_epoch + 1 if available\n#     initial_epoch = history.epoch[0] if history.epoch else 0\n#     history_df['epoch'] = np.array(history.epoch) + 1 # Display as 1-based index\n\n#     num_metrics = len(metrics)\n#     ncols = min(num_metrics, 3)\n#     nrows = ceil(num_metrics / ncols)\n#     fig, axes = plt.subplots(nrows, ncols, figsize=(6 * ncols, 4 * nrows), squeeze=False)\n#     axes = axes.flatten()\n\n#     palette = sns.color_palette(\"deep\", 2) # Train/Val colors\n\n#     for i, metric in enumerate(metrics):\n#         ax = axes[i]\n#         val_metric = f'val_{metric}'\n\n#         # Plot training metric if available\n#         if metric in history_df.columns:\n#             sns.lineplot(data=history_df, x='epoch', y=metric, ax=ax,\n#                          label=f'Train {metric.capitalize()}', color=palette[0], marker='o', markersize=4)\n#         # Plot validation metric if available\n#         if val_metric in history_df.columns:\n#             sns.lineplot(data=history_df, x='epoch', y=val_metric, ax=ax,\n#                          label=f'Val {metric.capitalize()}', color=palette[1], linestyle='--', marker='x', markersize=4)\n#         else:\n#              # If val metric is missing but train exists, add note or handle display\n#              if metric in history_df.columns:\n#                  print(f\"Note: Validation metric '{val_metric}' not found in history.\")\n\n#         ax.set_title(metric.replace('_', ' ').title())\n#         ax.set_xlabel(\"Epoch\")\n#         ax.set_ylabel(\"Value\")\n#         ax.xaxis.set_major_locator(plt.MaxNLocator(integer=True)) # Ensure integer epochs\n#         if metric in history_df.columns or val_metric in history_df.columns:\n#              ax.legend()\n#         ax.grid(True, linestyle='--', alpha=0.6)\n\n#     # Hide unused axes\n#     for j in range(num_metrics, len(axes)):\n#         axes[j].axis('off')\n\n#     fig.suptitle(f\"Training History {title_suffix}\".strip(), fontsize=16, y=1.02)\n#     plt.tight_layout(rect=[0, 0.03, 1, 0.96]) # Adjust rect to prevent title overlap\n#     plt.show()\n\n# def plot_confusion_matrix(cm, inv_label_map, title_suffix=\"\"):\n#     \"\"\"Plots an enhanced confusion matrix.\"\"\"\n#     if cm is None:\n#         print(f\"Cannot plot confusion matrix: Input CM is None [{title_suffix}]\")\n#         return\n#     if not isinstance(inv_label_map, dict):\n#         print(f\"Error: inv_label_map must be a dictionary. Got {type(inv_label_map)}\")\n#         # Fallback class names if map is invalid\n#         class_names = [f\"Class {i}\" for i in range(cm.shape[0])]\n#     else:\n#          # Ensure class names match CM dimensions, using provided map as priority\n#          num_classes = cm.shape[0]\n#          # Try to get names for indices 0 to num_classes-1\n#          class_names = [inv_label_map.get(i, f\"Class {i}\") for i in range(num_classes)]\n\n\n#     plt.figure(figsize=(max(5, num_classes*1.5), max(4, num_classes*1.2))) # Dynamic size\n#     sns.heatmap(cm, annot=True, fmt='d', cmap='Blues',\n#                 xticklabels=class_names, yticklabels=class_names,\n#                 annot_kws={\"size\": 11}, linewidths=.5, linecolor='grey')\n#     plt.title(f'Confusion Matrix {title_suffix}'.strip(), fontsize=14)\n#     plt.ylabel('True Label', fontsize=11)\n#     plt.xlabel('Predicted Label', fontsize=11)\n#     plt.xticks(rotation=45, ha='right') # Rotate if many classes\n#     plt.yticks(rotation=0, va='center')\n#     plt.tight_layout()\n#     plt.show()\n\n# def plot_roc_curve(y_true, y_pred_proba, roc_auc, inv_label_map, title_suffix=\"\"):\n#     \"\"\"Plots an enhanced ROC curve.\"\"\"\n#     if y_true is None or y_pred_proba is None:\n#         print(f\"Cannot plot ROC curve: Missing true labels or predictions [{title_suffix}].\")\n#         return\n#     if y_true.ndim != 1 or y_pred_proba.ndim != 1:\n#          print(f\"Cannot plot ROC curve: Inputs must be 1D arrays [{title_suffix}].\")\n#          return\n#     if len(np.unique(y_true)) < 2:\n#         print(f\"Cannot plot ROC curve: Only one class present in true labels [{title_suffix}].\")\n#         return\n#     # Check for NaN in roc_auc before plotting\n#     if roc_auc is None or np.isnan(roc_auc):\n#         print(f\"Cannot plot ROC curve: AUC score is invalid (None or NaN) [{title_suffix}].\")\n#         # Optionally, still plot the curve without the AUC score\n#         auc_label = \"AUC = N/A\"\n#     else:\n#          auc_label = f'AUC = {roc_auc:.4f}'\n\n\n#     fpr, tpr, _ = roc_curve(y_true, y_pred_proba)\n\n#     plt.figure(figsize=(7, 6))\n#     # Use label map to get positive class name, default if not found or map is invalid\n#     positive_class_name = \"Positive\"\n#     if isinstance(inv_label_map, dict):\n#          positive_class_name = inv_label_map.get(1, \"Class 1\")\n\n#     plt.plot(fpr, tpr, color='darkorange', lw=2.5,\n#              label=f'ROC curve ({positive_class_name}) ({auc_label})')\n#     plt.plot([0, 1], [0, 1], color='navy', lw=2, linestyle='--', label='Chance (AUC = 0.5)')\n#     plt.xlim([-0.02, 1.0])\n#     plt.ylim([0.0, 1.05])\n#     plt.xlabel('False Positive Rate')\n#     plt.ylabel('True Positive Rate')\n#     plt.title(f'Receiver Operating Characteristic (ROC) {title_suffix}'.strip(), fontsize=14)\n#     plt.legend(loc=\"lower right\")\n#     plt.grid(True, linestyle='--', alpha=0.6)\n#     plt.tight_layout()\n#     plt.show()\n\n\n# # def plot_comparison_bars(results_dict, title, metric_keys=['f1_opt', 'accuracy_opt', 'roc_auc_proba']):\n# #     \"\"\"\n# #     Creates publication-quality comparison bar charts for specified metrics.\n\n# #     Parameters:\n# #     - results_dict: Dictionary containing configuration names as keys and metric dictionaries as values\n# #     - title: Main title for the plot\n# #     - metric_keys: List of metric keys to plot (default: ['f1_opt', 'accuracy_opt', 'roc_auc_proba'])\n\n# #     Returns:\n# #     - matplotlib Figure object or None\n# #     \"\"\"\n# #     # Validation checks\n# #     if not results_dict:\n# #         print(f\"No results found for comparison: {title}\")\n# #         return None\n\n# #     # Filter for relevant metrics and create DataFrame\n# #     data_for_plot = {}\n# #     valid_configs = []\n# #     for config, metrics in results_dict.items():\n# #         if isinstance(metrics, dict):\n# #             # Check if at least one metric_key is present and not NaN\n# #             has_valid_metric = any(k in metrics and not pd.isna(metrics.get(k)) for k in metric_keys)\n# #             if has_valid_metric:\n# #                  data_for_plot[config] = {k: metrics.get(k, np.nan) for k in metric_keys}\n# #                  valid_configs.append(config)\n\n# #     if not data_for_plot:\n# #         print(\"No valid configurations with specified metrics found.\")\n# #         return None\n\n# #     # Create a mapping for configuration names (only for valid configs)\n# #     config_map = {name: f\"Cfg_{i+1}\" for i, name in enumerate(valid_configs)}\n\n# #     # Create DataFrame with short config names using only valid configs\n# #     renamed_data = {}\n# #     for orig_name in valid_configs:\n# #         renamed_data[config_map[orig_name]] = data_for_plot[orig_name]\n\n# #     df = pd.DataFrame(renamed_data).T.reset_index().rename(columns={'index': 'Configuration'})\n# #     if df.empty:\n# #         print(\"DataFrame empty for plotting after filtering.\")\n# #         return None\n\n# #     # Determine sort metric, handle missing metric\n# #     sort_metric = None\n# #     if 'f1_opt' in metric_keys and 'f1_opt' in df.columns:\n# #         sort_metric = 'f1_opt'\n# #     elif metric_keys:\n# #         for k in metric_keys: # Find first available metric key to sort by\n# #             if k in df.columns:\n# #                 sort_metric = k\n# #                 break\n# #     if sort_metric:\n# #         # Drop rows where the sort metric is NaN before sorting\n# #         df_sorted = df.dropna(subset=[sort_metric]).sort_values(by=sort_metric, ascending=False)\n# #         # Get the sorted order\n# #         sorted_order = df_sorted['Configuration'].tolist()\n# #         # Reindex the original df (with NaNs) according to this order\n# #         # This keeps all configs but sorts by the metric where available\n# #         df = df.set_index('Configuration').loc[sorted_order].reset_index()\n# #     else: # Fallback sort if no metric found\n# #          df = df.sort_values(by='Configuration')\n\n\n# #     # Set up plot grid\n# #     num_metrics = len(metric_keys)\n# #     ncols = min(num_metrics, 3)\n# #     nrows = ceil(num_metrics / ncols)\n\n# #     # Create figure with dynamic sizing based on number of configs\n# #     # Adjust base height and per-config height\n# #     base_height = 2.5 # Minimum height\n# #     height_per_config = 0.4 # Increase height per config\n# #     fig_height = max(4, base_height + len(df) * height_per_config)\n# #     fig_width = 6 * ncols # Keep width proportional to columns\n\n# #     fig, axes = plt.subplots(nrows, ncols, figsize=(fig_width, fig_height), squeeze=False)\n\n# #     # Increase vertical spacing between rows of plots\n# #     plt.subplots_adjust(hspace=0.6) # Spacing between rows of subplots\n\n# #     axes = axes.flatten()\n\n# #     # Set main title\n# #     fig.suptitle(title, fontsize=14, y=1.01, fontweight='bold') # Adjust y slightly\n\n# #     # Use a professional color palette\n# #     palette = sns.color_palette(\"viridis_r\", n_colors=num_metrics) # Changed palette\n\n# #     # Plot each metric\n# #     for i, metric in enumerate(metric_keys):\n# #         if i >= len(axes): continue # Skip if axes run out\n\n# #         ax = axes[i]\n\n# #         # Check if metric exists in DataFrame columns\n# #         if metric not in df.columns:\n# #             print(f\"Metric '{metric}' not found in results, skipping plot.\")\n# #             ax.set_title(f\"{metric} (Not Found)\", fontsize=10)\n# #             ax.axis('off') # Turn off the axis for missing metric\n# #             continue\n\n# #         # Create horizontal bar plot\n# #         # Filter out NaN values for this specific metric before plotting\n# #         plot_df = df[['Configuration', metric]].dropna(subset=[metric])\n\n# #         if plot_df.empty:\n# #              print(f\"No valid data for metric '{metric}', skipping plot.\")\n# #              ax.set_title(f\"{metric} (No Data)\", fontsize=10)\n# #              ax.axis('off')\n# #              continue\n\n# #         bars = sns.barplot(\n# #             data=plot_df,\n# #             x=metric,\n# #             y='Configuration',\n# #             ax=ax,\n# #             color=palette[i % len(palette)], # Use modulo for color cycling\n# #             orient='h',\n# #             saturation=0.85,\n# #             edgecolor='black', # Make edge more visible\n# #             linewidth=0.7\n# #         )\n\n# #         # Format axis and titles\n# #         metric_name = (\n# #             metric.replace('_', ' ')\n# #                  .replace(' opt', ' (Opt)')\n# #                  .replace(' proba', '')\n# #                  .title()\n# #         )\n# #         ax.set_title(metric_name, fontsize=12, pad=10)\n# #         ax.set_xlabel(\"Score\", fontsize=10)\n# #         ax.set_ylabel(\"\") # Remove y-axis label\n\n# #         # Adjust tick labels\n# #         ax.tick_params(axis='y', labelsize=9)\n# #         ax.tick_params(axis='x', labelsize=8)\n\n# #         # Set x-limits appropriate for the metric (0 to 1 for common metrics, else data-driven)\n# #         max_val = plot_df[metric].max() if not plot_df.empty else 0\n# #         is_common_metric = any(k in metric for k in ['accuracy', 'precision', 'recall', 'f1', 'auc'])\n# #         if is_common_metric and max_val <= 1:\n# #             upper_limit = 1.05 # Extend slightly beyond 1.0\n# #             ax.set_xlim(0, upper_limit)\n# #         elif not plot_df.empty:\n# #             upper_limit = max(0, max_val) * 1.1 + 0.01 # Data driven with buffer\n# #             ax.set_xlim(0, upper_limit)\n# #         else:\n# #              ax.set_xlim(0, 1) # Default if no data\n\n# #         # Add value annotations beside the bars (moved outside of bars)\n# #         for p in ax.patches:\n# #             width = p.get_width()\n# #             # Position text slightly outside the bar\n# #             ax.text(width + (ax.get_xlim()[1] * 0.01), # Adjust position based on axis limit\n# #                     p.get_y() + p.get_height()/2.,\n# #                     f'{width:.4f}',\n# #                     ha='left', va='center',\n# #                     fontsize=8.5, fontweight='bold', color='black') # Adjust font\n\n# #         # Add grid lines\n# #         ax.grid(axis='x', linestyle=':', alpha=0.5)\n\n# #         # Remove spines for cleaner look\n# #         sns.despine(ax=ax, left=True) # Also remove left spine\n\n# #     # Hide unused axes\n# #     for j in range(num_metrics, len(axes)):\n# #         if j < len(axes): # Check index bounds\n# #              axes[j].axis('off')\n\n# #     # Create a configuration name legend below the plots\n# #     config_legend = \"\\n\".join([f\"{short}: {orig}\" for short, orig in config_map.items() if orig in valid_configs])\n\n# #     # Place legend below plots using fig.text, adjust coordinates as needed\n# #     # Increase bottom margin in tight_layout to make space\n# #     fig.text(0.5, 0.02, config_legend, # Positioned relative to figure\n# #              ha='center', va='bottom',\n# #              fontsize=8,\n# #              bbox=dict(boxstyle='round,pad=0.5', facecolor='whitesmoke', alpha=0.8, edgecolor='lightgray'))\n\n# #     # Adjust layout with proper spacing, leaving space for title and legend\n# #     # rect=[left, bottom, right, top]\n# #     plt.tight_layout(rect=[0, 0.1, 1, 0.95]) # Increase bottom margin for legend\n# #     plt.subplots_adjust(left=0.25, wspace=0.35) # Adjust left margin and width spacing\n\n# #     return fig\n# def plot_comparison_bars(results_dict, title, metric_keys=['f1_opt', 'accuracy_opt', 'roc_auc_proba']):\n#     \"\"\"\n#     Creates publication-quality comparison bar charts for specified metrics.\n    \n#     Parameters:\n#     - results_dict: Dictionary containing configuration names as keys and metric dictionaries as values\n#     - title: Main title for the plot\n#     - metric_keys: List of metric keys to plot (default: ['f1_opt', 'accuracy_opt', 'roc_auc_proba'])\n    \n#     Returns:\n#     - matplotlib Figure object\n#     \"\"\"\n#     import numpy as np\n#     import pandas as pd\n#     import matplotlib.pyplot as plt\n#     import seaborn as sns\n#     from math import ceil\n    \n#     # Validation checks\n#     if not results_dict:\n#         print(f\"No results found for comparison: {title}\")\n#         return None\n    \n#     # Filter for relevant metrics and create DataFrame\n#     data_for_plot = {}\n#     for config, metrics in results_dict.items():\n#         if isinstance(metrics, dict):\n#             data_for_plot[config] = {k: metrics.get(k, np.nan) for k in metric_keys}\n    \n#     if not data_for_plot:\n#         print(\"No valid data for specified metrics.\")\n#         return None\n    \n#     # Create a mapping for configuration names\n#     config_names = list(data_for_plot.keys())\n#     config_map = {name: f\"Config_{i+1}\" for i, name in enumerate(config_names)}\n    \n#     # Create DataFrame with short config names\n#     renamed_data = {}\n#     for orig_name, metrics in data_for_plot.items():\n#         renamed_data[config_map[orig_name]] = metrics\n    \n#     df = pd.DataFrame(renamed_data).T.reset_index().rename(columns={'index': 'Configuration'})\n#     if df.empty:\n#         print(\"DataFrame empty for plotting.\")\n#         return None\n    \n#     # Sort by first metric (or f1_opt if available)\n#     if 'f1_opt' in metric_keys:\n#         sort_metric = 'f1_opt'\n#     elif metric_keys:\n#         sort_metric = metric_keys[0]\n#     else:\n#         sort_metric = 'Configuration'\n#     df = df.sort_values(by=sort_metric, ascending=False)\n    \n#     # Set up plot grid\n#     num_metrics = len(metric_keys)\n#     ncols = min(num_metrics, 3)\n#     nrows = ceil(num_metrics / ncols)\n    \n#     # Create figure with dynamic sizing\n#     fig_width = 6 * ncols\n#     fig_height = max(4, 0.5 * len(df)) \n#     fig, axes = plt.subplots(nrows, ncols, figsize=(fig_width, fig_height), squeeze=False)\n    \n#     # Increase vertical spacing between rows\n#     plt.subplots_adjust(hspace=0.6)  # Increased from 0.4 to 0.6\n    \n#     axes = axes.flatten()\n    \n#     # Set main title\n#     fig.suptitle(title, fontsize=14, y=1.05, fontweight='bold')\n    \n#     # Use a professional color palette\n#     palette = sns.color_palette(\"mako_r\", n_colors=num_metrics)\n    \n#     # Plot each metric\n#     for i, metric in enumerate(metric_keys):\n#         if i >= len(axes) or metric not in df.columns:\n#             continue\n            \n#         ax = axes[i]\n        \n#         # Create horizontal bar plot\n#         bars = sns.barplot(\n#             data=df, \n#             x=metric, \n#             y='Configuration', \n#             ax=ax,\n#             color=palette[i], \n#             orient='h', \n#             saturation=0.85,\n#             edgecolor='white',\n#             linewidth=0.5\n#         )\n        \n#         # Format axis and titles\n#         metric_name = (\n#             metric.replace('_', ' ')\n#                  .replace(' opt', ' (optimized)')\n#                  .replace(' proba', '')\n#                  .title()\n#         )\n#         ax.set_title(metric_name, fontsize=12, pad=10)\n#         ax.set_xlabel(\"Score\", fontsize=10)\n#         ax.set_ylabel(\"\")\n        \n#         # Adjust tick labels\n#         ax.tick_params(axis='y', labelsize=9)\n#         ax.tick_params(axis='x', labelsize=8)\n        \n#         # Set x-limits appropriate for the metric\n#         max_val = df[metric].max()\n#         upper_limit = min(1.05, max(1.0, max_val * 1.05)) if max_val <= 1 else max_val * 1.05\n#         ax.set_xlim(0, upper_limit)\n        \n#         # Add value annotations beside the bars (moved outside of bars)\n#         for i, p in enumerate(ax.patches):\n#             width = p.get_width()\n#             ax.text(width + 0.01, p.get_y() + p.get_height()/2, \n#                     f'{width:.4f}', \n#                     ha='left', va='center',\n#                     fontsize=9, fontweight='bold')\n        \n#         # Add grid lines\n#         ax.grid(axis='x', linestyle=':', alpha=0.4)\n        \n#         # Remove spines for cleaner look\n#         for spine in ['top', 'right']:\n#             ax.spines[spine].set_visible(False)\n    \n#     # Hide unused axes\n#     for j in range(num_metrics, len(axes)):\n#         axes[j].axis('off')\n    \n#     # Create a configuration name legend in the bottom right\n#     config_legend = \"\\n\".join([f\"{short} = {orig}\" for short, orig in config_map.items()])\n#     fig.text(0.98, 0.02, config_legend, \n#              ha='right', va='bottom', \n#              fontsize=8, \n#              bbox=dict(boxstyle='round', facecolor='whitesmoke', alpha=0.7))\n    \n#     # Adjust layout with proper spacing\n#     plt.tight_layout(rect=[0, 0.05, 1, 0.95])  # Leave space for title and legend\n#     plt.subplots_adjust(left=0.25, wspace=0.3)  # Adjust margins and spacing\n    \n#     return fig\n\n# print(\"Utility functions defined.\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-03T15:32:49.820834Z","iopub.execute_input":"2025-06-03T15:32:49.821071Z","iopub.status.idle":"2025-06-03T15:32:49.854645Z","shell.execute_reply.started":"2025-06-03T15:32:49.821044Z","shell.execute_reply":"2025-06-03T15:32:49.853879Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# # CELL 6: Core Training, Evaluation, and Enhanced Visualization Utilities\n# print(\"\\n--- Cell 6: Core Training, Evaluation, and Enhanced Visualization Utilities ---\")\n\n# import time\n# import numpy as np\n# import pandas as pd\n# import tensorflow as tf # Ensure tensorflow is imported\n# from tensorflow import keras # Ensure keras is imported\n# import matplotlib.pyplot as plt\n# import seaborn as sns\n# from sklearn.metrics import (\n#     accuracy_score, precision_score, recall_score, f1_score, # For find_optimal_threshold & evaluate_model\n#     roc_auc_score, confusion_matrix, classification_report, # For evaluate_model & plotting\n#     roc_curve, # For evaluate_model & plotting\n#     precision_recall_curve, average_precision_score # For new plotting\n# )\n# # import cv2 # Was in old Grad-CAM, not needed if Grad-CAM is fully removed\n# import os\n# # import traceback # For debugging if needed\n# from math import ceil\n\n# # --- Matplotlib Publication Quality Settings (from your new Cell 6) ---\n# plt.rcParams.update({\n#     'font.size': 11,\n#     'font.family': 'serif',\n#     'axes.labelsize': 12,\n#     'axes.titlesize': 14,\n#     'xtick.labelsize': 10,\n#     'ytick.labelsize': 10,\n#     'legend.fontsize': 11,\n#     'figure.titlesize': 16,\n#     'axes.linewidth': 1.2,\n#     'grid.linewidth': 0.8,\n#     'lines.linewidth': 2.0,\n#     'patch.linewidth': 0.5,\n#     'savefig.dpi': 300,\n#     'savefig.format': 'pdf',\n#     'savefig.bbox': 'tight'\n# })\n\n# # --- Training Function (from Old Cell 6) ---\n# def train_model(model, train_ds, val_ds, epochs, class_weights, strategy, learning_rate,\n#                 initial_epoch=0, callbacks=None, stage_name=\"Training\"):\n#     \"\"\"Compiles and trains the model within the strategy scope.\"\"\"\n#     if train_ds is None:\n#         print(f\"ERROR [{stage_name}]: Training dataset is None. Cannot train.\")\n#         return None, None\n\n#     with strategy.scope():\n#         metrics_list = [\n#             'accuracy',\n#             tf.keras.metrics.Precision(name='precision'),\n#             tf.keras.metrics.Recall(name='recall'),\n#             # tf.keras.metrics.AUC(name='auc')\n#             tfa.metrics.F1Score(num_classes=1, threshold=0.5, name='f1_score', average='micro'),\n#         ]\n#         model.compile(\n#             optimizer=keras.optimizers.Adam(learning_rate=learning_rate),\n#             loss='binary_crossentropy',\n#             metrics=metrics_list\n#         )\n\n#     print(f\"\\n--- Starting {stage_name} ---\")\n#     print(f\"Epochs: {epochs}, Initial Epoch: {initial_epoch}\")\n#     print(f\"Class Weights: {'Applied' if class_weights else 'None'}\")\n\n#     start_time = time.time()\n#     history = model.fit(\n#         train_ds,\n#         validation_data=val_ds,\n#         epochs=epochs,\n#         initial_epoch=initial_epoch,\n#         class_weight=class_weights,\n#         callbacks=callbacks or [],\n#         verbose=1\n#     )\n#     end_time = time.time()\n#     training_duration = end_time - start_time\n#     print(f\"--- {stage_name} Finished (Duration: {training_duration:.2f} seconds) ---\")\n#     return history, training_duration\n\n# # --- Threshold Optimization (from Old Cell 6) ---\n# def find_optimal_threshold(y_true, y_pred_proba, target_metric='f1'):\n#     \"\"\"Finds the threshold maximizing the target metric on validation predictions.\"\"\"\n#     best_threshold = 0.5\n#     best_score = -1.0\n\n#     if y_true is None or y_pred_proba is None or len(y_true) == 0 or len(y_true) != len(y_pred_proba):\n#         print(\"Warning: Invalid inputs for threshold optimization. Returning default 0.5.\")\n#         return best_threshold\n\n#     y_true = y_true.astype(int)\n#     thresholds = np.arange(0.01, 1.0, 0.01)\n#     scores = []\n\n#     for thresh in thresholds:\n#         y_pred_binary = (y_pred_proba >= thresh).astype(int)\n#         if target_metric == 'f1':\n#             score = f1_score(y_true, y_pred_binary, zero_division=0)\n#         elif target_metric == 'accuracy':\n#             score = accuracy_score(y_true, y_pred_binary)\n#         else: # Default to F1\n#             score = f1_score(y_true, y_pred_binary, zero_division=0)\n#         scores.append(score)\n\n#     if scores:\n#         best_idx = np.argmax(scores)\n#         # best_score = scores[best_idx] # This line was in old code, not strictly needed for return\n#         best_threshold = thresholds[best_idx]\n#     else:\n#         print(\"Warning: Could not compute scores for threshold optimization. Using default 0.5.\")\n#     return best_threshold\n\n# # --- Enhanced Plotting Functions (from your new Cell 6) ---\n# # In Cell 6:\n# # In Cell 6:\n# def plot_training_history_enhanced(history, title_suffix=\"\", save_dir=None, config_name=\"model\"):\n#     if history is None or not history.history:\n#         print(\"No history data found to plot.\")\n#         return\n#     history_df = pd.DataFrame(history.history)\n#     history_df['epoch'] = np.arange(1, len(history_df) + 1)\n\n#     potential_metrics_map = {\n#         'loss': 'Loss', 'accuracy': 'Accuracy', 'precision': 'Precision', 'recall': 'Recall',\n#         'f1_score': 'F1-score', 'f1': 'F1-score', 'auc': 'AUC'\n#     }\n#     available_history_keys = list(history_df.columns)\n#     metrics_for_plotting = []\n\n#     for key in ['loss', 'accuracy', 'precision', 'recall']:\n#         if key in available_history_keys:\n#             metrics_for_plotting.append({'key': key, 'name': potential_metrics_map[key]})\n\n#     f1_key_to_use = None\n#     if 'f1_score' in available_history_keys: f1_key_to_use = 'f1_score'\n#     elif 'f1' in available_history_keys: f1_key_to_use = 'f1'\n\n#     if f1_key_to_use:\n#         if len(metrics_for_plotting) < 5:\n#             metrics_for_plotting.append({'key': f1_key_to_use, 'name': potential_metrics_map[f1_key_to_use]})\n#     elif 'auc' in available_history_keys:\n#         if len(metrics_for_plotting) < 5:\n#             metrics_for_plotting.append({'key': 'auc', 'name': potential_metrics_map['auc']})\n    \n#     num_actual_plots = len(metrics_for_plotting)\n#     if num_actual_plots == 0:\n#         print(\"No plottable metrics found in history.\")\n#         return\n\n#     fig = plt.figure() # Initialize figure; size set below\n#     subplot_definitions = []\n\n#     if num_actual_plots == 5:\n#         fig.set_size_inches(18, 10) # 3 plots wide for the first row\n#         # Use a 2-row, 6-virtual-column gridspec for centering the second row\n#         gs_fig = fig.add_gridspec(2, 6, hspace=0.4, wspace=0.6) # Adjusted wspace for 6 columns\n#         subplot_definitions = [\n#             gs_fig[0, 0:2], gs_fig[0, 2:4], gs_fig[0, 4:6], # Row 1 (3 plots, each spanning 2 virtual cols)\n#             gs_fig[1, 1:3], gs_fig[1, 3:5]                  # Row 2 (2 plots, centered, each spanning 2 virtual cols)\n#         ]\n#     elif num_actual_plots == 4:\n#         fig.set_size_inches(12, 10) # 2 plots wide\n#         gs_fig = fig.add_gridspec(2, 2, hspace=0.4, wspace=0.3)\n#         subplot_definitions = [gs_fig[0,0], gs_fig[0,1], gs_fig[1,0], gs_fig[1,1]]\n#     elif num_actual_plots > 0: # 1, 2, or 3 plots\n#         fig.set_size_inches(6 * num_actual_plots, 5.5) # Slightly taller for single row titles\n#         gs_fig = fig.add_gridspec(1, num_actual_plots, hspace=0.4, wspace=0.3 if num_actual_plots > 1 else 0)\n#         subplot_definitions = [gs_fig[0,i] for i in range(num_actual_plots)]\n#     else: # Should be caught earlier\n#         return\n\n#     colors = {'train': '#2E86AB', 'val': '#A23B72'}\n\n#     for idx, metric_info in enumerate(metrics_for_plotting):\n#         metric_key = metric_info['key']\n#         display_name = metric_info['name']\n        \n#         ax = fig.add_subplot(subplot_definitions[idx])\n#         val_metric_key = f'val_{metric_key}'\n\n#         if metric_key in history_df.columns:\n#             ax.plot(history_df['epoch'], history_df[metric_key],\n#                     color=colors['train'], marker='o', markersize=4,\n#                     label='Training', linewidth=2)\n#         if val_metric_key in history_df.columns:\n#             ax.plot(history_df['epoch'], history_df[val_metric_key],\n#                     color=colors['val'], marker='s', markersize=4,\n#                     label='Validation', linewidth=2, linestyle='--')\n        \n#         ax.set_title(display_name, fontweight='bold', pad=10)\n#         ax.set_xlabel('Epoch')\n#         ax.set_ylabel('Value')\n#         ax.grid(True, alpha=0.3)\n#         ax.legend(frameon=True, fancybox=True, shadow=True)\n        \n#         if metric_key not in ['loss']:\n#             ax.set_ylim(0, 1.05)\n#         else: \n#             min_val_data = history_df[metric_key].dropna()\n#             max_val_data = history_df[metric_key].dropna()\n#             if val_metric_key in history_df: # Ensure val_metric_key exists before trying to access it\n#                 min_val_data = pd.concat([min_val_data, history_df[val_metric_key].dropna()])\n#                 max_val_data = pd.concat([max_val_data, history_df[val_metric_key].dropna()])\n            \n#             min_loss = min_val_data.min() if not min_val_data.empty else 0\n#             max_loss = max_val_data.max() if not max_val_data.empty else 1\n            \n#             padding_abs = 0.1 \n#             if pd.notna(min_loss) and pd.notna(max_loss) and (max_loss - min_loss) > 1e-5 :\n#                  padding = 0.1 * (max_loss - min_loss)\n#             else:\n#                  padding = padding_abs\n            \n#             y_min_plot = min_loss - padding if pd.isna(min_loss) else max(0, min_loss - padding) # Ensure y_min is not negative if loss is non-negative\n#             y_max_plot = max_loss + padding if pd.notna(max_loss) else y_min_plot + 2*padding_abs \n\n#             ax.set_ylim(y_min_plot, y_max_plot)\n\n#     fig.suptitle(f'Training History {title_suffix}', fontsize=16, fontweight='bold')\n#     plt.tight_layout(rect=[0, 0.03, 1, 0.95]) # Adjust rect to make space for suptitle\n\n#     if save_dir:\n#         os.makedirs(save_dir, exist_ok=True)\n#         filename = f\"training_history_{config_name}.pdf\"\n#         filepath = os.path.join(save_dir, filename)\n#         plt.savefig(filepath, dpi=300, bbox_inches='tight')\n#         print(f\"Training history saved: {filepath}\")\n#     plt.show()\n\n# def plot_performance_curves(y_true, y_pred_proba, optimal_threshold, inv_label_map,\n#                             title_suffix=\"\", save_dir=None, config_name=\"model\"):\n#     if y_true is None or y_pred_proba is None:\n#         print(\"Cannot plot performance curves: Missing data\")\n#         return\n#     if len(np.unique(y_true)) < 2:\n#         print(\"Cannot plot curves: Only one class present\")\n#         return\n\n#     fpr, tpr, roc_thresholds = roc_curve(y_true, y_pred_proba)\n#     roc_auc = roc_auc_score(y_true, y_pred_proba)\n#     precision, recall, pr_thresholds = precision_recall_curve(y_true, y_pred_proba)\n#     avg_precision = average_precision_score(y_true, y_pred_proba)\n#     positive_class = inv_label_map.get(1, \"Pneumonia\") if isinstance(inv_label_map, dict) else \"Positive\"\n\n#     fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(14, 6))\n#     # ROC Curve\n#     ax1.plot(fpr, tpr, color='#2E86AB', linewidth=3, label=f'ROC Curve (AUC = {roc_auc:.4f})')\n#     ax1.plot([0, 1], [0, 1], color='gray', linewidth=2, linestyle='--', alpha=0.7, label='Random Classifier')\n#     optimal_idx_roc = np.argmin(np.abs(roc_thresholds - optimal_threshold)) # Ensure roc_thresholds is not empty\n#     if optimal_idx_roc < len(fpr): # Check bounds\n#         ax1.scatter(fpr[optimal_idx_roc], tpr[optimal_idx_roc], color='red', s=100,\n#                     zorder=5, label=f'Optimal Threshold ({optimal_threshold:.3f})')\n#     ax1.set_xlim([0.0, 1.0]); ax1.set_ylim([0.0, 1.05])\n#     ax1.set_xlabel('False Positive Rate', fontweight='bold'); ax1.set_ylabel('True Positive Rate', fontweight='bold')\n#     ax1.set_title(f'ROC Curve\\n({positive_class} Detection)', fontweight='bold', pad=15)\n#     ax1.legend(loc=\"lower right\", frameon=True, fancybox=True, shadow=True); ax1.grid(True, alpha=0.3)\n\n#     # Precision-Recall Curve\n#     ax2.plot(recall, precision, color='#A23B72', linewidth=3, label=f'PR Curve (AP = {avg_precision:.4f})')\n#     pos_ratio = np.sum(y_true) / len(y_true) if len(y_true) > 0 else 0 # Handle empty y_true\n#     ax2.axhline(y=pos_ratio, color='gray', linewidth=2, linestyle='--', alpha=0.7,\n#                 label=f'Random Classifier (AP = {pos_ratio:.4f})')\n#     if len(pr_thresholds) > 0: # pr_thresholds can be shorter than precision/recall\n#         optimal_idx_pr = np.argmin(np.abs(pr_thresholds - optimal_threshold))\n#         # Ensure index is valid for precision and recall arrays which are 1 longer than pr_thresholds\n#         # The point on PR curve corresponds to pr_thresholds[optimal_idx_pr],\n#         # and this threshold would be between recall[optimal_idx_pr] and recall[optimal_idx_pr+1]\n#         # For simplicity, we use optimal_idx_pr for recall/precision directly, matching scikit-learn's display.\n#         # It's often recall[optimal_idx_pr+1] and precision[optimal_idx_pr+1] if using threshold index directly.\n#         # However, to plot a point for a specific *threshold value*, find the closest point.\n#         # Let's use the index for the arrays `recall` and `precision` that corresponds to the chosen threshold.\n#         # Since pr_thresholds are used to make decisions, find point just *after* this threshold is applied\n#         # The arrays are `precision[0]...precision[N]`, `recall[0]...recall[N]`, `pr_thresholds[0]...pr_thresholds[N-1]`\n#         # So, if pr_thresholds[i] is chosen, it applies to (recall[i+1], precision[i+1])\n#         if optimal_idx_pr < len(recall) -1 : # Check valid index for recall & precision\n#              ax2.scatter(recall[optimal_idx_pr+1], precision[optimal_idx_pr+1],\n#                         color='red', s=100, zorder=5,\n#                         label=f'Optimal Threshold ({optimal_threshold:.3f})')\n\n#     ax2.set_xlim([0.0, 1.0]); ax2.set_ylim([0.0, 1.05])\n#     ax2.set_xlabel('Recall', fontweight='bold'); ax2.set_ylabel('Precision', fontweight='bold')\n#     ax2.set_title(f'Precision-Recall Curve\\n({positive_class} Detection)', fontweight='bold', pad=15)\n#     ax2.legend(loc=\"lower left\", frameon=True, fancybox=True, shadow=True); ax2.grid(True, alpha=0.3)\n\n#     plt.suptitle(f'Performance Curves {title_suffix}', fontsize=16, fontweight='bold', y=1.02)\n#     plt.tight_layout()\n#     if save_dir:\n#         os.makedirs(save_dir, exist_ok=True)\n#         filename = f\"performance_curves_{config_name}.pdf\"\n#         filepath = os.path.join(save_dir, filename)\n#         plt.savefig(filepath, dpi=300, bbox_inches='tight', format='pdf')\n#         print(f\"Performance curves saved: {filepath}\")\n#     plt.show()\n\n# # --- Evaluation Function (Combining Old Logic with New Visualizations) ---\n# # This is the function Cell 7 should call\n# # CELL 6: Core Training, Evaluation, and Enhanced Visualization Utilities\n# # ... (other parts of Cell 6 remain the same) ...\n\n# # --- Evaluation Function (Combining Old Logic with New Visualizations) ---\n# # This is the function Cell 7 should call\n# def evaluate_model_optimized_with_viz(model, val_ds, test_ds, strategy, inv_label_map,\n#                                      target_metric='f1', dataset_name=\"Test\",\n#                                      save_dir=None, config_name=\"model\"):\n#     \"\"\"\n#     Evaluates the model using logic from old 'evaluate_model_optimized'\n#     and then calls new enhanced visualization functions.\n#     \"\"\"\n#     results_eval = {}\n#     y_true_val_np, y_pred_proba_val_np = None, None\n#     y_true_eval_np, y_pred_proba_eval_np = None, None\n#     # cm_eval_np = None # This was for the old return, not needed here if plots are internal\n#     optimal_threshold = 0.5 # Default\n\n#     print(f\"\\n--- Evaluating Model on {dataset_name} Set ---\")\n#     print(f\"  Using target metric '{target_metric}' for threshold optimization on validation set.\")\n\n#     # 1. Get Compiled Metrics on Evaluation Set (using model.evaluate @ 0.5 threshold)\n#     print(f\"Running model.evaluate() on {dataset_name} set...\")\n#     if test_ds is not None:\n#         try:\n#             eval_results = model.evaluate(test_ds, verbose=0, return_dict=True)\n#             print(\"  Compiled metrics (@ 0.5 Threshold):\")\n#             for k, v in eval_results.items():\n#                 metric_val = float(v) if isinstance(v, (np.float32, np.float64)) else v\n#                 print(f\"    {k}: {metric_val:.4f}\")\n#                 results_eval[k] = metric_val\n#         except Exception as e:\n#             print(f\"  Error during model.evaluate() on {dataset_name} set: {e}\")\n#             print(\"  Skipping compiled metrics evaluation.\")\n#     else:\n#         print(f\"  {dataset_name} dataset not provided. Skipping compiled metrics evaluation.\")\n\n#     # 2. Get Predictions on Validation Set for Threshold Optimization\n#     print(f\"\\nRunning model.predict() on validation set for threshold optimization...\")\n#     if val_ds is not None:\n#         try:\n#             # Collect all true labels from the validation set\n#             y_true_val_list = []\n#             for _, labels_batch in val_ds.unbatch().batch(1024).as_numpy_iterator(): # Efficiently get all labels\n#                 y_true_val_list.append(labels_batch)\n#             if y_true_val_list: y_true_val_np = np.concatenate(y_true_val_list)\n#             print(f\"  Extracted {len(y_true_val_np) if y_true_val_np is not None else 0} validation labels.\")\n\n#             if y_true_val_np is None or len(y_true_val_np) == 0:\n#                 print(\"  Warning: No labels extracted from validation dataset. Cannot optimize threshold.\")\n#             else:\n#                 # Get predictions for the entire validation set\n#                 y_pred_proba_val_np = model.predict(val_ds, verbose=0)\n#                 print(f\"  Generated {len(y_pred_proba_val_np) if y_pred_proba_val_np is not None else 0} validation predictions.\")\n\n#                 if y_true_val_np is not None and y_pred_proba_val_np is not None and len(y_true_val_np) == len(y_pred_proba_val_np):\n#                     if y_pred_proba_val_np.ndim > 1 and y_pred_proba_val_np.shape[1] == 1:\n#                         y_pred_proba_val_np = y_pred_proba_val_np.flatten()\n#                     optimal_threshold = find_optimal_threshold(y_true_val_np, y_pred_proba_val_np, target_metric)\n#                     print(f\"  Optimal threshold determined from validation set: {optimal_threshold:.3f}\") # Increased precision\n#                 else:\n#                     print(f\"  Warning: Mismatch or missing validation labels/predictions. Using default threshold 0.5.\")\n#         except Exception as e:\n#             print(f\"  Error during validation prediction/threshold optimization: {e}\")\n#             # import traceback # Make sure traceback is imported in the cell\n#             # traceback.print_exc() # Optional: for more detailed error\n#             print(\"  Using default threshold 0.5.\")\n#     else:\n#         print(\"  Validation dataset not provided. Using default threshold 0.5 for evaluation set metrics.\")\n\n#     results_eval['optimal_threshold'] = optimal_threshold\n#     results_eval['threshold_target_metric'] = target_metric if val_ds else 'N/A'\n\n\n#     # 3. Get Predictions on Evaluation Set for Detailed Metrics\n#     print(f\"\\nRunning model.predict() on {dataset_name} set for detailed metrics...\")\n#     if test_ds is not None:\n#         try:\n#             # Collect all true labels from the test set\n#             y_true_eval_list = []\n#             for _, labels_batch in test_ds.unbatch().batch(1024).as_numpy_iterator(): # Efficiently get all labels\n#                 y_true_eval_list.append(labels_batch)\n#             if y_true_eval_list: y_true_eval_np = np.concatenate(y_true_eval_list)\n#             print(f\"  Extracted {len(y_true_eval_np) if y_true_eval_np is not None else 0} {dataset_name} labels.\")\n\n#             if y_true_eval_np is None or len(y_true_eval_np) == 0:\n#                 print(f\"  Warning: No labels extracted from {dataset_name} dataset. Cannot calculate detailed metrics.\")\n#             else:\n#                 # Get predictions for the entire test set\n#                 y_pred_proba_eval_np = model.predict(test_ds, verbose=0)\n#                 print(f\"  Generated {len(y_pred_proba_eval_np) if y_pred_proba_eval_np is not None else 0} {dataset_name} predictions.\")\n\n#                 if y_true_eval_np is not None and y_pred_proba_eval_np is not None and len(y_true_eval_np) == len(y_pred_proba_eval_np):\n#                     if y_pred_proba_eval_np.ndim > 1 and y_pred_proba_eval_np.shape[1] == 1:\n#                         y_pred_proba_eval_np = y_pred_proba_eval_np.flatten()\n\n#                     y_pred_eval_optimized = (y_pred_proba_eval_np >= optimal_threshold).astype(int)\n#                     print(f\"\\nDetailed Metrics ({dataset_name} Set @ Optimal Threshold {optimal_threshold:.3f}):\") # Increased precision\n#                     try:\n#                         results_eval['accuracy_opt'] = accuracy_score(y_true_eval_np, y_pred_eval_optimized)\n#                         results_eval['precision_opt'] = precision_score(y_true_eval_np, y_pred_eval_optimized, zero_division=0)\n#                         results_eval['recall_opt'] = recall_score(y_true_eval_np, y_pred_eval_optimized, zero_division=0)\n#                         results_eval['f1_opt'] = f1_score(y_true_eval_np, y_pred_eval_optimized, zero_division=0)\n\n#                         if len(np.unique(y_true_eval_np)) > 1:\n#                             try:\n#                                 results_eval['roc_auc_proba'] = roc_auc_score(y_true_eval_np, y_pred_proba_eval_np)\n#                             except ValueError as auc_e:\n#                                 print(f\"  Warning: Could not calculate ROC AUC score: {auc_e}\")\n#                                 results_eval['roc_auc_proba'] = np.nan\n#                         else:\n#                             results_eval['roc_auc_proba'] = np.nan\n#                             print(\"  Warning: Only one class present in test labels, ROC AUC is undefined.\")\n\n#                         print(f\"  Accuracy (opt):  {results_eval.get('accuracy_opt', np.nan):.4f}\")\n#                         print(f\"  Precision (opt): {results_eval.get('precision_opt', np.nan):.4f}\")\n#                         print(f\"  Recall (opt):    {results_eval.get('recall_opt', np.nan):.4f}\")\n#                         print(f\"  F1 Score (opt):  {results_eval.get('f1_opt', np.nan):.4f}\")\n#                         print(f\"  ROC AUC (proba): {results_eval.get('roc_auc_proba', np.nan):.4f}\")\n\n#                         # cm_eval_np = confusion_matrix(y_true_eval_np, y_pred_eval_optimized) # For old return\n\n#                         print(f\"\\nClassification Report ({dataset_name} Set @ Optimal Threshold {optimal_threshold:.3f}):\") # Increased precision\n#                         num_classes_eval = len(np.unique(y_true_eval_np))\n#                         target_names_report = [inv_label_map.get(i, f\"Class {i}\") for i in sorted(np.unique(y_true_eval_np))]\n#                         if not target_names_report: target_names_report = [inv_label_map.get(0,\"Class 0\"), inv_label_map.get(1,\"Class 1\")] if num_classes_eval == 2 else [\"Unknown\"]\n\n\n#                         print(classification_report(y_true_eval_np, y_pred_eval_optimized, target_names=target_names_report, labels=np.unique(y_true_eval_np), zero_division=0))\n#                     except Exception as metric_e:\n#                         print(f\"  Error calculating detailed metrics: {metric_e}\")\n#                 else:\n#                     print(f\"  Warning: Mismatch or missing {dataset_name} labels/predictions. Cannot calculate detailed metrics.\")\n#         except Exception as e:\n#             print(f\"  Error during {dataset_name} prediction or detailed metrics calculation: {e}\")\n#             # import traceback # Make sure traceback is imported in the cell\n#             # traceback.print_exc() # Optional: for more detailed error\n#     else:\n#         print(f\"  {dataset_name} dataset not provided. Cannot calculate detailed metrics.\")\n\n#     # --- Call Enhanced Visualizations ---\n#     if y_true_eval_np is not None and y_pred_proba_eval_np is not None: # Ensure data exists for plotting\n#         print(\"\\n--- Generating Enhanced Visualizations ---\")\n#         plot_dual_confusion_matrices(\n#             y_true_eval_np, y_pred_proba_eval_np, optimal_threshold, inv_label_map,\n#             title_suffix=f\"({dataset_name} Set, Config: {config_name})\", save_dir=save_dir, config_name=config_name\n#         )\n#         plot_performance_curves(\n#             y_true_eval_np, y_pred_proba_eval_np, optimal_threshold, inv_label_map,\n#             title_suffix=f\"({dataset_name} Set, Config: {config_name})\", save_dir=save_dir, config_name=config_name\n#         )\n#     else:\n#         print(\"\\n--- Skipping Enhanced Visualizations due to missing evaluation data ---\")\n\n#     print(\"--- Evaluation Complete ---\")\n#     return results_eval, y_true_eval_np, y_pred_proba_eval_np, None # cm_eval_np (4th element) is not returned\n\n# # ... (rest of Cell 6, including other plotting functions)\n# # --- Comparison Plot (from your new Cell 6) ---\n# # Renamed to plot_comparison_bars_enhanced to match your new cell 6 naming\n# def plot_comparison_bars_enhanced(results_dict, title, save_dir=None,\n#                                   metric_keys=['f1_opt', 'accuracy_opt', 'roc_auc_proba']):\n#     if not results_dict:\n#         print(f\"No results found for comparison: {title}\")\n#         return None\n#     data_for_plot = {}\n#     for config, metrics in results_dict.items():\n#         if isinstance(metrics, dict) and metrics.get('status') != 'failed': # Exclude failed runs\n#             data_for_plot[config] = {k: metrics['metrics'].get(k, np.nan) for k in metric_keys if 'metrics' in metrics}\n#     if not data_for_plot:\n#         print(\"No valid data for specified metrics in successful runs.\")\n#         return None\n\n#     df = pd.DataFrame(data_for_plot).T.reset_index().rename(columns={'index': 'Configuration'})\n#     sort_metric = 'f1_opt' if 'f1_opt' in df.columns else (metric_keys[0] if metric_keys and metric_keys[0] in df.columns else None)\n#     if sort_metric:\n#         df = df.sort_values(by=sort_metric, ascending=False, na_position='last')\n#     else: # Fallback if no sort metric is available\n#         df = df.sort_values(by='Configuration', ascending=True)\n\n\n#     num_metrics = len([mk for mk in metric_keys if mk in df.columns]) # Count only available metrics for plotting\n#     if num_metrics == 0:\n#         print(\"None of the specified metric_keys are available in the results for plotting.\")\n#         return None\n\n#     fig, axes = plt.subplots(1, num_metrics, figsize=(5 * num_metrics, 8), squeeze=False) # Ensure axes is 2D\n#     axes = axes.flatten()\n\n#     colors = ['#2E86AB', '#A23B72', '#F18F01', '#C73E1D', '#593E2A']\n#     plot_idx = 0\n#     for metric in metric_keys:\n#         if metric not in df.columns:\n#             print(f\"Metric '{metric}' not found in DataFrame columns. Skipping for bar plot.\")\n#             continue\n#         ax = axes[plot_idx]\n#         plot_idx += 1\n\n#         y_pos = np.arange(len(df))\n#         bars = ax.barh(y_pos, df[metric].fillna(0), color=colors[plot_idx-1 % len(colors)], # Fill NaN with 0 for plotting\n#                        alpha=0.8, edgecolor='black', linewidth=0.5)\n#         metric_name = (metric.replace('_', ' ').replace(' opt', ' (Opt)')\n#                        .replace(' proba', '').title())\n#         ax.set_title(metric_name, fontweight='bold', pad=15)\n#         ax.set_xlabel('Score', fontweight='bold')\n#         ax.set_yticks(y_pos)\n#         ax.set_yticklabels(df['Configuration'], fontsize=10)\n#         ax.set_xlim(0, 1.05 if df[metric].max() <=1 else df[metric].max() * 1.1) # Adjust xlim\n#         ax.grid(axis='x', alpha=0.3)\n#         for j, bar_val in enumerate(df[metric]): # Iterate over original values to show NaN if it was NaN\n#             width = bar_val if not np.isnan(bar_val) else 0\n#             text_val = f'{bar_val:.3f}' if not np.isnan(bar_val) else 'N/A'\n#             ax.text(width + 0.01, bars[j].get_y() + bars[j].get_height() / 2,\n#                     text_val, ha='left', va='center',\n#                     fontweight='bold', fontsize=9)\n\n#     for k in range(plot_idx, len(axes)): # Hide unused axes\n#         axes[k].axis('off')\n\n#     plt.suptitle(title, fontsize=16, fontweight='bold', y=0.98)\n#     plt.tight_layout(rect=[0, 0, 1, 0.95]) # Adjust layout\n#     if save_dir:\n#         os.makedirs(save_dir, exist_ok=True)\n#         safe_title = \"\".join(c if c.isalnum() else \"_\" for c in title.lower())\n#         filename = f\"comparison_bars_{safe_title}.pdf\"\n#         filepath = os.path.join(save_dir, filename)\n#         plt.savefig(filepath, dpi=300, bbox_inches='tight', format='pdf')\n#         print(f\"Comparison bar plot saved: {filepath}\")\n#     plt.show()\n#     return df\n\n\n# print(\"Cell 6: All utility functions (training, evaluation, enhanced plotting) are defined.\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-03T15:32:49.856684Z","iopub.execute_input":"2025-06-03T15:32:49.857116Z","iopub.status.idle":"2025-06-03T15:32:49.886663Z","shell.execute_reply.started":"2025-06-03T15:32:49.857074Z","shell.execute_reply":"2025-06-03T15:32:49.8859Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# CELL 6: Core Training, Evaluation, and Enhanced Visualization Utilities\nprint(\"\\n--- Cell 6: Core Training, Evaluation, and Enhanced Visualization Utilities ---\")\n\nimport time\nimport numpy as np\nimport pandas as pd\nimport tensorflow as tf # Ensure tensorflow is imported\nfrom tensorflow import keras # Ensure keras is imported\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nfrom sklearn.metrics import (\n    accuracy_score, precision_score, recall_score, f1_score, # For find_optimal_threshold & evaluate_model\n    roc_auc_score, confusion_matrix, classification_report, # For evaluate_model & plotting\n    roc_curve, # For evaluate_model & plotting\n    precision_recall_curve, average_precision_score # For new plotting\n)\nimport os\nimport traceback # For debugging if needed\nfrom math import ceil\n\n# --- Matplotlib Publication Quality Settings ---\nplt.rcParams.update({\n    'font.size': 11,\n    'font.family': 'serif',\n    'axes.labelsize': 12,\n    'axes.titlesize': 14,\n    'xtick.labelsize': 10,\n    'ytick.labelsize': 10,\n    'legend.fontsize': 11,\n    'figure.titlesize': 16,\n    'axes.linewidth': 1.2,\n    'grid.linewidth': 0.8,\n    'lines.linewidth': 2.0,\n    'patch.linewidth': 0.5,\n    'savefig.dpi': 300,\n    # 'savefig.format': 'pdf', # Default format for savefig can be specified directly in the call\n    'savefig.bbox': 'tight'\n})\n\n# --- Training Function ---\ndef train_model(model, train_ds, val_ds, epochs, class_weights, strategy, learning_rate,\n                initial_epoch=0, callbacks=None, stage_name=\"Training\"):\n    \"\"\"Compiles and trains the model within the strategy scope.\"\"\"\n    if train_ds is None:\n        print(f\"ERROR [{stage_name}]: Training dataset is None. Cannot train.\")\n        return None, None\n\n    with strategy.scope():\n        # IMPORTANT: For F1-score to be in history, add it here.\n        # Ensure 'import tensorflow_addons as tfa' is in Cell 1.\n        metrics_list = [\n            'accuracy',\n            tf.keras.metrics.Precision(name='precision'),\n            tf.keras.metrics.Recall(name='recall'),\n            # tf.keras.metrics.AUC(name='auc')\n            # Example to add F1-score (make sure tfa is imported in Cell 1):\n            tfa.metrics.F1Score(num_classes=1, threshold=0.5, name='f1_score', average='micro'), # For binary\n        ]\n        model.compile(\n            optimizer=keras.optimizers.Adam(learning_rate=learning_rate),\n            loss='binary_crossentropy', # Assuming binary classification from num_classes=1 elsewhere\n            metrics=metrics_list\n        )\n\n    print(f\"\\n--- Starting {stage_name} ---\")\n    print(f\"Epochs: {epochs}, Initial Epoch: {initial_epoch}\")\n    print(f\"Learning Rate: {learning_rate}\")\n    print(f\"Class Weights: {'Applied' if class_weights else 'None'}\")\n\n    start_time = time.time()\n    history = model.fit(\n        train_ds,\n        validation_data=val_ds,\n        epochs=epochs,\n        initial_epoch=initial_epoch,\n        class_weight=class_weights,\n        callbacks=callbacks or [],\n        verbose=1\n    )\n    end_time = time.time()\n    training_duration = end_time - start_time\n    print(f\"--- {stage_name} Finished (Duration: {training_duration:.2f} seconds) ---\")\n    return history, training_duration\n\n# --- Threshold Optimization ---\ndef find_optimal_threshold(y_true, y_pred_proba, target_metric='f1'):\n    \"\"\"Finds the threshold maximizing the target metric on validation predictions.\"\"\"\n    best_threshold = 0.5\n    best_score = -1.0\n\n    if y_true is None or y_pred_proba is None or len(y_true) == 0 or len(y_true) != len(y_pred_proba):\n        print(\"Warning: Invalid inputs for threshold optimization. Returning default 0.5.\")\n        return best_threshold\n\n    y_true = np.array(y_true).astype(int) # Ensure numpy array\n    y_pred_proba = np.array(y_pred_proba) # Ensure numpy array\n\n    thresholds = np.arange(0.01, 1.0, 0.01)\n    scores = []\n\n    for thresh in thresholds:\n        y_pred_binary = (y_pred_proba >= thresh).astype(int)\n        if target_metric == 'f1':\n            score = f1_score(y_true, y_pred_binary, zero_division=0)\n        elif target_metric == 'accuracy':\n            score = accuracy_score(y_true, y_pred_binary)\n        elif target_metric == 'precision':\n            score = precision_score(y_true, y_pred_binary, zero_division=0)\n        elif target_metric == 'recall':\n            score = recall_score(y_true, y_pred_binary, zero_division=0)\n        else: # Default to F1\n            print(f\"Warning: Unknown target_metric '{target_metric}' for threshold optimization. Defaulting to F1.\")\n            score = f1_score(y_true, y_pred_binary, zero_division=0)\n        scores.append(score)\n\n    if scores:\n        best_idx = np.argmax(scores)\n        best_score = scores[best_idx]\n        best_threshold = thresholds[best_idx]\n    else:\n        print(\"Warning: Could not compute scores for threshold optimization. Using default 0.5.\")\n    # print(f\"Best threshold for '{target_metric}': {best_threshold:.3f} with score: {best_score:.4f}\") # Optional debug\n    return best_threshold\n\n# --- Enhanced Plotting Functions ---\ndef plot_training_history_enhanced(history, title_suffix=\"\", save_dir=None, config_name=\"model\"):\n    if history is None or not history.history:\n        print(\"No history data found to plot.\")\n        return\n    history_df = pd.DataFrame(history.history)\n    history_df['epoch'] = np.arange(1, len(history_df) + 1)\n\n    potential_metrics_map = {\n        'loss': 'Loss', 'accuracy': 'Accuracy', 'precision': 'Precision', 'recall': 'Recall',\n        'f1_score': 'F1-score', 'f1': 'F1-score', # Common keys for F1\n        'auc': 'AUC'\n    }\n    available_history_keys = list(history_df.columns)\n    metrics_for_plotting = []\n\n    # Add base metrics if available\n    for key in ['loss', 'accuracy', 'precision', 'recall']:\n        if key in available_history_keys:\n            metrics_for_plotting.append({'key': key, 'name': potential_metrics_map.get(key, key.title())})\n\n    # Determine 5th metric: F1-score (preferred) or AUC\n    f1_key_to_use = None\n    if 'f1_score' in available_history_keys: f1_key_to_use = 'f1_score'\n    elif 'f1' in available_history_keys: f1_key_to_use = 'f1'\n\n    if f1_key_to_use:\n        if len(metrics_for_plotting) < 5:\n            metrics_for_plotting.append({'key': f1_key_to_use, 'name': potential_metrics_map.get(f1_key_to_use, 'F1-score')})\n    elif 'auc' in available_history_keys: # If F1 not found, try AUC\n        if len(metrics_for_plotting) < 5:\n            metrics_for_plotting.append({'key': 'auc', 'name': potential_metrics_map.get('auc', 'AUC')})\n    \n    num_actual_plots = len(metrics_for_plotting)\n    if num_actual_plots == 0:\n        print(\"No plottable metrics found in history.\")\n        return\n\n    fig = plt.figure() # Initialize figure; size set below\n    subplot_definitions = []\n\n    if num_actual_plots == 5:\n        fig.set_size_inches(18, 10) \n        gs_fig = fig.add_gridspec(2, 6, hspace=0.45, wspace=0.5) # hspace for title, wspace for between plots\n        subplot_definitions = [\n            gs_fig[0, 0:2], gs_fig[0, 2:4], gs_fig[0, 4:6], \n            gs_fig[1, 1:3], gs_fig[1, 3:5]                  \n        ]\n    elif num_actual_plots == 4:\n        fig.set_size_inches(12, 10) \n        gs_fig = fig.add_gridspec(2, 2, hspace=0.35, wspace=0.3)\n        subplot_definitions = [gs_fig[0,0], gs_fig[0,1], gs_fig[1,0], gs_fig[1,1]]\n    elif num_actual_plots > 0: # 1, 2, or 3 plots\n        fig.set_size_inches(6 * num_actual_plots, 5.5) \n        gs_fig = fig.add_gridspec(1, num_actual_plots, hspace=0.3, wspace=0.25 if num_actual_plots > 1 else 0)\n        subplot_definitions = [gs_fig[0,i] for i in range(num_actual_plots)]\n    else: # Should be caught by num_actual_plots == 0\n        return\n\n    colors = {'train': '#2E86AB', 'val': '#A23B72'}\n    for idx, metric_info in enumerate(metrics_for_plotting):\n        metric_key = metric_info['key']\n        display_name = metric_info['name']\n        \n        ax = fig.add_subplot(subplot_definitions[idx])\n        val_metric_key = f'val_{metric_key}'\n\n        if metric_key in history_df.columns:\n            ax.plot(history_df['epoch'], history_df[metric_key],\n                    color=colors['train'], marker='o', markersize=3,\n                    label='Training', linewidth=2)\n        if val_metric_key in history_df.columns:\n            ax.plot(history_df['epoch'], history_df[val_metric_key],\n                    color=colors['val'], marker='*', markersize=3,\n                    label='Validation', linewidth=2, linestyle='--')\n        \n        ax.set_title(display_name, fontweight='bold', pad=10)\n        ax.set_xlabel('Epoch')\n        ax.set_ylabel('Value')\n        ax.grid(True, alpha=0.3)\n        ax.legend(frameon=True, fancybox=True, shadow=True)\n        \n        if metric_key not in ['loss']:\n            ax.set_ylim(-0.05, 1.05) # Allow slight dip below 0 for visual\n        else: \n            min_val_data = history_df[metric_key].dropna()\n            max_val_data = history_df[metric_key].dropna()\n            if val_metric_key in history_df and not history_df[val_metric_key].dropna().empty:\n                min_val_data = pd.concat([min_val_data, history_df[val_metric_key].dropna()])\n                max_val_data = pd.concat([max_val_data, history_df[val_metric_key].dropna()])\n            \n            min_loss = min_val_data.min() if not min_val_data.empty else 0\n            max_loss = max_val_data.max() if not max_val_data.empty else 1.0 # Ensure max_loss is float\n            \n            padding_abs = 0.1 \n            if pd.notna(min_loss) and pd.notna(max_loss) and (max_loss - min_loss) > 1e-5 :\n                 padding = 0.1 * (max_loss - min_loss)\n                 padding = max(padding, 0.05) # ensure some minimal padding\n            else:\n                 padding = padding_abs\n            \n            y_min_plot = float(min_loss - padding) if pd.isna(min_loss) else float(max(0, min_loss - padding))\n            y_max_plot = float(max_loss + padding) if pd.notna(max_loss) else float(y_min_plot + 2*padding_abs)\n            if y_max_plot <= y_min_plot: y_max_plot = y_min_plot + padding_abs # Ensure max > min\n\n            ax.set_ylim(y_min_plot, y_max_plot)\n\n    fig.suptitle(f'Training History {title_suffix}', fontsize=16, fontweight='bold')\n    plt.tight_layout(rect=[0, 0.03, 1, 0.95]) \n\n    if save_dir:\n        os.makedirs(save_dir, exist_ok=True)\n        filename = f\"training_history_{config_name}.pdf\"\n        filepath = os.path.join(save_dir, filename)\n        plt.savefig(filepath, dpi=300, bbox_inches='tight', format='pdf')\n        print(f\"Training history saved: {filepath}\")\n    plt.show()\n\ndef plot_dual_confusion_matrices(y_true, y_pred_proba, optimal_threshold, inv_label_map,\n                                 title_suffix=\"\", save_dir=None, config_name=\"model\"):\n    if y_true is None or y_pred_proba is None:\n        print(\"Cannot plot confusion matrices: Missing data\")\n        return\n    y_true = np.array(y_true).astype(int)\n    y_pred_proba = np.array(y_pred_proba)\n\n    y_pred_default = (y_pred_proba >= 0.5).astype(int)\n    y_pred_optimal = (y_pred_proba >= optimal_threshold).astype(int)\n    \n    cm_default = confusion_matrix(y_true, y_pred_default)\n    cm_optimal = confusion_matrix(y_true, y_pred_optimal)\n    \n    # Determine class names robustly\n    unique_labels = sorted(np.unique(y_true))\n    if not inv_label_map or not all(lbl in inv_label_map for lbl in unique_labels) :\n        class_names = [f\"Class {i}\" for i in unique_labels]\n    else:\n        class_names = [inv_label_map.get(lbl, f\"Class {lbl}\") for lbl in unique_labels]\n    if not class_names: class_names = [\"Class 0\", \"Class 1\"] # Fallback\n\n    fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(14, 6.5)) # Slightly taller\n    sns.heatmap(cm_default, annot=True, fmt='d', cmap='Blues',\n                xticklabels=class_names, yticklabels=class_names,\n                ax=ax1, cbar_kws={'shrink': 0.8}, square=True,\n                annot_kws={'size': 14, 'weight': 'bold'})\n    ax1.set_title('Confusion Matrix\\n(Threshold = 0.5)', fontweight='bold', pad=15)\n    ax1.set_xlabel('Predicted Label', fontweight='bold'); ax1.set_ylabel('True Label', fontweight='bold')\n\n    sns.heatmap(cm_optimal, annot=True, fmt='d', cmap='Greens',\n                xticklabels=class_names, yticklabels=class_names,\n                ax=ax2, cbar_kws={'shrink': 0.8}, square=True,\n                annot_kws={'size': 14, 'weight': 'bold'})\n    ax2.set_title(f'Confusion Matrix\\n(Optimal Threshold = {optimal_threshold:.3f})',\n                  fontweight='bold', pad=15)\n    ax2.set_xlabel('Predicted Label', fontweight='bold'); ax2.set_ylabel('True Label', fontweight='bold')\n    \n    plt.suptitle(f'Confusion Matrix Comparison {title_suffix}',\n                 fontsize=16, fontweight='bold', y=1.0) # Adjusted y\n    plt.tight_layout(rect=[0, 0, 1, 0.95]) # Make space for suptitle\n    if save_dir:\n        os.makedirs(save_dir, exist_ok=True)\n        filename = f\"confusion_matrices_{config_name}.pdf\"\n        filepath = os.path.join(save_dir, filename)\n        plt.savefig(filepath, dpi=300, bbox_inches='tight', format='pdf')\n        print(f\"Confusion matrices saved: {filepath}\")\n    plt.show()\n\ndef plot_performance_curves(y_true, y_pred_proba, optimal_threshold, inv_label_map,\n                            title_suffix=\"\", save_dir=None, config_name=\"model\"):\n    if y_true is None or y_pred_proba is None:\n        print(\"Cannot plot performance curves: Missing data\")\n        return\n    y_true = np.array(y_true).astype(int)\n    y_pred_proba = np.array(y_pred_proba)\n\n    if len(np.unique(y_true)) < 2:\n        print(\"Cannot plot ROC/PR curves: Only one class present in true labels.\")\n        return\n\n    fpr, tpr, roc_thresholds = roc_curve(y_true, y_pred_proba)\n    roc_auc = roc_auc_score(y_true, y_pred_proba)\n    precision_vals, recall_vals, pr_thresholds = precision_recall_curve(y_true, y_pred_proba)\n    avg_precision = average_precision_score(y_true, y_pred_proba)\n    \n    positive_class_label = 1 # Assuming positive class is 1 for binary\n    positive_class_name = inv_label_map.get(positive_class_label, \"Positive Class\") if isinstance(inv_label_map, dict) else \"Positive Class\"\n\n\n    fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(14, 6.5)) # Slightly taller\n    # ROC Curve\n    ax1.plot(fpr, tpr, color='#2E86AB', linewidth=3, label=f'ROC Curve (AUC = {roc_auc:.4f})')\n    ax1.plot([0, 1], [0, 1], color='gray', linewidth=2, linestyle='--', alpha=0.7, label='Random Classifier')\n    \n    # Find point for optimal threshold on ROC curve\n    # roc_thresholds also includes some high values like inf, so be careful with argmin\n    if len(roc_thresholds) > 0:\n        optimal_idx_roc = np.argmin(np.abs(roc_thresholds - optimal_threshold))\n        # Ensure index is valid for fpr and tpr arrays\n        if optimal_idx_roc < len(fpr) and optimal_idx_roc < len(tpr):\n             ax1.scatter(fpr[optimal_idx_roc], tpr[optimal_idx_roc], color='red', s=100,\n                        zorder=5, label=f'Optimal Threshold ({optimal_threshold:.3f})')\n\n    ax1.set_xlim([-0.02, 1.0]); ax1.set_ylim([0.0, 1.05]) # Start x slightly before 0\n    ax1.set_xlabel('False Positive Rate', fontweight='bold'); ax1.set_ylabel('True Positive Rate', fontweight='bold')\n    ax1.set_title(f'ROC Curve\\n({positive_class_name} Detection)', fontweight='bold', pad=15)\n    ax1.legend(loc=\"lower right\", frameon=True, fancybox=True, shadow=True); ax1.grid(True, alpha=0.3)\n\n    # Precision-Recall Curve\n    ax2.plot(recall_vals, precision_vals, color='#A23B72', linewidth=3, label=f'PR Curve (AP = {avg_precision:.4f})')\n    no_skill = len(y_true[y_true==positive_class_label]) / len(y_true) if len(y_true) > 0 else 0\n    ax2.axhline(y=no_skill, color='gray', linewidth=2, linestyle='--', alpha=0.7,\n                label=f'No-Skill Classifier (AP = {no_skill:.4f})')\n\n    # Find point for optimal threshold on PR curve\n    # pr_thresholds is shorter by 1 than precision and recall.\n    # It corresponds to decisions made *between* points on the curve.\n    # So, pr_thresholds[i] is the threshold used to get recall_vals[i+1] and precision_vals[i+1]\n    if len(pr_thresholds) > 0:\n        # Find index in pr_thresholds closest to optimal_threshold\n        optimal_idx_pr_thresh = np.argmin(np.abs(pr_thresholds - optimal_threshold))\n        # The corresponding point on PR curve is at index optimal_idx_pr_thresh + 1 for recall and precision\n        # Ensure this index is valid for recall_vals and precision_vals\n        point_idx_pr = optimal_idx_pr_thresh +1\n        if point_idx_pr < len(recall_vals) and point_idx_pr < len(precision_vals):\n            ax2.scatter(recall_vals[point_idx_pr], precision_vals[point_idx_pr],\n                        color='red', s=100, zorder=5,\n                        label=f'Optimal Threshold ({optimal_threshold:.3f})')\n\n    ax2.set_xlim([0.0, 1.02]); ax2.set_ylim([0.0, 1.05]) # End x slightly after 1\n    ax2.set_xlabel('Recall', fontweight='bold'); ax2.set_ylabel('Precision', fontweight='bold')\n    ax2.set_title(f'Precision-Recall Curve\\n({positive_class_name} Detection)', fontweight='bold', pad=15)\n    ax2.legend(loc=\"lower left\", frameon=True, fancybox=True, shadow=True); ax2.grid(True, alpha=0.3)\n\n    plt.suptitle(f'Performance Curves {title_suffix}', fontsize=16, fontweight='bold', y=1.0) # Adjusted y\n    plt.tight_layout(rect=[0, 0, 1, 0.95])\n    if save_dir:\n        os.makedirs(save_dir, exist_ok=True)\n        filename = f\"performance_curves_{config_name}.pdf\"\n        filepath = os.path.join(save_dir, filename)\n        plt.savefig(filepath, dpi=300, bbox_inches='tight', format='pdf')\n        print(f\"Performance curves saved: {filepath}\")\n    plt.show()\n\n# --- Evaluation Function (Fixed model.predict calls) ---\n# In your consolidated Cell 6:\n# Ensure these imports are at the top of Cell 6 if not already:\n# from sklearn.metrics import average_precision_score, roc_auc_score, ... (other metrics)\n# import numpy as np\n\ndef evaluate_model_optimized_with_viz(model, val_ds, test_ds, strategy, inv_label_map,\n                                      target_metric='f1', dataset_name=\"Test\",\n                                      save_dir=None, config_name=\"model\"):\n    \"\"\"\n    Evaluates the model using logic from old 'evaluate_model_optimized'\n    and then calls new enhanced visualization functions. NOW INCLUDES PR AUC.\n    \"\"\"\n    results_eval = {}\n    y_true_val_np, y_pred_proba_val_np = None, None\n    y_true_eval_np, y_pred_proba_eval_np = None, None\n    # cm_eval_np = None # Not returned by this version as run_experiment doesn't use it directly\n    optimal_threshold = 0.5 \n\n    print(f\"\\n--- Evaluating Model on {dataset_name} Set ---\")\n    print(f\"  Using target metric '{target_metric}' for threshold optimization on validation set.\")\n\n    # 1. Get Compiled Metrics on Evaluation Set (using model.evaluate @ 0.5 threshold)\n    print(f\"Running model.evaluate() on {dataset_name} set...\")\n    if test_ds is not None:\n        try:\n            eval_results_tf = model.evaluate(test_ds, verbose=0, return_dict=True) # Renamed to avoid clash\n            print(\"  Compiled metrics (@ 0.5 Threshold from model.evaluate):\")\n            for k, v in eval_results_tf.items():\n                metric_val = float(v) if isinstance(v, (np.float32, np.float64)) else v\n                print(f\"    {k}: {metric_val:.4f}\")\n                results_eval[k] = metric_val # Stores loss, accuracy, precision, recall, auc\n        except Exception as e:\n            print(f\"  Error during model.evaluate() on {dataset_name} set: {e}\")\n            print(\"  Skipping compiled metrics evaluation from model.evaluate.\")\n    else:\n        print(f\"  {dataset_name} dataset not provided. Skipping compiled metrics evaluation from model.evaluate.\")\n\n    # 2. Get Predictions on Validation Set for Threshold Optimization\n    # ... (this part remains the same - it finds optimal_threshold) ...\n    print(f\"\\nRunning model.predict() on validation set for threshold optimization...\")\n    if val_ds is not None:\n        try:\n            # Efficiently get all labels and predictions\n            y_true_val_list_batches = []\n            y_pred_proba_val_list_batches = []\n            print(\"  Extracting true labels and making predictions on validation set...\")\n            for images_batch_val, labels_batch_val in val_ds.as_numpy_iterator(): # Iterate once\n                y_true_val_list_batches.append(labels_batch_val)\n                y_pred_proba_val_list_batches.append(model.predict(images_batch_val, verbose=0))\n\n            if y_true_val_list_batches:\n                y_true_val_np = np.concatenate([item.flatten() for item in y_true_val_list_batches])\n                y_pred_proba_val_np = np.concatenate([item.flatten() for item in y_pred_proba_val_list_batches])\n                print(f\"  Extracted {len(y_true_val_np)} validation labels and made {len(y_pred_proba_val_np)} predictions.\")\n\n                if len(y_true_val_np) == len(y_pred_proba_val_np) and len(y_true_val_np) > 0:\n                    optimal_threshold = find_optimal_threshold(y_true_val_np, y_pred_proba_val_np, target_metric)\n                    print(f\"  Optimal threshold determined from validation set: {optimal_threshold:.3f}\")\n                else:\n                    print(f\"  Warning: Mismatch or empty validation labels/predictions ({len(y_true_val_np)} vs {len(y_pred_proba_val_np)}). Using default threshold 0.5.\")\n                    optimal_threshold = 0.5\n            else:\n                print(\"  Warning: No labels/data extracted from validation dataset. Using default threshold 0.5.\")\n                optimal_threshold = 0.5\n        except Exception as e:\n            print(f\"  Error during validation prediction/threshold optimization: {e}\")\n            # import traceback\n            # traceback.print_exc()\n            print(\"  Using default threshold 0.5.\")\n            optimal_threshold = 0.5\n    else:\n        print(\"  Validation dataset not provided. Using default threshold 0.5 for evaluation set metrics.\")\n        optimal_threshold = 0.5\n    results_eval['optimal_threshold'] = optimal_threshold\n    results_eval['threshold_target_metric'] = target_metric if val_ds else 'N/A'\n\n\n    # 3. Get Predictions on Evaluation Set (e.g., test_ds) for Detailed Metrics\n    print(f\"\\nRunning model.predict() on {dataset_name} set for detailed metrics...\")\n    if test_ds is not None:\n        try:\n            y_true_eval_list_batches = []\n            y_pred_proba_eval_list_batches = []\n            print(f\"  Extracting true labels and making predictions on {dataset_name} set...\")\n            for images_batch_eval, labels_batch_eval in test_ds.as_numpy_iterator(): # Iterate once\n                y_true_eval_list_batches.append(labels_batch_eval)\n                y_pred_proba_eval_list_batches.append(model.predict(images_batch_eval, verbose=0))\n\n            if y_true_eval_list_batches:\n                y_true_eval_np = np.concatenate([item.flatten() for item in y_true_eval_list_batches])\n                y_pred_proba_eval_np = np.concatenate([item.flatten() for item in y_pred_proba_eval_list_batches])\n                print(f\"  Extracted {len(y_true_eval_np)} {dataset_name} labels and made {len(y_pred_proba_eval_np)} predictions.\")\n\n                if len(y_true_eval_np) == len(y_pred_proba_eval_np) and len(y_true_eval_np) > 0:\n                    y_pred_eval_optimized = (y_pred_proba_eval_np >= optimal_threshold).astype(int)\n                    print(f\"\\nDetailed Metrics ({dataset_name} Set @ Optimal Threshold {optimal_threshold:.3f}):\")\n                    try:\n                        results_eval['accuracy_opt'] = accuracy_score(y_true_eval_np, y_pred_eval_optimized)\n                        results_eval['precision_opt'] = precision_score(y_true_eval_np, y_pred_eval_optimized, zero_division=0)\n                        results_eval['recall_opt'] = recall_score(y_true_eval_np, y_pred_eval_optimized, zero_division=0)\n                        results_eval['f1_opt'] = f1_score(y_true_eval_np, y_pred_eval_optimized, zero_division=0)\n\n                        if len(np.unique(y_true_eval_np)) > 1: # Need at least two classes for AUCs\n                            try:\n                                results_eval['roc_auc_proba'] = roc_auc_score(y_true_eval_np, y_pred_proba_eval_np)\n                                # --- ADDED PR AUC (Average Precision) ---\n                                results_eval['pr_auc'] = average_precision_score(y_true_eval_np, y_pred_proba_eval_np)\n                                # --------------------------------------\n                            except ValueError as auc_e:\n                                print(f\"  Warning: Could not calculate ROC AUC or PR AUC scores: {auc_e}\")\n                                results_eval['roc_auc_proba'] = np.nan\n                                results_eval['pr_auc'] = np.nan # Also set PR AUC to NaN\n                        else:\n                            results_eval['roc_auc_proba'] = np.nan\n                            results_eval['pr_auc'] = np.nan # Also set PR AUC to NaN for single class\n                            print(\"  Warning: Only one class present in test labels, ROC AUC and PR AUC are undefined.\")\n\n                        print(f\"  Accuracy (opt):  {results_eval.get('accuracy_opt', np.nan):.4f}\")\n                        print(f\"  Precision (opt): {results_eval.get('precision_opt', np.nan):.4f}\")\n                        print(f\"  Recall (opt):    {results_eval.get('recall_opt', np.nan):.4f}\")\n                        print(f\"  F1 Score (opt):  {results_eval.get('f1_opt', np.nan):.4f}\")\n                        print(f\"  ROC AUC (proba): {results_eval.get('roc_auc_proba', np.nan):.4f}\")\n                        # --- ADDED PR AUC PRINT ---\n                        print(f\"  PR AUC (AvgPrec):{results_eval.get('pr_auc', np.nan):.4f}\")\n                        # -------------------------\n\n                        cm_eval_np = confusion_matrix(y_true_eval_np, y_pred_eval_optimized)\n                        print(f\"\\nClassification Report ({dataset_name} Set @ Optimal Threshold {optimal_threshold:.3f}):\")\n                        num_classes_eval = len(np.unique(y_true_eval_np))\n                        target_names_report = [inv_label_map.get(i, f\"Class {i}\") for i in range(num_classes_eval)] if num_classes_eval == 2 else [inv_label_map.get(c, f\"Class {c}\") for c in sorted(np.unique(y_true_eval_np))]\n                        if not target_names_report: target_names_report = [\"Unknown\"]\n                        print(classification_report(y_true_eval_np, y_pred_eval_optimized, target_names=target_names_report, labels=np.unique(y_true_eval_np), zero_division=0))\n                    except Exception as metric_e:\n                        print(f\"  Error calculating detailed sklearn metrics: {metric_e}\")\n                else:\n                    print(f\"  Warning: Mismatch or empty {dataset_name} labels/predictions. Cannot calculate detailed sklearn metrics.\")\n            else:\n                print(f\"  Warning: No labels/data extracted from {dataset_name} dataset. Cannot calculate detailed sklearn metrics.\")\n        except Exception as e:\n            print(f\"  Error during {dataset_name} prediction or detailed metrics calculation: {e}\")\n            # import traceback\n            # traceback.print_exc()\n    else:\n        print(f\"  {dataset_name} dataset not provided. Cannot calculate detailed sklearn metrics.\")\n\n    # --- Call Enhanced Visualizations ---\n    if y_true_eval_np is not None and y_pred_proba_eval_np is not None:\n        print(\"\\n--- Generating Enhanced Visualizations ---\")\n        # These plotting functions already calculate ROC AUC and Avg Precision internally for their plots\n        plot_dual_confusion_matrices( \n            y_true_eval_np, y_pred_proba_eval_np, optimal_threshold, inv_label_map,\n            title_suffix=f\"({dataset_name} Set, Config: {config_name})\", save_dir=save_dir, config_name=config_name\n        )\n        plot_performance_curves(\n            y_true_eval_np, y_pred_proba_eval_np, optimal_threshold, inv_label_map,\n            title_suffix=f\"({dataset_name} Set, Config: {config_name})\", save_dir=save_dir, config_name=config_name\n        )\n    else:\n        print(\"\\n--- Skipping Enhanced Visualizations due to missing evaluation data ---\")\n\n    print(\"--- Evaluation Complete ---\")\n    return results_eval, y_true_eval_np, y_pred_proba_eval_np, None # cm_eval_np is not directly needed by run_experiment\n\n# In your consolidated Cell 6:\n# def plot_comparison_bars_enhanced(config_keys_to_plot, metrics_dir, title, save_dir=None,\n#                                   metrics_to_display=['f1_opt', 'accuracy_opt', 'roc_auc_proba', 'pr_auc']):\n#     \"\"\"\n#     Creates publication-quality comparison bar charts by reading metrics from saved JSON files.\n#     Removes top and right spines from subplots for a cleaner look.\n\n#     Args:\n#         config_keys_to_plot (list): List of configuration keys (strings) to load and plot.\n#         metrics_dir (str): Path to the directory containing the \"evaluation_metrics_{key}.json\" files.\n#         title (str): Main title for the plot.\n#         save_dir (str, optional): Directory to save the plot PDF. Defaults to None (no save).\n#         metrics_to_display (list, optional): List of metric keys (strings) from the JSON files\n#                                              to extract and plot.\n#     Returns:\n#         pandas.DataFrame: DataFrame containing the plotted data, or None if plotting failed.\n#     \"\"\"\n#     if not config_keys_to_plot:\n#         print(f\"No configuration keys provided for comparison plot: {title}\")\n#         return None\n#     if not os.path.isdir(metrics_dir):\n#         print(f\"Metrics directory not found: {metrics_dir}\")\n#         return None\n\n#     data_for_plot = {}\n#     for config_key in config_keys_to_plot:\n#         json_filename = f\"evaluation_metrics_{config_key}.json\"\n#         json_path = os.path.join(metrics_dir, json_filename)\n#         if os.path.exists(json_path):\n#             try:\n#                 with open(json_path, 'r') as f:\n#                     metrics_from_json = json.load(f)\n#                 temp_metrics_for_this_config = {}\n#                 has_at_least_one_valid_value = False\n#                 for metric_name in metrics_to_display:\n#                     value = metrics_from_json.get(metric_name, np.nan)\n#                     temp_metrics_for_this_config[metric_name] = value\n#                     if not pd.isna(value):\n#                         has_at_least_one_valid_value = True\n#                 if has_at_least_one_valid_value:\n#                     data_for_plot[config_key] = temp_metrics_for_this_config\n#             except Exception as e:\n#                 print(f\"Error loading or parsing JSON for config '{config_key}' from {json_path}: {e}\")\n\n#     if not data_for_plot:\n#         print(f\"No valid data could be loaded from JSON files for the specified configurations and metrics ({metrics_to_display}) for plot: {title}\")\n#         return None\n\n#     df = pd.DataFrame(data_for_plot).T.reset_index().rename(columns={'index': 'Configuration'})\n#     if df.empty:\n#         print(f\"DataFrame is empty after processing JSON files for plot: {title}\")\n#         return None\n\n#     plottable_metric_keys = [mk for mk in metrics_to_display if mk in df.columns and not df[mk].isnull().all()]\n#     if not plottable_metric_keys:\n#         print(f\"None of the specified metrics_to_display ({metrics_to_display}) have any valid data in the loaded JSONs for plotting: {title}\")\n#         return df\n\n#     sort_metric = None\n#     if 'f1_opt' in plottable_metric_keys: sort_metric = 'f1_opt'\n#     elif plottable_metric_keys: sort_metric = plottable_metric_keys[0]\n#     if sort_metric: df = df.sort_values(by=sort_metric, ascending=False, na_position='last')\n#     else: df = df.sort_values(by='Configuration', ascending=True)\n\n#     num_metrics_to_plot = len(plottable_metric_keys)\n#     ncols = min(num_metrics_to_plot, 3)\n#     nrows = ceil(num_metrics_to_plot / ncols)\n#     fig, axes = plt.subplots(nrows, ncols, figsize=(5 * ncols, max(4, 0.6 * len(df) + 1.5)), squeeze=False)\n#     axes = axes.flatten()\n#     colors = ['#2E86AB', '#A23B72', '#F18F01', '#C73E1D', '#593E2A', '#3A7D44', '#B565A7']\n    \n#     plot_idx = 0\n#     for metric in plottable_metric_keys:\n#         ax = axes[plot_idx]\n        \n#         y_pos = np.arange(len(df))\n#         bar_values = df[metric].fillna(0) \n#         bars = ax.barh(y_pos, bar_values, color=colors[plot_idx % len(colors)],\n#                        alpha=0.85, edgecolor='black', linewidth=0.7)\n        \n#         if metric == 'pr_auc': metric_name_display = \"PR AUC (AvgPrec)\"\n#         elif metric == 'roc_auc_proba': metric_name_display = \"ROC AUC\"\n#         else: metric_name_display = (metric.replace('_', ' ').replace(' opt', ' (Opt)').title())\n            \n#         ax.set_title(metric_name_display, fontweight='bold', pad=12, fontsize=12)\n#         ax.set_xlabel('Score', fontweight='bold', fontsize=10)\n#         ax.set_yticks(y_pos)\n#         ax.set_yticklabels(df['Configuration'], fontsize=9)\n        \n#         max_val_metric = df[metric].max()\n#         if pd.isna(max_val_metric) or max_val_metric == 0 : upper_limit = 0.1 \n#         elif max_val_metric <= 1.0: upper_limit = 1.05\n#         else: upper_limit = max_val_metric * 1.15\n#         ax.set_xlim(0, upper_limit)\n#         ax.tick_params(axis='x', labelsize=8)\n#         ax.grid(axis='x', linestyle=':', alpha=0.6)\n\n#         # --- MODIFICATION: Remove top and right spines ---\n#         ax.spines['top'].set_visible(False)\n#         ax.spines['right'].set_visible(False)\n#         # You could also use sns.despine(ax=ax, top=True, right=True, left=False, bottom=False)\n#         # If you also want to remove left/bottom, set them to True in despine or use ax.spines['left'].set_visible(False) etc.\n#         # -------------------------------------------------\n\n#         for bar_idx, bar_obj in enumerate(bars):\n#             original_value = df[metric].iloc[bar_idx]\n#             width_for_text = bar_obj.get_width()\n#             text_label = f'{original_value:.3f}' if not pd.isna(original_value) else 'N/A'\n            \n#             # Adjust text position slightly if needed, especially after removing spines\n#             padding_from_bar = ax.get_xlim()[1] * 0.015 # Slightly increased padding\n#             ax.text(width_for_text + padding_from_bar, \n#                     bar_obj.get_y() + bar_obj.get_height() / 2,\n#                     text_label, ha='left', va='center',\n#                     fontweight='normal', fontsize=8.5, color='dimgray')\n#         plot_idx += 1\n\n#     for k_ax in range(plot_idx, len(axes)): axes[k_ax].axis('off')\n#     fig.suptitle(title, fontsize=16, fontweight='bold', y=0.99 if nrows > 1 else 1.02)\n#     plt.tight_layout(rect=[0, 0.03, 1, 0.95 if nrows > 1 else 0.92])\n\n#     if save_dir:\n#         os.makedirs(save_dir, exist_ok=True)\n#         safe_title = \"\".join(c if c.isalnum() else \"_\" for c in title.lower())\n#         filename = f\"comparison_bars_{safe_title}.pdf\"\n#         filepath = os.path.join(save_dir, filename)\n#         try:\n#             plt.savefig(filepath, dpi=300, bbox_inches='tight', format='pdf')\n#             print(f\"Comparison bar plot saved: {filepath}\")\n#         except Exception as e:\n#             print(f\"Error saving comparison bar plot to {filepath}: {e}\")\n    \n#     plt.show()\n#     return df\ndef plot_comparison_bars_enhanced(config_keys_to_plot, metrics_dir, title, save_dir=None,\n                                  metrics_to_display=['f1_opt', 'accuracy_opt', 'roc_auc_proba', 'pr_auc']):\n    \"\"\"\n    Creates publication-quality comparison bar charts by reading metrics from saved JSON files.\n    Removes top and right spines from subplots for a cleaner look.\n\n    Args:\n        config_keys_to_plot (list): List of configuration keys (strings) to load and plot.\n        metrics_dir (str): Path to the directory containing the \"evaluation_metrics_{key}.json\" files.\n        title (str): Main title for the plot.\n        save_dir (str, optional): Directory to save the plot PDF. Defaults to None (no save).\n        metrics_to_display (list, optional): List of metric keys (strings) from the JSON files\n                                             to extract and plot.\n    Returns:\n        pandas.DataFrame: DataFrame containing the plotted data, or None if plotting failed.\n    \"\"\"\n    if not config_keys_to_plot:\n        print(f\"No configuration keys provided for comparison plot: {title}\")\n        return None\n    if not os.path.isdir(metrics_dir):\n        print(f\"Metrics directory not found: {metrics_dir}\")\n        return None\n\n    data_for_plot = {}\n    for config_key in config_keys_to_plot:\n        json_filename = f\"evaluation_metrics_{config_key}.json\"\n        json_path = os.path.join(metrics_dir, json_filename)\n        if os.path.exists(json_path):\n            try:\n                with open(json_path, 'r') as f:\n                    metrics_from_json = json.load(f)\n                temp_metrics_for_this_config = {}\n                has_at_least_one_valid_value = False\n                for metric_name in metrics_to_display:\n                    value = metrics_from_json.get(metric_name, np.nan)\n                    temp_metrics_for_this_config[metric_name] = value\n                    if not pd.isna(value):\n                        has_at_least_one_valid_value = True\n                if has_at_least_one_valid_value:\n                    data_for_plot[config_key] = temp_metrics_for_this_config\n            except Exception as e:\n                print(f\"Error loading or parsing JSON for config '{config_key}' from {json_path}: {e}\")\n\n    if not data_for_plot:\n        print(f\"No valid data could be loaded from JSON files for the specified configurations and metrics ({metrics_to_display}) for plot: {title}\")\n        return None\n\n    df = pd.DataFrame(data_for_plot).T.reset_index().rename(columns={'index': 'Configuration'})\n    df['Short Configuration'] = df['Configuration'].apply(lambda x: '_'.join(x.split('_')[-2:]))\n\n    if df.empty:\n        print(f\"DataFrame is empty after processing JSON files for plot: {title}\")\n        return None\n\n    plottable_metric_keys = [mk for mk in metrics_to_display if mk in df.columns and not df[mk].isnull().all()]\n    if not plottable_metric_keys:\n        print(f\"None of the specified metrics_to_display ({metrics_to_display}) have any valid data in the loaded JSONs for plotting: {title}\")\n        return df\n\n    sort_metric = None\n    if 'f1_opt' in plottable_metric_keys: sort_metric = 'f1_opt'\n    elif plottable_metric_keys: sort_metric = plottable_metric_keys[0]\n    if sort_metric: df = df.sort_values(by=sort_metric, ascending=False, na_position='last')\n    else: df = df.sort_values(by='Configuration', ascending=True)\n\n    num_metrics_to_plot = len(plottable_metric_keys)\n    ncols = min(num_metrics_to_plot, 3)\n    nrows = ceil(num_metrics_to_plot / ncols)\n    fig, axes = plt.subplots(nrows, ncols, figsize=(5 * ncols, max(4, 0.6 * len(df) + 1.5)), squeeze=False)\n    axes = axes.flatten()\n    colors = ['#2E86AB', '#A23B72', '#F18F01', '#C73E1D', '#593E2A', '#3A7D44', '#B565A7']\n    \n    plot_idx = 0\n    for metric in plottable_metric_keys:\n        ax = axes[plot_idx]\n        \n        y_pos = np.arange(len(df))\n        bar_values = df[metric].fillna(0) \n        bars = ax.barh(y_pos, bar_values, color=colors[plot_idx % len(colors)],\n                       alpha=0.85, edgecolor='black', linewidth=0.7)\n        \n        if metric == 'pr_auc': metric_name_display = \"PR AUC (AvgPrec)\"\n        elif metric == 'roc_auc_proba': metric_name_display = \"ROC AUC\"\n        else: metric_name_display = (metric.replace('_', ' ').replace(' opt', ' (Opt)').title())\n            \n        ax.set_title(metric_name_display, fontweight='bold', pad=12, fontsize=12)\n        ax.set_xlabel('Score', fontweight='bold', fontsize=10)\n        ax.set_yticks(y_pos)\n        ax.set_yticklabels(df['Short Configuration'], fontsize=9)  # <-- Shortened labels here\n        \n        max_val_metric = df[metric].max()\n        if pd.isna(max_val_metric) or max_val_metric == 0 : upper_limit = 0.1 \n        elif max_val_metric <= 1.0: upper_limit = 1.05\n        else: upper_limit = max_val_metric * 1.15\n        ax.set_xlim(0, upper_limit)\n        ax.tick_params(axis='x', labelsize=8)\n        ax.grid(axis='x', linestyle=':', alpha=0.6)\n\n        ax.spines['top'].set_visible(False)\n        ax.spines['right'].set_visible(False)\n\n        for bar_idx, bar_obj in enumerate(bars):\n            original_value = df[metric].iloc[bar_idx]\n            width_for_text = bar_obj.get_width()\n            text_label = f'{original_value:.3f}' if not pd.isna(original_value) else 'N/A'\n            \n            padding_from_bar = ax.get_xlim()[1] * 0.015\n            ax.text(width_for_text + padding_from_bar, \n                    bar_obj.get_y() + bar_obj.get_height() / 2,\n                    text_label, ha='left', va='center',\n                    fontweight='normal', fontsize=8.5, color='dimgray')\n        plot_idx += 1\n\n    for k_ax in range(plot_idx, len(axes)): axes[k_ax].axis('off')\n    fig.suptitle(title, fontsize=16, fontweight='bold', y=0.99 if nrows > 1 else 1.02)\n    plt.tight_layout(rect=[0, 0.03, 1, 0.95 if nrows > 1 else 0.92])\n\n    if save_dir:\n        os.makedirs(save_dir, exist_ok=True)\n        safe_title = \"\".join(c if c.isalnum() else \"_\" for c in title.lower())\n        filename = f\"comparison_bars_{safe_title}.pdf\"\n        filepath = os.path.join(save_dir, filename)\n        try:\n            plt.savefig(filepath, dpi=300, bbox_inches='tight', format='pdf')\n            print(f\"Comparison bar plot saved: {filepath}\")\n        except Exception as e:\n            print(f\"Error saving comparison bar plot to {filepath}: {e}\")\n    \n    plt.show()\n    return df\n\n    \nprint(\"Cell 6: All utility functions (training, evaluation, enhanced plotting) are defined.\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-03T16:19:05.345607Z","iopub.execute_input":"2025-06-03T16:19:05.345938Z","iopub.status.idle":"2025-06-03T16:19:05.448361Z","shell.execute_reply.started":"2025-06-03T16:19:05.345905Z","shell.execute_reply":"2025-06-03T16:19:05.447539Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# # CELL 7: Experiment Runner Function\n# print(\"\\n--- Cell 7: Experiment Runner Function ---\")\n\n# # ADD test_paths_global, test_labels_global, label_dict_global to the signature\n# def run_experiment(config, test_paths_global, test_labels_global, label_dict_global):\n#         \"\"\"\n#         Runs a complete experiment stage: builds datasets, builds model,\n#         trains, evaluates, runs Grad-CAM, stores results, and cleans up.\n#         # ... (Args documentation remains the same) ...\n#         \"\"\"\n#         key = config['key']\n#         print(\"\\n\" + \"=\"*70)\n#         print(f\" Starting Experiment: {key} \")\n#         print(\"=\"*70)\n#         # ... (Config printing logic remains the same) ...\n#         print(\"-\"*70)\n\n#         start_time_total = time.time()\n#         model = None\n#         history = None\n#         eval_metrics = None\n#         training_duration = 0\n#         model_to_eval = None # Use this after potentially loading best weights\n#         # Define checkpoint path for this specific run\n#         checkpoint_filepath = os.path.join(CHECKPOINT_DIR, f\"{key}_best.keras\")\n\n#         # --- Main Experiment Logic ---\n#         try:\n#             # --- 1. Build Datasets ---\n#             print(\"\\n[1. Building Datasets...]\")\n#             if 'img_size' not in config['parse_args']:\n#                  config['parse_args']['img_size'] = IMG_SIZE\n#             train_ds = build_dataset(\n#                 train_paths, train_labels, preprocess_image, config['parse_args'],\n#                 GLOBAL_BATCH_SIZE, f\"Train ({key})\", shuffle=True,\n#                 augment_in_map=config['parse_args'].get('apply_augment', False),\n#                 oversample=config.get('oversample_train', False), cache=True\n#             )\n#             val_parse_args = config['parse_args'].copy(); val_parse_args['apply_augment'] = False\n#             val_ds = build_dataset(\n#                 val_paths, val_labels, preprocess_image, val_parse_args,\n#                 GLOBAL_BATCH_SIZE, f\"Validation ({key})\", shuffle=False, cache=True\n#             )\n#             test_parse_args = config['parse_args'].copy(); test_parse_args['apply_augment'] = False\n#             test_ds = build_dataset(\n#                 test_paths, test_labels, preprocess_image, test_parse_args,\n#                 GLOBAL_BATCH_SIZE, f\"Test ({key})\", shuffle=False, cache=True\n#             )\n#             if not all([train_ds, val_ds, test_ds]): raise RuntimeError(\"Dataset build failed.\")\n#             print(\"Datasets built successfully.\")\n\n#             # --- 2. Build Model ---\n#             print(\"\\n[2. Building Model...]\")\n#             with strategy.scope():\n#                  if 'num_classes' not in config['model_args']: config['model_args']['num_classes'] = 1\n#                  model = build_full_model(IMG_SHAPE, **config['model_args'])\n#             model.summary(line_length=100)\n#             print(f\"Model '{model.name}' built.\")\n\n#             # --- 3. Setup Callbacks ---\n#             print(\"\\n[3. Setting up Callbacks...]\")\n#             callbacks_list = [\n#                 EarlyStopping(monitor='val_loss', patience=PATIENCE_EARLY_STOPPING, verbose=1, restore_best_weights=False),\n#                 ReduceLROnPlateau(monitor='val_loss', factor=0.2, patience=PATIENCE_REDUCE_LR, min_lr=MIN_LR, verbose=1),\n#                 ModelCheckpoint(filepath=checkpoint_filepath, monitor='val_loss', save_best_only=True, save_weights_only=False, verbose=1)\n#             ]\n#             print(f\"Model checkpoint path: {checkpoint_filepath}\")\n\n#             # --- 4. Determine Class Weights ---\n#             train_class_weights = None\n#             if config['train_args'].get('class_weights_setting') == 'balanced':\n#                  train_class_weights = class_weights_dict\n#                  print(\"Using 'balanced' class weights for training.\")\n#             else: print(\"No class weights applied for training.\")\n\n#             # --- 5. Train Model ---\n#             print(\"\\n[5. Training Model...]\")\n#             history, training_duration = train_model(\n#                 model, train_ds, val_ds,\n#                 epochs=config['train_args']['epochs'], class_weights=train_class_weights,\n#                 strategy=strategy, learning_rate=config['train_args']['learning_rate'],\n#                 callbacks=callbacks_list, stage_name=f\"Training ({key})\"\n#             )\n#             if history is None: raise RuntimeError(\"Model training failed.\")\n\n#             # --- 6. Load Best Weights ---\n#             print(\"\\n[6. Loading Best Weights from Checkpoint...]\")\n#             if os.path.exists(checkpoint_filepath):\n#                  with strategy.scope():\n#                       model_to_eval = keras.models.load_model(checkpoint_filepath, custom_objects=custom_objects_map)\n#                  print(f\"Successfully loaded best model weights from {checkpoint_filepath}\")\n#             else:\n#                  print(f\"WARNING: Checkpoint file not found at {checkpoint_filepath}. Evaluating with the last epoch's weights.\")\n#                  model_to_eval = model # Use the model object from the last epoch of training\n\n#             # --- 7. Evaluate Model ---\n#             print(\"\\n[7. Evaluating Model...]\")\n#             eval_metrics, y_true_test, y_pred_proba_test, cm_test = evaluate_model_optimized(\n#                 model=model_to_eval, val_ds=val_ds, test_ds=test_ds, strategy=strategy,\n#                 inv_label_map=inv_label_dict, target_metric=TARGET_METRIC\n#             )\n#             if not eval_metrics: print(\"Warning: Evaluation failed or returned no metrics.\"); eval_metrics = {}\n\n#            # --- 8. Grad-CAM Visualization ---\n#             print(\"\\n[8. Generating Grad-CAM...]\")\n#             base_model_for_gradcam = None # This will be the layer object to target\n#             try:\n#                 # Find base model layer object within the evaluated model\n#                 current_model_for_gradcam = model_to_eval # Use the evaluated model\n#                 for layer in current_model_for_gradcam.layers:\n#                      if isinstance(layer, keras.Model) and 'densenet' in layer.name:\n#                           base_model_for_gradcam = layer; break # Store the layer OBJECT\n#                 if base_model_for_gradcam is None:\n#                      # Attempt fallback\n#                      if len(current_model_for_gradcam.layers) > 1 and isinstance(current_model_for_gradcam.layers[1], keras.Model) and 'densenet' in current_model_for_gradcam.layers[1].name:\n#                           base_model_for_gradcam = current_model_for_gradcam.layers[1]\n#                           print(f\"Warning: Assuming base model layer is '{base_model_for_gradcam.name}' for Grad-CAM.\")\n#                      else: raise ValueError(\"Could not identify base DenseNet model layer object for Grad-CAM.\")\n    \n#                 print(f\"Targeting output of base model layer '{base_model_for_gradcam.name}' for Grad-CAM.\")\n    \n#                 # Select sample images (using passed global args - logic remains same)\n#                 sample_paths = []\n#                 if test_paths_global and test_labels_global and label_dict_global:\n#                     normal_label_val = label_dict_global.get('NORMAL', -99)\n#                     pneumonia_label_val = label_dict_global.get('PNEUMONIA', -99)\n#                     normal_idx = next((i for i, lbl in enumerate(test_labels_global) if lbl == normal_label_val), None)\n#                     pneumonia_idx = next((i for i, lbl in enumerate(test_labels_global) if lbl == pneumonia_label_val), None)\n#                     if normal_idx is not None and normal_idx < len(test_paths_global): sample_paths.append(test_paths_global[normal_idx])\n#                     if pneumonia_idx is not None and pneumonia_idx < len(test_paths_global): sample_paths.append(test_paths_global[pneumonia_idx])\n#                     print(f\"Selected sample paths for Grad-CAM: {sample_paths}\")\n#                 else:\n#                      print(\"Warning: Global test paths/labels/dict not properly passed. Cannot select samples for Grad-CAM.\")\n    \n#                 # Run Grad-CAM generation using the modified function call\n#                 if sample_paths and base_model_for_gradcam:\n#                      run_gradcam_on_samples(\n#                          full_model=current_model_for_gradcam,\n#                          target_layer=base_model_for_gradcam, # Pass the base model layer OBJECT\n#                          img_paths=sample_paths,\n#                          img_size=IMG_SIZE,\n#                          output_dir=GRADCAM_DIR,\n#                          config_key=key\n#                      )\n#                 elif not sample_paths: print(\"Could not find valid sample images for Grad-CAM.\")\n#                 else: print(\"Could not run Grad-CAM due to missing base model layer info.\")\n    \n#             except Exception as grad_e:\n#                 print(f\"Error during Grad-CAM setup/execution: {grad_e}\")\n#                 # traceback.print_exc() # Uncomment if needed\n#             # --- 9. Store Results ---\n#             print(\"\\n[9. Storing Results...]\")\n#             total_duration = time.time() - start_time_total\n#             results[key] = {\n#                 'config': config,\n#                 'metrics': eval_metrics,\n#                 'training_duration_sec': training_duration,\n#                 'total_duration_sec': total_duration,\n#                 'checkpoint_path': checkpoint_filepath if os.path.exists(checkpoint_filepath) else None,\n#                 # History object can be large, store only if absolutely needed\n#                 # 'history_data': history.history if history else None\n#             }\n#             print(f\"Results for '{key}' stored.\")\n\n#             # --- 10. Plotting ---\n#             print(\"\\n[10. Plotting Results...]\")\n#             plot_history(history, title_suffix=f\"({key})\")\n#             plot_confusion_matrix(cm_test, inv_label_dict, title_suffix=f\"({key} @ Opt Threshold)\")\n#             if y_true_test is not None and y_pred_proba_test is not None:\n#                 plot_roc_curve(y_true_test, y_pred_proba_test,\n#                                eval_metrics.get('roc_auc_proba', None), # Pass calculated AUC\n#                                inv_label_dict, title_suffix=f\"({key})\")\n\n#             print(f\"--- Experiment {key} Complete ---\")\n#             return eval_metrics # Return metrics dict\n\n#         # --- Error Handling for the entire experiment run ---\n#         except Exception as e:\n#             print(f\"\\n\\n ****** ERROR during experiment {key} ****** \")\n#             print(f\"Error Type: {type(e).__name__}\")\n#             print(f\"Error Details: {e}\")\n#             print(\"Traceback:\")\n#             traceback.print_exc()\n#             # Store minimal error info\n#             results[key] = {'status': 'failed', 'error': str(e)}\n#             return None # Indicate failure\n\n#         # --- Cleanup, always runs ---\n#         finally:\n#             print(\"\\n[11. Cleaning up resources...]\")\n#             del model, model_to_eval, train_ds, val_ds, test_ds, history # Delete large objects\n#             keras.backend.clear_session() # Clear Keras session\n#             gc.collect() # Force garbage collection\n#             print(\"Cleanup complete.\")\n#             print(\"=\"*70 + \"\\n\")\n\n# # --- End of run_experiment definition ---","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-03T15:32:49.99126Z","iopub.execute_input":"2025-06-03T15:32:49.991556Z","iopub.status.idle":"2025-06-03T15:32:50.007881Z","shell.execute_reply.started":"2025-06-03T15:32:49.991518Z","shell.execute_reply":"2025-06-03T15:32:50.007207Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# CELL 7: Experiment Runner Function\nprint(\"\\n--- Cell 7: Experiment Runner Function ---\")\n\nimport time\nimport os\nimport json\nimport traceback\nimport numpy as np # Often used within the experiment logic or metrics\nimport pandas as pd # Often used within the experiment logic or metrics\nimport tensorflow as tf # For tf.keras.utils.plot_model\nfrom tensorflow import keras # For general Keras types if needed\nfrom IPython.display import Image, display # For displaying the model plot in the notebook\n\n# Ensure custom_objects_map is available if needed for loading models\n# It's defined in Cell 5, so it should be in the global scope.\n# Example: custom_objects_map = { 'SelfAttention': SelfAttention, ... }\n\ndef run_experiment(config):\n    \"\"\"\n    Runs a complete experiment stage: builds datasets, builds model,\n    trains, evaluates, stores results, and cleans up.\n    Includes model architecture visualization.\n    \"\"\"\n    key = config['key']\n    print(\"\\n\" + \"=\"*70)\n    print(f\" Starting Experiment: {key} \")\n    print(\"=\"*70)\n    # Print key configuration parameters for the current experiment\n    print(\"Key Configuration Parameters:\")\n    print(f\"  Key: {config.get('key')}\")\n    # Safely print parse_args, model_args, train_args by checking existence and handling special cases like augment_layer\n    if 'parse_args' in config:\n        parse_args_display = {k: str(v) if k != 'augment_layer' else '<AugmentationLayerObject>' for k, v in config['parse_args'].items()}\n        print(f\"  Parse Args: {parse_args_display}\")\n    if 'model_args' in config:\n        print(f\"  Model Args: {config['model_args']}\")\n    if 'train_args' in config:\n        print(f\"  Train Args: {config['train_args']}\")\n    if 'oversample_train' in config: # From your example config structure\n        print(f\"  Oversample Train: {config['oversample_train']}\")\n    print(\"-\"*70)\n\n    start_time_total = time.time()\n    model = None\n    history = None\n    eval_metrics = None\n    training_duration = 0\n    model_to_eval = None\n    # CHECKPOINT_DIR should be globally defined from Cell 1\n    checkpoint_filepath = os.path.join(CHECKPOINT_DIR, f\"{key}_best.keras\")\n\n    try:\n        # --- 1. Build Datasets ---\n        print(\"\\n[1. Building Datasets...]\")\n        # IMG_SIZE, train_paths, train_labels, val_paths, val_labels, test_paths, test_labels,\n        # preprocess_image, GLOBAL_BATCH_SIZE, build_dataset should be globally defined\n        if 'img_size' not in config['parse_args']: # Assuming IMG_SIZE is global\n            config['parse_args']['img_size'] = IMG_SIZE\n        train_ds = build_dataset(\n            train_paths, train_labels, preprocess_image, config['parse_args'],\n            GLOBAL_BATCH_SIZE, f\"Train ({key})\", shuffle=True,\n            augment_in_map=config['parse_args'].get('apply_augment', False),\n            oversample=config.get('oversample_train', False), cache=True\n        )\n        val_parse_args = config['parse_args'].copy(); val_parse_args['apply_augment'] = False\n        val_ds = build_dataset(\n            val_paths, val_labels, preprocess_image, val_parse_args,\n            GLOBAL_BATCH_SIZE, f\"Validation ({key})\", shuffle=False, cache=True\n        )\n        test_parse_args = config['parse_args'].copy(); test_parse_args['apply_augment'] = False\n        test_ds = build_dataset(\n            test_paths, test_labels, preprocess_image, test_parse_args,\n            GLOBAL_BATCH_SIZE, f\"Test ({key})\", shuffle=False, cache=True\n        )\n        if not all([train_ds, val_ds, test_ds]): raise RuntimeError(\"Dataset build failed.\")\n        print(\"Datasets built successfully.\")\n\n        # --- 2. Build Model ---\n        print(\"\\n[2. Building Model...]\")\n        # strategy, IMG_SHAPE, build_full_model should be globally defined\n        with strategy.scope():\n            if 'num_classes' not in config['model_args']: config['model_args']['num_classes'] = 1\n            model = build_full_model(IMG_SHAPE, **config['model_args'])\n        model.summary(line_length=100)\n        print(f\"Model '{model.name}' built.\")\n\n        # +++ VISUALIZE MODEL ARCHITECTURE HERE (Corrected) +++\n        # PLOTS_DIR should be globally defined from Cell 1\n        model_plot_path = os.path.join(PLOTS_DIR, f\"{model.name}_architecture.png\")\n        try:\n            print(f\"\\nAttempting to plot model architecture to: {model_plot_path}\")\n            tf.keras.utils.plot_model(\n                model,\n                to_file=model_plot_path,\n                show_shapes=True,\n                show_dtype=False,\n                show_layer_names=True,\n                # show_layer_activations=True, # REMOVED due to compatibility issues\n                expand_nested=True,\n                dpi=96\n            )\n            print(f\"Model plot saved to {model_plot_path}\")\n            display(Image(filename=model_plot_path))\n        except ImportError as e:\n            print(f\"ImportError for model plotting: {e}. Make sure pydot and graphviz are installed.\")\n            print(\"  You can typically install them with: !pip install pydot graphviz\")\n            print(\"  On Linux systems, you might also need: !sudo apt-get install graphviz\")\n        except Exception as e:\n            print(f\"An error occurred during model plotting: {e}\")\n            print(\"  Ensure Graphviz is correctly installed and in your system's PATH if the error is not related to an unexpected argument.\")\n        # +++ END OF VISUALIZATION +++\n\n        # --- 3. Setup Callbacks ---\n        print(\"\\n[3. Setting up Callbacks...]\")\n        # PATIENCE_EARLY_STOPPING, PATIENCE_REDUCE_LR, MIN_LR should be globally defined\n        callbacks_list = [\n            EarlyStopping(monitor='val_loss', patience=PATIENCE_EARLY_STOPPING, verbose=1, restore_best_weights=False),\n            ReduceLROnPlateau(monitor='val_loss', factor=0.2, patience=PATIENCE_REDUCE_LR, min_lr=MIN_LR, verbose=1),\n            ModelCheckpoint(filepath=checkpoint_filepath, monitor='val_loss', save_best_only=True, save_weights_only=False, verbose=1)\n        ]\n        print(f\"Model checkpoint path: {checkpoint_filepath}\")\n\n        # --- 4. Determine Class Weights ---\n        train_class_weights = None\n        # class_weights_dict should be globally defined from Cell 3\n        if config['train_args'].get('class_weights_setting') == 'balanced':\n            train_class_weights = class_weights_dict\n            print(\"Using 'balanced' class weights for training.\")\n        else: print(\"No class weights applied for training.\")\n\n        # --- 5. Train Model ---\n        print(\"\\n[5. Training Model...]\")\n        # train_model function should be globally defined from Cell 6\n        history, training_duration = train_model(\n            model, train_ds, val_ds,\n            epochs=config['train_args']['epochs'], class_weights=train_class_weights,\n            strategy=strategy, learning_rate=config['train_args']['learning_rate'],\n            callbacks=callbacks_list, stage_name=f\"Training ({key})\"\n        )\n        if history is None: raise RuntimeError(\"Model training failed.\")\n\n        # --- 6. Load Best Weights ---\n        print(\"\\n[6. Loading Best Weights from Checkpoint...]\")\n        if os.path.exists(checkpoint_filepath):\n            with strategy.scope():\n                # custom_objects_map should be globally defined from Cell 5\n                model_to_eval = keras.models.load_model(checkpoint_filepath, custom_objects=custom_objects_map)\n            print(f\"Successfully loaded best model weights from {checkpoint_filepath}\")\n        else:\n            print(f\"WARNING: Checkpoint file not found at {checkpoint_filepath}. Evaluating with the last epoch's weights.\")\n            model_to_eval = model\n\n        # --- 7. Evaluate Model ---\n        print(\"\\n[7. Evaluating Model...]\")\n        # evaluate_model_optimized_with_viz, inv_label_dict, TARGET_METRIC, PLOTS_DIR should be globally defined\n        eval_metrics, y_true_test, y_pred_proba_test, _ = evaluate_model_optimized_with_viz(\n            model=model_to_eval, val_ds=val_ds, test_ds=test_ds, strategy=strategy,\n            inv_label_map=inv_label_dict, target_metric=TARGET_METRIC,\n            dataset_name=f\"Test ({key})\",\n            save_dir=PLOTS_DIR,\n            config_name=key\n        )\n        if not eval_metrics: print(\"Warning: Evaluation failed or returned no metrics.\"); eval_metrics = {}\n\n        # --- 8. Grad-CAM Visualization (Commented out as per your notebook) ---\n        # print(\"\\n[8. Generating Grad-CAM...]\")\n        # ...\n\n        # --- 9. Store Results ---\n        print(\"\\n[9. Storing Results...]\")\n        total_duration = time.time() - start_time_total\n        \n        metrics_json_save_path = None\n        if eval_metrics:\n            # METRICS_DIR should be globally defined from Cell 1\n            metrics_json_save_path = os.path.join(METRICS_DIR, f\"evaluation_metrics_{key}.json\")\n            try:\n                serializable_metrics = {}\n                for m_key, m_value in eval_metrics.items():\n                    if isinstance(m_value, np.generic): # Handles numpy float/int types\n                        serializable_metrics[m_key] = m_value.item()\n                    elif isinstance(m_value, np.ndarray):\n                        serializable_metrics[m_key] = m_value.tolist()\n                    else:\n                        serializable_metrics[m_key] = m_value\n                \n                with open(metrics_json_save_path, 'w') as f:\n                    json.dump(serializable_metrics, f, indent=4)\n                print(f\"Evaluation metrics saved to {metrics_json_save_path}\")\n            except Exception as e:\n                print(f\"Error saving evaluation metrics to {metrics_json_save_path}: {e}\")\n                metrics_json_save_path = None\n        \n        # 'results' dictionary should be globally defined (e.g., in Cell 1)\n        results[key] = {\n            'config': config,\n            'metrics': eval_metrics,\n            'training_duration_sec': training_duration,\n            'total_duration_sec': total_duration,\n            'checkpoint_path': checkpoint_filepath if os.path.exists(checkpoint_filepath) else None,\n            'metrics_json_path': metrics_json_save_path\n        }\n        print(f\"Results for '{key}' stored (including path to JSON metrics).\")\n\n        # --- 10. Plotting ---\n        print(\"\\n[10. Plotting Results...]\")\n        if history:\n            # plot_training_history_enhanced and PLOTS_DIR should be globally defined\n            plot_training_history_enhanced(\n                history,\n                title_suffix=f\" ({key})\",\n                save_dir=PLOTS_DIR,\n                config_name=key\n            )\n\n        print(f\"--- Experiment {key} Complete ---\")\n        return eval_metrics\n\n    except Exception as e:\n        print(f\"\\n\\n ****** ERROR during experiment {key} ****** \")\n        print(f\"Error Type: {type(e).__name__}\")\n        print(f\"Error Details: {e}\")\n        print(\"Traceback:\")\n        traceback.print_exc()\n        # 'results' dictionary should be globally defined\n        results[key] = {'status': 'failed', 'error': str(e), 'config': config}\n        return None\n\n    # finally:\n    #     # --- 11. Cleaning up resources... --- (As in your notebook)\n    #     # print(\"\\n[11. Cleaning up resources...]\")\n    #     # del model, model_to_eval, train_ds, val_ds, test_ds, history\n    #     # if 'keras' in globals() or 'tensorflow.keras' in globals():\n    #     #     keras.backend.clear_session()\n    #     # import gc # Make sure gc is imported if not already\n    #     # gc.collect()\n    #     # print(\"Cleanup complete.\")\n    #     # print(\"=\"*70 + \"\\n\")\n\n# --- End of run_experiment definition ---\n\nprint(\"Cell 7: All utility functions (training, evaluation, enhanced plotting) are defined.\")\n# The print statement above seems to be copied from Cell 6 in your original notebook.\n# For Cell 7, it should probably be:\n# print(\"Cell 7: Experiment Runner Function defined.\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-03T16:19:12.44707Z","iopub.execute_input":"2025-06-03T16:19:12.447324Z","iopub.status.idle":"2025-06-03T16:19:12.473195Z","shell.execute_reply.started":"2025-06-03T16:19:12.447296Z","shell.execute_reply":"2025-06-03T16:19:12.472213Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"LEARNING_RATE= 0.02\nDROPOUT_RATE = 0.1","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# CELL 8: Stage 1 - Run Baseline (Transfer Learning + Standard Augmentation)\nprint(\"\\n--- Cell 8: Stage 1 - Baseline (Transfer Learning + Standard Augmentation) ---\")\n\n\nconfig_baseline = {\n    'key': \"Baseline_StdAug\",\n    'parse_args': {\n        'apply_augment': True,\n        'augment_layer': standard_augmentation, # Defined in Cell 4\n        'apply_clahe': False,\n        'clahe_clip_limit': 2.0, # Default, not used\n        'img_size': IMG_SIZE\n    },\n    'model_args': {\n        'pooling': 'avg',\n        'attention': None,\n        'dropout': DROPOUT_RATE,\n        'base_trainable': False, # Base frozen for initial training\n        'num_classes': 1 # Binary\n    },\n    'train_args': {\n        'epochs': EPOCHS_HEAD,\n        'learning_rate': LEARNING_RATE,\n        'class_weights_setting': None # Start without explicit imbalance handling\n    },\n    'oversample_train': False # Not oversampling baseline\n}\n\n# Run the baseline experiment\nbaseline_metrics = run_experiment(\n    config_baseline,\n)\n\n# Initialize the variable to track the best configuration key\n# It starts with the baseline, assuming it ran successfully\ncurrent_best_config_key = config_baseline['key'] if baseline_metrics else None\nif current_best_config_key:\n     print(f\"\\nBaseline run complete. Current best configuration key: '{current_best_config_key}'\")\nelse:\n     print(\"\\nERROR: Baseline run failed. Cannot proceed with subsequent experiments.\")\n     # Optionally raise an error here\n\n\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-03T15:32:50.040205Z","iopub.execute_input":"2025-06-03T15:32:50.040575Z","iopub.status.idle":"2025-06-03T15:35:19.552744Z","shell.execute_reply.started":"2025-06-03T15:32:50.040537Z","shell.execute_reply":"2025-06-03T15:35:19.551878Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"LEARNING_RATE=0.001\nDROPOUT_RATE = 0.3\n# LEARNING_RATE=.0001\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# CELL 9: Stage 2 - CLAHE Experiments\n\nprint(\"\\n--- Cell 9: Stage 2 - CLAHE Experiments ---\")\n\nif current_best_config_key is None:\n    print(\"Skipping CLAHE experiments because baseline failed.\")\nelse:\n    clahe_clip_limits_to_test = [1.0, 2.0, 3.0]\n    clahe_stage_keys = [current_best_config_key] # Start comparison list with the baseline key\n\n    # Get the baseline config to modify\n    base_config_for_clahe = results[current_best_config_key]['config']\n\n    for clip_limit in clahe_clip_limits_to_test:\n        clahe_key = f\"{current_best_config_key}_CLAHE{clip_limit:.1f}\" # Build key based on baseline\n\n        config_clahe = {\n            'key': clahe_key,\n            'parse_args': base_config_for_clahe['parse_args'].copy(), # Copy baseline parse args\n            'model_args': base_config_for_clahe['model_args'].copy(), # Copy baseline model args\n            'train_args': base_config_for_clahe['train_args'].copy(), # Copy baseline train args\n            'oversample_train': base_config_for_clahe['oversample_train'] # Copy baseline oversample flag\n        }\n\n        # Modify only the CLAHE settings in parse_args\n        config_clahe['parse_args']['apply_clahe'] = True\n        config_clahe['parse_args']['clahe_clip_limit'] = clip_limit\n\n        # Run the experiment for this CLAHE variation\n        # Inside the loop:\n        # Run the experiment for this CLAHE variation\n        clahe_metrics = run_experiment(\n            config_clahe,\n        )\n        if clahe_metrics: # Add key only if run succeeded\n            clahe_stage_keys.append(clahe_key)\n\n    print(\"\\nCompleted CLAHE experiments.\")\n\n\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-03T15:35:19.554081Z","iopub.execute_input":"2025-06-03T15:35:19.554375Z","iopub.status.idle":"2025-06-03T15:45:18.793535Z","shell.execute_reply.started":"2025-06-03T15:35:19.554333Z","shell.execute_reply":"2025-06-03T15:45:18.792712Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# CELL 10: Stage 2 - CLAHE Comparison & Selection\nprint(\"\\n--- Cell 10: Stage 2 - CLAHE Comparison & Selection ---\")\n\nimport os\nimport json\nimport numpy as np # Required if directly handling np.nan, though pd.isna is often used\n\n# --- Configuration for this Stage ---\n# These global variables should be defined in previous cells:\n# METRICS_DIR (str): Path to the directory where evaluation_metrics_{key}.json files are stored.\n# PLOTS_DIR (str): Path to the directory where plots will be saved.\n# TARGET_METRIC (str): The primary metric for selecting the best model (e.g., 'f1_opt').\n# results (dict): The global dictionary populated by run_experiment, holding all results.\n# current_best_config_key (str): Key of the best model from the PREVIOUS stage.\n\n# Explicitly list the EXACT configuration keys for this CLAHE comparison stage.\n# These names MUST match the {key} part of your \"evaluation_metrics_{key}.json\" filenames.\n# This list includes the baseline that CLAHE is being compared against/applied to.\n# We assume \"Baseline_StdAug\" is the relevant baseline for these CLAHE variants.\nkeys_for_clahe_stage_evaluation = [\n    \"Baseline_StdAug\",\n    \"Baseline_StdAug_CLAHE1.0\",\n    \"Baseline_StdAug_CLAHE2.0\",\n    \"Baseline_StdAug_CLAHE3.0\"\n    # Add any other specific CLAHE variant keys if you ran them based on \"Baseline_StdAug\"\n]\n\n# This is the specific baseline configuration key within this stage's comparison list.\n# It's used for context in titles or for fetching its score for comparison messages.\n# It should be one of the keys from keys_for_clahe_stage_evaluation.\n# If current_best_config_key (from previous stage) is this baseline, that's good,\n# otherwise, ensure this key is correct for what you consider the 'base' in this comparison.\nreference_baseline_key_in_this_stage = \"Baseline_StdAug\"\n\nprint(f\"DEBUG: Initial current_best_config_key (from previous stage): '{current_best_config_key}'\")\nprint(f\"DEBUG: Reference baseline for this CLAHE stage comparison: '{reference_baseline_key_in_this_stage}'\")\nprint(f\"DEBUG: TARGET_METRIC for selection is: '{TARGET_METRIC}'\")\nprint(f\"DEBUG: METRICS_DIR is: '{METRICS_DIR}'\")\n\n# Filter the defined keys to only include those for which a metrics JSON file actually exists\nvalid_keys_for_comparison = [\n    key for key in keys_for_clahe_stage_evaluation\n    if os.path.exists(os.path.join(METRICS_DIR, f\"evaluation_metrics_{key}.json\"))\n]\n\nprint(f\"Found metric files for and will compare: {valid_keys_for_comparison}\")\n\nif not valid_keys_for_comparison:\n    print(\"ERROR: No metric files found for any of the specified CLAHE stage configurations. Cannot proceed with comparison.\")\n    # current_best_config_key remains unchanged\nelif len(valid_keys_for_comparison) == 1 and reference_baseline_key_in_this_stage in valid_keys_for_comparison:\n    print(f\"Warning: Only the reference baseline '{reference_baseline_key_in_this_stage}' metric file was found. \"\n          \"No other CLAHE variants to compare against in the provided list for this stage.\")\n    # current_best_config_key remains unchanged if this is the only valid key\n    # Or it becomes this key if it wasn't already.\n    if current_best_config_key != reference_baseline_key_in_this_stage :\n         print(f\"Setting current_best_config_key to '{reference_baseline_key_in_this_stage}' as it's the only valid one found for this stage.\")\n         current_best_config_key = reference_baseline_key_in_this_stage\nelse:\n    # --- Plotting ---\n    metrics_to_request_for_plot = [\n        'f1_opt',\n        'accuracy_opt',\n        'precision_opt',\n        'recall_opt',\n        'roc_auc_proba',\n        'pr_auc'  # Added PR AUC\n    ]\n    # Ensure TARGET_METRIC is in the list to be plotted, preferably first\n    if TARGET_METRIC not in metrics_to_request_for_plot:\n        metrics_to_request_for_plot.insert(0, TARGET_METRIC)\n    metrics_to_request_for_plot = list(dict.fromkeys(metrics_to_request_for_plot)) # Remove duplicates, keep order\n\n    df_clahe_comparison = plot_comparison_bars_enhanced(\n        config_keys_to_plot=valid_keys_for_comparison,\n        metrics_dir=METRICS_DIR,\n        title=f\"CLAHE Stage Comparison (Ref: {reference_baseline_key_in_this_stage})\",\n        save_dir=PLOTS_DIR,\n        metrics_to_display=metrics_to_request_for_plot\n    )\n\n    # --- Selection Logic (based on JSON files) ---\n    best_score_this_stage = -1.0  # Initialize with a value lower than any possible valid score\n    winner_key_this_stage = None\n    winner_metrics_this_stage = None # To store metrics dict of the winner\n\n    # Get the score of the reference baseline for this stage's comparison message\n    reference_baseline_score = -1.0\n    if reference_baseline_key_in_this_stage in valid_keys_for_comparison:\n        ref_baseline_json_path = os.path.join(METRICS_DIR, f\"evaluation_metrics_{reference_baseline_key_in_this_stage}.json\")\n        try:\n            with open(ref_baseline_json_path, 'r') as f:\n                ref_baseline_metrics_data = json.load(f)\n            reference_baseline_score = ref_baseline_metrics_data.get(TARGET_METRIC, -1.0)\n        except Exception as e:\n            print(f\"Warning: Error loading metrics for reference baseline '{reference_baseline_key_in_this_stage}': {e}\")\n\n    print(f\"\\nSelecting best configuration from this CLAHE stage using '{TARGET_METRIC}' \"\n          f\"(Score of '{reference_baseline_key_in_this_stage}' for reference: {reference_baseline_score:.4f}):\")\n\n    for config_key in valid_keys_for_comparison:\n        json_path = os.path.join(METRICS_DIR, f\"evaluation_metrics_{config_key}.json\")\n        current_score_from_json = -1.0 # Default for this iteration\n        current_metrics_from_json = None\n\n        try:\n            with open(json_path, 'r') as f:\n                current_metrics_from_json = json.load(f)\n            current_score_from_json = current_metrics_from_json.get(TARGET_METRIC, -1.0)\n            \n            metric_display_value = f\"{current_score_from_json:.4f}\" if current_score_from_json != -1.0 or TARGET_METRIC in current_metrics_from_json else \"Not Found\"\n            print(f\"  - Config '{config_key}': {TARGET_METRIC} = {metric_display_value}\")\n\n            # Assumes higher is better for TARGET_METRIC\n            if current_score_from_json > best_score_this_stage:\n                best_score_this_stage = current_score_from_json\n                winner_key_this_stage = config_key\n                winner_metrics_this_stage = current_metrics_from_json\n        except Exception as e:\n            print(f\"  - Config '{config_key}': Error loading/processing metrics from JSON - {e}. Skipping for winner selection.\")\n            continue # Skip to the next config_key\n\n    # --- Announce Winner of this Stage and Update the GLOBAL current_best_config_key ---\n    if winner_key_this_stage and best_score_this_stage > -1.0: # Check if a valid positive score was found\n        print(f\"\\n🏆 Winner of CLAHE Stage: '{winner_key_this_stage}' ({TARGET_METRIC}: {best_score_this_stage:.4f})\")\n        print(\"This configuration will be used as the new 'current_best_config_key' for subsequent stages.\")\n        current_best_config_key = winner_key_this_stage # Update the global best key\n\n        if winner_key_this_stage in results and 'config' in results[winner_key_this_stage]:\n            print(\"\\nWinning Configuration Details (from in-memory 'results'):\")\n            winner_config_dict = results[winner_key_this_stage]['config']\n            # Simple print, assuming you might have a dedicated print_config_details function\n            for detail_key, detail_value in winner_config_dict.items():\n                if isinstance(detail_value, dict):\n                    print(f\"  {detail_key}:\")\n                    for sub_key, sub_value in detail_value.items():\n                        if sub_key == 'augment_layer': print(f\"    {sub_key}: <Keras Layer Object>\")\n                        else: print(f\"    {sub_key}: {sub_value}\")\n                else: print(f\"  {detail_key}: {detail_value}\")\n        else:\n            print(f\"Full configuration details for winner '{winner_key_this_stage}' not found in in-memory 'results'.\")\n\n        if winner_metrics_this_stage:\n            print(\"\\nWinning Metrics (from JSON):\")\n            for m_key, m_val in winner_metrics_this_stage.items():\n                if isinstance(m_val, (float, np.floating)): print(f\"  {m_key}: {m_val:.4f}\")\n                else: print(f\"  {m_key}: {m_val}\")\n    else:\n        print(\"\\nCould not determine a new winner for the CLAHE stage (e.g., all scores were -1.0 or no valid positive scores found).\")\n        if reference_baseline_key_in_this_stage in valid_keys_for_comparison and reference_baseline_score >= best_score_this_stage:\n            print(f\"The configuration '{reference_baseline_key_in_this_stage}' (Score: {reference_baseline_score:.4f}) \"\n                  \"remains the best among those evaluated in this stage, or no improvement was found.\")\n            # If the baseline for this stage is better than any other in this stage, it becomes the new overall best.\n            if current_best_config_key != reference_baseline_key_in_this_stage and reference_baseline_score != -1.0 :\n                 current_best_config_key = reference_baseline_key_in_this_stage\n                 print(f\"Updating overall best configuration to '{current_best_config_key}'.\")\n            elif current_best_config_key == reference_baseline_key_in_this_stage:\n                 print(f\"Overall best configuration remains '{current_best_config_key}'.\")\n\n        else: # No clear winner and baseline itself wasn't valid or wasn't best\n             print(f\"The overall best configuration '{current_best_config_key}' (from before this stage) remains unchanged.\")\n\nprint(\"-\" * 70)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-03T16:19:51.794255Z","iopub.execute_input":"2025-06-03T16:19:51.794591Z","iopub.status.idle":"2025-06-03T16:19:53.505249Z","shell.execute_reply.started":"2025-06-03T16:19:51.794556Z","shell.execute_reply":"2025-06-03T16:19:53.504081Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"LEARNING_RATE=0.0005\nDROPOUT_RATE = 0.35\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-03T15:46:22.642862Z","iopub.execute_input":"2025-06-03T15:46:22.643137Z","iopub.status.idle":"2025-06-03T15:46:22.647296Z","shell.execute_reply.started":"2025-06-03T15:46:22.643109Z","shell.execute_reply":"2025-06-03T15:46:22.646259Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# CELL 11: Stage 3 - Imbalance Handling Experiments\nprint(\"\\n--- Cell 11: Stage 3 - Imbalance Handling Experiments ---\")\n\n\n# The baseline for this stage is the best configuration identified after Cell 10\nimbalance_baseline_key = current_best_config_key\nimbalance_stage_keys = [] # Keep track of keys for comparison in this stage\n\nif imbalance_baseline_key is None or imbalance_baseline_key not in results:\n    print(\"ERROR: Cannot proceed with Imbalance Handling stage. Baseline configuration key is missing or invalid.\")\n    # Optionally raise error: raise ValueError(\"Baseline configuration for Imbalance Handling stage is missing.\")\nelse:\n    print(f\"Using configuration '{imbalance_baseline_key}' as baseline for Imbalance Handling stage.\")\n    # Add baseline key to the list for comparison\n    imbalance_stage_keys.append(imbalance_baseline_key)\n\n    # Retrieve the configuration dictionary of the baseline\n    baseline_config_imbalance = results[imbalance_baseline_key]['config']\n\n    # --- Experiment 3a: Apply Class Weights ---\n    print(\"\\n--- Running Imbalance Experiment: Class Weights ---\")\n    config_weights_key = f\"{imbalance_baseline_key}_ClassWeights\"\n    config_weights = {\n        'key': config_weights_key,\n        'parse_args': baseline_config_imbalance['parse_args'].copy(),\n        'model_args': baseline_config_imbalance['model_args'].copy(),\n        'train_args': baseline_config_imbalance['train_args'].copy(),\n        'oversample_train': False # Ensure oversampling is off\n    }\n    # Modify train_args to apply balanced class weights\n    config_weights['train_args']['class_weights_setting'] = 'balanced'\n\n    # Run the experiment\n    weights_metrics = run_experiment(\n    config_weights,\n    )\n    if weights_metrics: # Add key only if run succeeded\n        imbalance_stage_keys.append(config_weights_key)\n\n\n    # --- Experiment 3b: Apply Oversampling ---\n    print(\"\\n--- Running Imbalance Experiment: Oversampling ---\")\n    config_oversample_key = f\"{imbalance_baseline_key}_Oversample\"\n    config_oversample = {\n        'key': config_oversample_key,\n        'parse_args': baseline_config_imbalance['parse_args'].copy(),\n        'model_args': baseline_config_imbalance['model_args'].copy(),\n        'train_args': baseline_config_imbalance['train_args'].copy(),\n        'oversample_train': True # Enable oversampling in build_dataset\n    }\n    # Ensure class weights are explicitly off when oversampling\n    config_oversample['train_args']['class_weights_setting'] = None\n\n    # Run the experiment\n    oversample_metrics = run_experiment(\n    config_oversample,\n    )\n    if oversample_metrics: # Add key only if run succeeded\n        imbalance_stage_keys.append(config_oversample_key)\n\n    print(\"\\nCompleted Imbalance Handling experiments.\")\n\n\n\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-03T15:46:25.400924Z","iopub.execute_input":"2025-06-03T15:46:25.401223Z","iopub.status.idle":"2025-06-03T15:54:20.772446Z","shell.execute_reply.started":"2025-06-03T15:46:25.401193Z","shell.execute_reply":"2025-06-03T15:54:20.771531Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# CELL 12: Stage 3 - Imbalance Handling Comparison & Selection\nprint(\"\\n--- Cell 12: Stage 3 - Imbalance Handling Comparison & Selection ---\")\n\nimport os\nimport json\nimport numpy as np # Though pd.isna is used in plot function, direct use might occur\n\n# --- Configuration for this Stage ---\n# These global variables should be defined in previous cells:\n# METRICS_DIR (str): Path to evaluation_metrics_{key}.json files.\n# PLOTS_DIR (str): Path to save plots.\n# TARGET_METRIC (str): Primary metric for selection (e.g., 'f1_opt').\n# results (dict): Global dictionary with all experiment results (configs, paths, etc.).\n# current_best_config_key (str): Key of the best model from the PREVIOUS stage (e.g., winner of CLAHE stage).\n\n# The baseline for THIS Imbalance Handling stage is the winner from the previous stage.\nimbalance_stage_baseline_key = current_best_config_key\n\n# IMPORTANT: Define the list of exact keys for this Imbalance Handling stage.\n# This list should include the baseline for this stage and all its imbalance variants.\n# These names MUST match the {key} part of your \"evaluation_metrics_{key}.json\" filenames.\nkeys_for_imbalance_stage = [\n    imbalance_stage_baseline_key,  # e.g., \"Baseline_StdAug_CLAHE1.0\"\n    f\"{imbalance_stage_baseline_key}_ClassWeights\",\n    f\"{imbalance_stage_baseline_key}_Oversample\"\n    # Add other specific imbalance handling experiment keys if you ran more based on this baseline\n]\n\n# --- Pre-computation & Sanity Checks ---\nprint(f\"DEBUG: Baseline for this Imbalance Stage (current_best_config_key entering Cell 12): '{imbalance_stage_baseline_key}'\")\nprint(f\"DEBUG: TARGET_METRIC for selection is: '{TARGET_METRIC}'\") # Ensure this is 'f1_opt' or similar from your JSON\nprint(f\"DEBUG: METRICS_DIR is: '{METRICS_DIR}'\")\nprint(f\"DEBUG: Intended keys for Imbalance Stage (before file check): {keys_for_imbalance_stage}\")\n\n# Filter these keys to only include those for which a metrics JSON file actually exists\nvalid_keys_for_imbalance_comparison = [\n    key for key in keys_for_imbalance_stage\n    if os.path.exists(os.path.join(METRICS_DIR, f\"evaluation_metrics_{key}.json\"))\n]\nprint(f\"Found metric files for and will compare these Imbalance Stage configurations: {valid_keys_for_imbalance_comparison}\")\n\n\nif not imbalance_stage_baseline_key or imbalance_stage_baseline_key not in valid_keys_for_imbalance_comparison:\n    print(f\"ERROR: Baseline for Imbalance Stage ('{imbalance_stage_baseline_key}') metric file not found or key is None. \"\n          \"Skipping Imbalance Handling comparison.\")\n    # current_best_config_key remains unchanged from before this cell\nelif len(valid_keys_for_imbalance_comparison) <= 1: # Needs at least baseline + 1 variant for a meaningful comparison\n    print(\"Not enough successful Imbalance Handling runs (including this stage's baseline) to perform a meaningful comparison.\")\n    print(f\"Keeping previous best configuration: '{current_best_config_key}'\")\n    # current_best_config_key remains unchanged\nelse:\n    # --- Plotting ---\n    metrics_to_request_for_plot = [\n        'f1_opt',\n        'accuracy_opt',\n        'precision_opt',\n        'recall_opt',\n        'roc_auc_proba',\n        'pr_auc'  # Make sure this key exists in your JSON files\n    ]\n    if TARGET_METRIC not in metrics_to_request_for_plot:\n        metrics_to_request_for_plot.insert(0, TARGET_METRIC)\n    metrics_to_request_for_plot = list(dict.fromkeys(metrics_to_request_for_plot)) # Remove duplicates\n\n    df_imbalance_comparison = plot_comparison_bars_enhanced(\n        config_keys_to_plot=valid_keys_for_imbalance_comparison,\n        metrics_dir=METRICS_DIR,\n        title=f\"Imbalance Handling Stage Comparison (Base: {imbalance_stage_baseline_key})\",\n        save_dir=PLOTS_DIR,\n        metrics_to_display=metrics_to_request_for_plot\n    )\n\n    # --- Selection Logic (based on JSON files) ---\n    best_score_this_stage = -1.0\n    winner_key_this_stage = None \n    winner_metrics_this_stage = None\n\n    baseline_score_for_this_stage_ref = -1.0\n    baseline_ref_json_path = os.path.join(METRICS_DIR, f\"evaluation_metrics_{imbalance_stage_baseline_key}.json\")\n    try:\n        with open(baseline_ref_json_path, 'r') as f:\n            baseline_ref_metrics_data = json.load(f)\n        baseline_score_for_this_stage_ref = baseline_ref_metrics_data.get(TARGET_METRIC, -1.0)\n    except Exception as e:\n        print(f\"Warning: Error loading metrics for this stage's baseline '{imbalance_stage_baseline_key}': {e}\")\n\n    print(f\"\\nSelecting best configuration from Imbalance Handling stage using '{TARGET_METRIC}' \"\n          f\"(Score of '{imbalance_stage_baseline_key}' for reference: {baseline_score_for_this_stage_ref:.4f}):\")\n\n    for config_key in valid_keys_for_imbalance_comparison:\n        json_path = os.path.join(METRICS_DIR, f\"evaluation_metrics_{config_key}.json\")\n        current_score_from_json = -1.0\n        current_metrics_from_json = None\n\n        try:\n            with open(json_path, 'r') as f:\n                current_metrics_from_json = json.load(f)\n            current_score_from_json = current_metrics_from_json.get(TARGET_METRIC, -1.0)\n            \n            metric_display_value = f\"{current_score_from_json:.4f}\" if current_score_from_json != -1.0 or TARGET_METRIC in current_metrics_from_json else \"Not Found\"\n            print(f\"  - Config '{config_key}': {TARGET_METRIC} = {metric_display_value}\")\n\n            if current_score_from_json > best_score_this_stage: # Assumes higher is better\n                best_score_this_stage = current_score_from_json\n                winner_key_this_stage = config_key\n                winner_metrics_this_stage = current_metrics_from_json\n        except Exception as e:\n            print(f\"  - Config '{config_key}': Error loading/processing metrics from JSON - {e}. Skipping for winner selection.\")\n            continue\n\n    # --- Announce Winner of this Stage and Update the GLOBAL current_best_config_key ---\n    if winner_key_this_stage and best_score_this_stage > -1.0: \n        print(f\"\\n🏆 Winner of Imbalance Handling Stage: '{winner_key_this_stage}' ({TARGET_METRIC}: {best_score_this_stage:.4f})\")\n        print(\"This configuration will be updated as the new 'current_best_config_key'.\")\n        current_best_config_key = winner_key_this_stage # Update the global best key\n\n        if winner_key_this_stage in results and 'config' in results[winner_key_this_stage]:\n            print(\"\\nWinning Configuration Details (from in-memory 'results'):\")\n            winner_config_dict = results[winner_key_this_stage]['config']\n            for detail_key, detail_value in winner_config_dict.items():\n                if isinstance(detail_value, dict):\n                    print(f\"  {detail_key}:\")\n                    for sub_key, sub_value in detail_value.items():\n                        if sub_key == 'augment_layer': print(f\"    {sub_key}: <Keras Layer Object>\")\n                        else: print(f\"    {sub_key}: {sub_value}\")\n                else: print(f\"  {detail_key}: {detail_value}\")\n        else:\n            print(f\"Full configuration details for winner '{winner_key_this_stage}' not found in in-memory 'results'.\")\n\n        if winner_metrics_this_stage:\n            print(\"\\nWinning Metrics (from JSON):\")\n            for m_key, m_val in winner_metrics_this_stage.items():\n                if isinstance(m_val, (float, np.floating)): print(f\"  {m_key}: {m_val:.4f}\")\n                else: print(f\"  {m_key}: {m_val}\")\n    else:\n        print(\"\\nCould not determine a new winner for the Imbalance Handling stage (e.g., all scores were -1.0 or no improvement).\")\n        if imbalance_stage_baseline_key in valid_keys_for_imbalance_comparison and \\\n           baseline_score_for_this_stage_ref >= best_score_this_stage and \\\n           baseline_score_for_this_stage_ref > -1.0 : # Ensure baseline had a valid positive score\n            print(f\"The configuration '{imbalance_stage_baseline_key}' (Score: {baseline_score_for_this_stage_ref:.4f}) \"\n                  \"remains the best among those evaluated in this stage.\")\n            if current_best_config_key != imbalance_stage_baseline_key: # If current best was somehow different\n                 current_best_config_key = imbalance_stage_baseline_key \n                 print(f\"Updating overall best configuration to '{current_best_config_key}'.\")\n            else:\n                 print(f\"Overall best configuration remains '{current_best_config_key}'.\")\n        else:\n             print(f\"The overall best configuration '{current_best_config_key}' (from before this stage started, or due to errors/no improvement) remains unchanged.\")\n\nprint(\"-\" * 70)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-03T16:19:31.967476Z","iopub.execute_input":"2025-06-03T16:19:31.96779Z","iopub.status.idle":"2025-06-03T16:19:33.544946Z","shell.execute_reply.started":"2025-06-03T16:19:31.967756Z","shell.execute_reply":"2025-06-03T16:19:33.544284Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"LEARNING_RATE=0.0002\nDROPOUT_RATE = 0.4\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-03T16:22:33.410539Z","iopub.execute_input":"2025-06-03T16:22:33.411522Z","iopub.status.idle":"2025-06-03T16:22:33.417341Z","shell.execute_reply.started":"2025-06-03T16:22:33.411483Z","shell.execute_reply":"2025-06-03T16:22:33.416371Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# CELL 13: Stage 4 - Pooling Experiments\nprint(\"\\n--- Cell 13: Stage 4 - Pooling Experiments ---\")\n\n# The baseline for this stage is the best configuration identified after Cell 12\npooling_baseline_key = current_best_config_key\npooling_stage_keys = [] # Keep track of keys for comparison in this stage\n\nif pooling_baseline_key is None or pooling_baseline_key not in results:\n    print(\"ERROR: Cannot proceed with Pooling stage. Baseline configuration key is missing or invalid.\")\n    # Optionally raise error: raise ValueError(\"Baseline configuration for Pooling stage is missing.\")\nelse:\n    print(f\"Using configuration '{pooling_baseline_key}' as baseline for Pooling stage.\")\n    # Add baseline key to the list for comparison\n    pooling_stage_keys.append(pooling_baseline_key)\n\n    # Retrieve the configuration dictionary of the baseline\n    baseline_config_pooling = results[pooling_baseline_key]['config']\n    baseline_pooling_type = baseline_config_pooling['model_args'].get('pooling', 'avg') # Get current pooling type\n    print(f\"Baseline pooling type for this stage: '{baseline_pooling_type}'\")\n\n    pooling_types_to_test = ['max', 'hybrid']\n\n    for test_pooling_type in pooling_types_to_test:\n        # Skip if the type to test is the same as the baseline's pooling type\n        if test_pooling_type == baseline_pooling_type:\n            print(f\"Skipping pooling type '{test_pooling_type}' as it matches the baseline.\")\n            continue\n\n        print(f\"\\n--- Running Pooling Experiment: {test_pooling_type.upper()} ---\")\n        # Construct key by appending pooling type to the baseline key for this stage\n        config_pool_key = f\"{pooling_baseline_key}_Pool{test_pooling_type.upper()}\"\n\n        config_pool = {\n            'key': config_pool_key,\n            'parse_args': baseline_config_pooling['parse_args'].copy(),\n            'model_args': baseline_config_pooling['model_args'].copy(),\n            'train_args': baseline_config_pooling['train_args'].copy(),\n            'oversample_train': baseline_config_pooling['oversample_train']\n        }\n\n        # Modify only the pooling setting in model_args\n        config_pool['model_args']['pooling'] = test_pooling_type\n        # Ensure attention is still off for this stage\n        config_pool['model_args']['attention'] = None\n\n        # Run the experiment\n        pool_metrics = run_experiment(\n        config_pool,\n        )\n        if pool_metrics: # Add key only if run succeeded\n            pooling_stage_keys.append(config_pool_key)\n\n    print(\"\\nCompleted Pooling experiments.\")\n\n\n\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-03T16:22:39.457502Z","iopub.execute_input":"2025-06-03T16:22:39.457829Z","iopub.status.idle":"2025-06-03T16:29:05.683148Z","shell.execute_reply.started":"2025-06-03T16:22:39.457794Z","shell.execute_reply":"2025-06-03T16:29:05.682421Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# CELL 14: Stage 4 - Pooling Comparison & Selection\nprint(\"\\n--- Cell 14: Stage 4 - Pooling Comparison & Selection ---\")\n\nimport os\nimport json\nimport numpy as np # For np.nan if needed, pd.isna used in plot function\n\n# --- Configuration for this Stage ---\n# These global variables should be defined from previous cells:\n# METRICS_DIR (str): Path to evaluation_metrics_{key}.json files.\n# PLOTS_DIR (str): Path to save plots.\n# TARGET_METRIC (str): Primary metric for selection (e.g., 'f1_opt').\n# results (dict): Global dictionary with all experiment results (configs, paths, etc.).\n# current_best_config_key (str): Key of the best model from the PREVIOUS stage (e.g., winner of Imbalance Handling).\n\n# The baseline for THIS Pooling stage is the winner from the previous stage.\npooling_stage_baseline_key = current_best_config_key\n\n# IMPORTANT: Define the list of exact keys for THIS Pooling stage.\n# This list should include the pooling_stage_baseline_key AND all its pooling variants.\n# These names MUST match the {key} part of your \"evaluation_metrics_{key}.json\" filenames.\n# Example: If pooling_stage_baseline_key = \"Baseline_StdAug_CLAHE1.0_OverSample\"\nkeys_for_pooling_stage = [\n    pooling_stage_baseline_key,  # e.g., \"Baseline_StdAug_CLAHE1.0_OverSample\" (represents default/current pooling)\n    f\"{pooling_stage_baseline_key}_PoolMAX\",\n    f\"{pooling_stage_baseline_key}_PoolAVG\",    # If you ran a specific 'PoolAVG' variant\n    f\"{pooling_stage_baseline_key}_PoolHYBRID\"\n    # Add other specific pooling strategy experiment keys you ran based on this baseline.\n    # If your pooling experiments ALSO include attention, add those keys here,\n    # e.g., f\"{pooling_stage_baseline_key}_PoolHYBRID_AttnCBAM\"\n    # For now, this example focuses purely on comparing pooling types.\n]\n# Remove duplicates if the baseline itself represents one of the pooling types implicitly\nkeys_for_pooling_stage = list(dict.fromkeys(keys_for_pooling_stage))\n\n\n# --- Pre-computation & Sanity Checks ---\nprint(f\"DEBUG: Baseline for this Pooling Stage (current_best_config_key entering Cell 14): '{pooling_stage_baseline_key}'\")\nprint(f\"DEBUG: TARGET_METRIC for selection is: '{TARGET_METRIC}'\")\nprint(f\"DEBUG: METRICS_DIR is: '{METRICS_DIR}'\")\nprint(f\"DEBUG: Intended keys for Pooling Stage (before file check): {keys_for_pooling_stage}\")\n\n# Filter these keys to only include those for which a metrics JSON file actually exists\nvalid_keys_for_pooling_comparison = [\n    key for key in keys_for_pooling_stage\n    if os.path.exists(os.path.join(METRICS_DIR, f\"evaluation_metrics_{key}.json\"))\n]\nprint(f\"Found metric files for and will compare these Pooling Stage configurations: {valid_keys_for_pooling_comparison}\")\n\n\nif not pooling_stage_baseline_key or pooling_stage_baseline_key not in valid_keys_for_pooling_comparison:\n    print(f\"ERROR: Baseline for Pooling Stage ('{pooling_stage_baseline_key}') metric file not found or key is None. \"\n          \"Skipping Pooling comparison.\")\nelif len(valid_keys_for_pooling_comparison) <= 1:\n    print(\"Not enough successful Pooling runs (including this stage's baseline) to perform a meaningful comparison.\")\n    print(f\"Keeping previous best configuration: '{current_best_config_key}'\")\nelse:\n    # --- Plotting ---\n    metrics_to_request_for_plot = [\n        'f1_opt', 'accuracy_opt', 'precision_opt', 'recall_opt', 'roc_auc_proba', 'pr_auc'\n    ]\n    if TARGET_METRIC not in metrics_to_request_for_plot:\n        metrics_to_request_for_plot.insert(0, TARGET_METRIC)\n    metrics_to_request_for_plot = list(dict.fromkeys(metrics_to_request_for_plot))\n\n    df_pooling_comparison = plot_comparison_bars_enhanced(\n        config_keys_to_plot=valid_keys_for_pooling_comparison,\n        metrics_dir=METRICS_DIR,\n        title=f\"Pooling Strategy Stage Comparison (Base: {pooling_stage_baseline_key})\",\n        save_dir=PLOTS_DIR,\n        metrics_to_display=metrics_to_request_for_plot\n    )\n\n    # --- Selection Logic (based on JSON files) ---\n    best_score_this_stage = -1.0\n    winner_key_this_stage = None \n    winner_metrics_this_stage = None\n\n    baseline_score_for_this_stage_ref = -1.0\n    baseline_ref_json_path = os.path.join(METRICS_DIR, f\"evaluation_metrics_{pooling_stage_baseline_key}.json\")\n    try:\n        with open(baseline_ref_json_path, 'r') as f:\n            baseline_ref_metrics_data = json.load(f)\n        baseline_score_for_this_stage_ref = baseline_ref_metrics_data.get(TARGET_METRIC, -1.0)\n    except Exception as e:\n        print(f\"Warning: Error loading metrics for this stage's baseline '{pooling_stage_baseline_key}': {e}\")\n\n    print(f\"\\nSelecting best configuration from Pooling stage using '{TARGET_METRIC}' \"\n          f\"(Score of '{pooling_stage_baseline_key}' for reference: {baseline_score_for_this_stage_ref:.4f}):\")\n\n    for config_key in valid_keys_for_pooling_comparison:\n        json_path = os.path.join(METRICS_DIR, f\"evaluation_metrics_{config_key}.json\")\n        current_score_from_json = -1.0\n        current_metrics_from_json = None\n        try:\n            with open(json_path, 'r') as f:\n                current_metrics_from_json = json.load(f)\n            current_score_from_json = current_metrics_from_json.get(TARGET_METRIC, -1.0)\n            metric_display_value = f\"{current_score_from_json:.4f}\" if current_score_from_json != -1.0 or TARGET_METRIC in current_metrics_from_json else \"Not Found\"\n            print(f\"  - Config '{config_key}': {TARGET_METRIC} = {metric_display_value}\")\n            if current_score_from_json > best_score_this_stage: # Assumes higher is better\n                best_score_this_stage = current_score_from_json\n                winner_key_this_stage = config_key\n                winner_metrics_this_stage = current_metrics_from_json\n        except Exception as e:\n            print(f\"  - Config '{config_key}': Error loading/processing metrics from JSON - {e}. Skipping.\")\n            continue\n            \n    # --- Announce Winner of this Stage and Update the GLOBAL current_best_config_key ---\n    if winner_key_this_stage and best_score_this_stage > -1.0: \n        print(f\"\\n🏆 Winner of Pooling Stage: '{winner_key_this_stage}' ({TARGET_METRIC}: {best_score_this_stage:.4f})\")\n        # The old code said: \"This configuration will be used as the baseline for the next stage (Attention).\"\n        # You might have a separate Attention stage, or combine Pooling & Attention. Adjust message as needed.\n        print(\"This configuration will be updated as the new 'current_best_config_key'.\")\n        current_best_config_key = winner_key_this_stage \n\n        if winner_key_this_stage in results and 'config' in results[winner_key_this_stage]:\n            print(\"\\nWinning Configuration Details (from in-memory 'results'):\")\n            winner_config_dict = results[winner_key_this_stage]['config']\n            winning_pooling_type = winner_config_dict.get('model_args', {}).get('pooling', 'N/A')\n            print(f\"  (Winning Pooling Type from config: {winning_pooling_type})\")\n            for detail_key, detail_value in winner_config_dict.items():\n                if isinstance(detail_value, dict):\n                    print(f\"  {detail_key}:\")\n                    for sub_key, sub_value in detail_value.items():\n                        if sub_key == 'augment_layer': print(f\"    {sub_key}: <Keras Layer Object>\")\n                        else: print(f\"    {sub_key}: {sub_value}\")\n                else: print(f\"  {detail_key}: {detail_value}\")\n        else:\n            print(f\"Full configuration details for winner '{winner_key_this_stage}' not found in 'results'.\")\n\n        if winner_metrics_this_stage:\n            print(\"\\nWinning Metrics (from JSON):\")\n            for m_key, m_val in winner_metrics_this_stage.items():\n                if isinstance(m_val, (float, np.floating)): print(f\"  {m_key}: {m_val:.4f}\")\n                else: print(f\"  {m_key}: {m_val}\")\n    else:\n        print(\"\\nCould not determine a new winner for the Pooling stage.\")\n        if pooling_stage_baseline_key in valid_keys_for_pooling_comparison and \\\n           baseline_score_for_this_stage_ref >= best_score_this_stage and \\\n           baseline_score_for_this_stage_ref > -1.0:\n            print(f\"The configuration '{pooling_stage_baseline_key}' (Score: {baseline_score_for_this_stage_ref:.4f}) remains the best.\")\n            current_best_config_key = pooling_stage_baseline_key\n        else:\n            print(f\"The overall best configuration '{current_best_config_key}' remains unchanged.\")\nprint(\"-\" * 70)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-03T16:32:10.669037Z","iopub.execute_input":"2025-06-03T16:32:10.669348Z","iopub.status.idle":"2025-06-03T16:32:12.294284Z","shell.execute_reply.started":"2025-06-03T16:32:10.669315Z","shell.execute_reply":"2025-06-03T16:32:12.293491Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# LEARNING_RATE=0.00035\n# DROPOUT_RATE = 0.1\n# print(LEARNING_RATE)\nLEARNING_RATE=0.0001\nDROPOUT_RATE = 0.45","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-03T16:32:33.249801Z","iopub.execute_input":"2025-06-03T16:32:33.250091Z","iopub.status.idle":"2025-06-03T16:32:33.255131Z","shell.execute_reply.started":"2025-06-03T16:32:33.25006Z","shell.execute_reply":"2025-06-03T16:32:33.254187Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# CELL 15: Stage 5 - Attention Mechanism Experiments\nprint(\"\\n--- Cell 15: Stage 5 - Attention Mechanism Experiments ---\")\n\n# The baseline for this stage is the best configuration identified after Cell 14\nattention_baseline_key = current_best_config_key\nattention_stage_keys = [] # Keep track of keys for comparison in this stage\n\nif attention_baseline_key is None or attention_baseline_key not in results:\n    print(\"ERROR: Cannot proceed with Attention stage. Baseline configuration key is missing or invalid.\")\n    # Optionally raise error: raise ValueError(\"Baseline configuration for Attention stage is missing.\")\nelse:\n    print(f\"Using configuration '{attention_baseline_key}' as baseline for Attention stage (No Attention).\")\n    # Add baseline key (representing No Attention) to the list for comparison\n    attention_stage_keys.append(attention_baseline_key)\n\n    # Retrieve the configuration dictionary of the baseline\n    baseline_config_attention = results[attention_baseline_key]['config']\n    # Verify baseline has no attention\n    baseline_attention_type = baseline_config_attention['model_args'].get('attention', None)\n    if baseline_attention_type is not None:\n         print(f\"WARNING: Baseline config '{attention_baseline_key}' for Attention stage unexpectedly has attention type '{baseline_attention_type}'.\")\n\n    attention_types_to_test = ['self', 'channel', 'spatial', 'cbam']\n\n    for test_attention_type in attention_types_to_test:\n        print(f\"\\n--- Running Attention Experiment: {test_attention_type.upper()} ---\")\n        # Construct key by appending attention type to the baseline key for this stage\n        config_attn_key = f\"{attention_baseline_key}_Attn{test_attention_type.upper()}\"\n\n        config_attn = {\n            'key': config_attn_key,\n            'parse_args': baseline_config_attention['parse_args'].copy(),\n            'model_args': baseline_config_attention['model_args'].copy(),\n            'train_args': baseline_config_attention['train_args'].copy(),\n            'oversample_train': baseline_config_attention['oversample_train']\n        }\n\n        # Modify only the attention setting in model_args\n        config_attn['model_args']['attention'] = test_attention_type\n\n        # Run the experiment\n        attn_metrics = run_experiment(\n        config_attn,\n        )\n        if attn_metrics: # Add key only if run succeeded\n            attention_stage_keys.append(config_attn_key)\n\n    print(\"\\nCompleted Attention experiments.\")\n\n\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-03T16:33:05.874842Z","iopub.execute_input":"2025-06-03T16:33:05.875127Z","iopub.status.idle":"2025-06-03T16:43:09.718182Z","shell.execute_reply.started":"2025-06-03T16:33:05.875098Z","shell.execute_reply":"2025-06-03T16:43:09.71729Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# CELL 16: Stage 5 - Attention Comparison & Selection (Best Overall Pre-Finetuning)\nprint(\"\\n--- Cell 16: Stage 5 - Attention Comparison & Selection (Best Overall Pre-Finetuning) ---\")\n\nimport os\nimport json\nimport numpy as np # For np.nan if needed, pd.isna used in plot function\n\n# --- Configuration for this Stage ---\n# These global variables should be defined from previous cells:\n# METRICS_DIR, PLOTS_DIR, TARGET_METRIC, results, current_best_config_key\n\n# The baseline for THIS Attention stage is the winner from the previous (Pooling) stage.\nattention_stage_baseline_key = current_best_config_key\n\n# IMPORTANT: Define the list of exact keys for THIS Attention stage.\n# This list should include the attention_stage_baseline_key (representing no new/specific attention or a default)\n# AND all its attention variants. These names MUST match your \"evaluation_metrics_{key}.json\" filenames.\n# Example: If attention_stage_baseline_key = \"Baseline_StdAug_CLAHE1.0_PoolHYBRID\"\nkeys_for_attention_stage = [\n    attention_stage_baseline_key,  # This is the model configuration before adding specific attention mechanisms for this stage\n    f\"{attention_stage_baseline_key}_AttnCBAM\",\n    f\"{attention_stage_baseline_key}_AttnCHANNEL\",\n    f\"{attention_stage_baseline_key}_AttnSPATIAL\",\n    # Add other specific attention mechanism experiment keys you ran based on this baseline.\n    # Adjust if your naming convention is different (e.g., if attention is part of the pooling key like _PoolHYBRID_AttnCBAM)\n    # In that case, your list might be more like the pooling stage list but varying the Attn part.\n    # Given your file list, if pooling_stage_baseline_key was \"Baseline_StdAug_CLAHE1.0_PoolHYBRID\", then:\n    # keys_for_attention_stage = [\n    #    \"Baseline_StdAug_CLAHE1.0_PoolHYBRID\", # No specific additional attention\n    #    \"Baseline_StdAug_CLAHE1.0_PoolHYBRID_AttnCBAM\",\n    #    \"Baseline_StdAug_CLAHE1.0_PoolHYBRID_AttnCHANNEL\",\n    #    \"Baseline_StdAug_CLAHE1.0_PoolHYBRID_AttnSPATIAL\",\n    # ]\n]\n# Remove duplicates if the baseline itself represents one of the attention types implicitly\nkeys_for_attention_stage = list(dict.fromkeys(keys_for_attention_stage))\n\n# This variable will store the ultimate winner before any fine-tuning.\nbest_overall_pre_finetune_key = None # Initialize\n\n# --- Pre-computation & Sanity Checks ---\nprint(f\"DEBUG: Baseline for this Attention Stage (current_best_config_key entering Cell 16): '{attention_stage_baseline_key}'\")\nprint(f\"DEBUG: TARGET_METRIC for selection is: '{TARGET_METRIC}'\")\nprint(f\"DEBUG: METRICS_DIR is: '{METRICS_DIR}'\")\nprint(f\"DEBUG: Intended keys for Attention Stage (before file check): {keys_for_attention_stage}\")\n\n# Filter these keys to only include those for which a metrics JSON file actually exists\nvalid_keys_for_attention_comparison = [\n    key for key in keys_for_attention_stage\n    if os.path.exists(os.path.join(METRICS_DIR, f\"evaluation_metrics_{key}.json\"))\n]\nprint(f\"Found metric files for and will compare these Attention Stage configurations: {valid_keys_for_attention_comparison}\")\n\n\nif not attention_stage_baseline_key or attention_stage_baseline_key not in valid_keys_for_attention_comparison:\n    print(f\"ERROR: Baseline for Attention Stage ('{attention_stage_baseline_key}') metric file not found or key is None. \"\n          \"Skipping Attention comparison.\")\n    best_overall_pre_finetune_key = current_best_config_key # Fallback to previous stage's winner\n    print(f\"Best overall pre-finetune key defaults to: '{best_overall_pre_finetune_key}'\")\nelif len(valid_keys_for_attention_comparison) <= 1:\n    print(\"Not enough successful Attention runs (including this stage's baseline/No Attention) to perform a meaningful comparison.\")\n    best_overall_pre_finetune_key = current_best_config_key # The baseline for this stage is effectively the winner\n    print(f\"Best overall pre-finetune key set to: '{best_overall_pre_finetune_key}' (baseline of this stage).\")\nelse:\n    # --- Plotting ---\n    metrics_to_request_for_plot = [\n        'f1_opt', 'accuracy_opt', 'precision_opt', 'recall_opt', 'roc_auc_proba', 'pr_auc'\n    ]\n    if TARGET_METRIC not in metrics_to_request_for_plot:\n        metrics_to_request_for_plot.insert(0, TARGET_METRIC)\n    metrics_to_request_for_plot = list(dict.fromkeys(metrics_to_request_for_plot))\n\n    df_attention_comparison = plot_comparison_bars_enhanced(\n        config_keys_to_plot=valid_keys_for_attention_comparison,\n        metrics_dir=METRICS_DIR,\n        title=f\"Attention Mechanism Stage Comparison (Base: {attention_stage_baseline_key})\",\n        save_dir=PLOTS_DIR,\n        metrics_to_display=metrics_to_request_for_plot\n    )\n\n    # --- Selection Logic (based on JSON files) ---\n    best_score_this_stage = -1.0\n    winner_key_this_stage = None \n    winner_metrics_this_stage = None\n\n    baseline_score_for_this_stage_ref = -1.0\n    baseline_ref_json_path = os.path.join(METRICS_DIR, f\"evaluation_metrics_{attention_stage_baseline_key}.json\")\n    try:\n        with open(baseline_ref_json_path, 'r') as f:\n            baseline_ref_metrics_data = json.load(f)\n        baseline_score_for_this_stage_ref = baseline_ref_metrics_data.get(TARGET_METRIC, -1.0)\n    except Exception as e:\n        print(f\"Warning: Error loading metrics for this stage's baseline '{attention_stage_baseline_key}': {e}\")\n\n    print(f\"\\nSelecting best configuration from Attention stage using '{TARGET_METRIC}' \"\n          f\"(Score of '{attention_stage_baseline_key}' (No new Attention) for reference: {baseline_score_for_this_stage_ref:.4f}):\")\n\n    for config_key in valid_keys_for_attention_comparison:\n        json_path = os.path.join(METRICS_DIR, f\"evaluation_metrics_{config_key}.json\")\n        current_score_from_json = -1.0\n        current_metrics_from_json = None\n        try:\n            with open(json_path, 'r') as f:\n                current_metrics_from_json = json.load(f)\n            current_score_from_json = current_metrics_from_json.get(TARGET_METRIC, -1.0)\n            metric_display_value = f\"{current_score_from_json:.4f}\" if current_score_from_json != -1.0 or TARGET_METRIC in current_metrics_from_json else \"Not Found\"\n            print(f\"  - Config '{config_key}': {TARGET_METRIC} = {metric_display_value}\")\n            if current_score_from_json > best_score_this_stage: # Assumes higher is better\n                best_score_this_stage = current_score_from_json\n                winner_key_this_stage = config_key\n                winner_metrics_this_stage = current_metrics_from_json\n        except Exception as e:\n            print(f\"  - Config '{config_key}': Error loading/processing metrics from JSON - {e}. Skipping.\")\n            continue\n            \n    # --- Announce Winner of this Stage and Update GLOBAL current_best_config_key ---\n    # This winner is also the best_overall_pre_finetune_key\n    if winner_key_this_stage and best_score_this_stage > -1.0: \n        print(f\"\\n🏆 Winner of Attention Stage (Best Overall Pre-Finetune): '{winner_key_this_stage}' ({TARGET_METRIC}: {best_score_this_stage:.4f})\")\n        current_best_config_key = winner_key_this_stage \n        best_overall_pre_finetune_key = current_best_config_key # Store specifically\n        print(\"This configuration will be used for the final Fine-tuning stage.\")\n\n\n        if winner_key_this_stage in results and 'config' in results[winner_key_this_stage]:\n            print(\"\\nBest Overall (Pre-Finetuning) Configuration Details (from in-memory 'results'):\")\n            winner_config_dict = results[winner_key_this_stage]['config']\n            winning_attention_type = winner_config_dict.get('model_args', {}).get('attention', 'N/A (or baseline)')\n            print(f\"  (Winning Attention Type from config: {winning_attention_type})\")\n            for detail_key, detail_value in winner_config_dict.items():\n                if isinstance(detail_value, dict): # More concise print for nested dicts\n                    # Check for 'model_args' specifically to print its content if desired\n                    if detail_key == 'model_args':\n                        print(f\"  {detail_key}:\")\n                        for sub_k, sub_v in detail_value.items(): print(f\"    {sub_k}: {sub_v}\")\n                    elif detail_key == 'parse_args' and 'augment_layer' in detail_value:\n                        print(f\"  {detail_key}:\")\n                        for pa_key, pa_value in detail_value.items():\n                             if pa_key == 'augment_layer': print(f\"    {pa_key}: <Keras Layer Object>\")\n                             else: print(f\"    {pa_key}: {pa_value}\")\n                    else:\n                        print(f\"  {detail_key}: {{...}}\") # Default concise print for other dicts\n                else: print(f\"  {detail_key}: {detail_value}\")\n        else:\n            print(f\"Full configuration details for winner '{winner_key_this_stage}' not found in 'results'.\")\n\n        if winner_metrics_this_stage:\n            print(\"\\nBest Overall (Pre-Finetuning) Metrics (from JSON):\")\n            for m_key, m_val in winner_metrics_this_stage.items():\n                if isinstance(m_val, (float, np.floating)): print(f\"  {m_key}: {m_val:.4f}\")\n                else: print(f\"  {m_key}: {m_val}\")\n    else:\n        print(\"\\nCould not determine a new winner for the Attention stage.\")\n        # If no new winner, the baseline for this stage (winner of previous stage) remains the best pre-finetune\n        best_overall_pre_finetune_key = attention_stage_baseline_key \n        # current_best_config_key also remains attention_stage_baseline_key\n        print(f\"The configuration '{attention_stage_baseline_key}' (Score: {baseline_score_for_this_stage_ref:.4f}) remains the best overall pre-finetune.\")\n\n# --- Final Sanity Check before potentially moving to a fine-tuning cell ---\nif best_overall_pre_finetune_key is None or \\\n   not os.path.exists(os.path.join(METRICS_DIR, f\"evaluation_metrics_{best_overall_pre_finetune_key}.json\")):\n    print(\"\\nCRITICAL WARNING: Could not determine a valid best overall configuration before fine-tuning, or its metrics file is missing. \"\n          \"The next Fine-tuning stage might not proceed correctly.\")\n    # Consider raising an error: raise RuntimeError(\"Failed to determine best configuration for fine-tuning.\")\nelse:\n    print(f\"\\nReady for Fine-Tuning using configuration: '{best_overall_pre_finetune_key}'\")\n\nprint(\"-\" * 70)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-03T16:45:02.408324Z","iopub.execute_input":"2025-06-03T16:45:02.408595Z","iopub.status.idle":"2025-06-03T16:45:04.183744Z","shell.execute_reply.started":"2025-06-03T16:45:02.408567Z","shell.execute_reply":"2025-06-03T16:45:04.182925Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# CELL 17: Stage 6 - Fine-Tuning Experiment\nprint(\"\\n--- Cell 17: Stage 6 - Fine-Tuning Experiment ---\")\n\nimport os\nimport time # Make sure time is imported if not already at the top\nimport numpy as np # Make sure numpy is imported\n# import tensorflow as tf # Already imported in Cell 6 usually\n# from tensorflow import keras # Already imported in Cell 6 usually\nimport gc # Make sure gc is imported\n\n# Assumed global variables:\n# best_overall_pre_finetune_key (from Cell 16)\n# results (global dictionary)\n# CHECKPOINT_DIR, PLOTS_DIR, METRICS_DIR\n# train_paths, train_labels, val_paths, val_labels, test_paths, test_labels\n# preprocess_image, IMG_SIZE, GLOBAL_BATCH_SIZE\n# strategy, custom_objects_map, inv_label_dict, label_dict\n# LEARNING_RATE_FINETUNE, EPOCHS_FINETUNE\n# PATIENCE_EARLY_STOPPING, PATIENCE_REDUCE_LR, MIN_LR\n# TARGET_METRIC (e.g., 'f1_opt')\n# train_model (function from Cell 6)\n# evaluate_model_optimized_with_viz (function from Cell 6)\n# plot_training_history_enhanced (function from Cell 6)\n# class_weights_dict (if class weights are used)\n\nfine_tuned_run_key = None # Use a different variable name to avoid confusion if cell is re-run\n\nif best_overall_pre_finetune_key is None or best_overall_pre_finetune_key not in results:\n    print(\"CRITICAL ERROR: Cannot proceed with Fine-tuning. Best pre-finetune configuration key is missing or invalid.\")\nelse:\n    print(f\"Starting Fine-tuning based on configuration: '{best_overall_pre_finetune_key}'\")\n\n    pre_ft_config_data = results[best_overall_pre_finetune_key].get('config')\n    pre_ft_checkpoint_path = results[best_overall_pre_finetune_key].get('checkpoint_path')\n\n    if not pre_ft_config_data or not pre_ft_checkpoint_path or not os.path.exists(pre_ft_checkpoint_path):\n        print(f\"ERROR: Cannot fine-tune. Missing config or checkpoint file ({pre_ft_checkpoint_path}) for the best pre-finetune model '{best_overall_pre_finetune_key}'.\")\n    else:\n        fine_tuned_run_key = f\"{best_overall_pre_finetune_key}_FineTuned\" # Construct a unique key\n        \n        # Define Fine-tuning specific parameters\n        unfreeze_from_block = 'conv5_block' # Example: Start unfreezing from conv block 5 for DenseNet\n        fine_tuning_lr = LEARNING_RATE_FINETUNE # Should be a very small LR\n        fine_tuning_epochs = EPOCHS_FINETUNE\n        fine_tune_checkpoint_filepath = os.path.join(CHECKPOINT_DIR, f\"{fine_tuned_run_key}_best.keras\")\n\n        print(f\"Fine-tuning Key: {fine_tuned_run_key}\")\n        print(f\"Loading model from: {pre_ft_checkpoint_path}\")\n        print(f\"Unfreezing from: '{unfreeze_from_block}' (or all if not found)\")\n        print(f\"Fine-tuning LR: {fine_tuning_lr}, Max Epochs: {fine_tuning_epochs}\")\n        print(f\"Best fine-tuned model will be saved to: {fine_tune_checkpoint_filepath}\")\n\n        model_ft = None\n        history_ft = None\n        eval_metrics_ft = None\n        training_duration_ft = 0\n        start_time_total_ft = time.time()\n\n        try:\n            # --- 1. Build Datasets (using config from best pre-ft run) ---\n            print(\"\\n[FT-1. Building Datasets for Fine-Tuning...]\")\n            ft_parse_args = pre_ft_config_data['parse_args'].copy()\n            ft_oversample = pre_ft_config_data.get('oversample_train', False)\n\n            train_ds_ft = build_dataset(\n                train_paths, train_labels, preprocess_image, ft_parse_args,\n                GLOBAL_BATCH_SIZE, f\"Train ({fine_tuned_run_key})\", shuffle=True,\n                augment_in_map=ft_parse_args.get('apply_augment', False),\n                oversample=ft_oversample, cache=True\n            )\n            val_ft_parse_args = ft_parse_args.copy(); val_ft_parse_args['apply_augment'] = False\n            test_ft_parse_args = ft_parse_args.copy(); test_ft_parse_args['apply_augment'] = False\n            val_ds_ft = build_dataset(val_paths, val_labels, preprocess_image, val_ft_parse_args, GLOBAL_BATCH_SIZE, f\"Val ({fine_tuned_run_key})\", shuffle=False, cache=True)\n            test_ds_ft = build_dataset(test_paths, test_labels, preprocess_image, test_ft_parse_args, GLOBAL_BATCH_SIZE, f\"Test ({fine_tuned_run_key})\", shuffle=False, cache=True)\n\n            if not all([train_ds_ft, val_ds_ft, test_ds_ft]):\n                raise RuntimeError(\"Failed to build fine-tuning datasets.\")\n            print(\"Fine-tuning datasets built successfully.\")\n\n            # --- 2. Load Best Pre-FT Model ---\n            print(\"\\n[FT-2. Loading Best Pre-Finetune Model...]\")\n            with strategy.scope():\n                model_ft = keras.models.load_model(pre_ft_checkpoint_path, custom_objects=custom_objects_map)\n            print(f\"Model '{model_ft.name}' loaded successfully.\")\n\n            # --- 3. Unfreeze Layers ---\n            print(f\"\\n[FT-3. Unfreezing Base Model Layers from '{unfreeze_from_block}'...]\")\n            base_model_to_unfreeze = None\n            for layer in model_ft.layers:\n                if isinstance(layer, keras.Model) and ('densenet' in layer.name or 'efficientnet' in layer.name or 'resnet' in layer.name): # More generic base model check\n                    base_model_to_unfreeze = layer\n                    break\n            if not base_model_to_unfreeze and len(model_ft.layers) > 1 and isinstance(model_ft.layers[1], keras.Model): # Fallback\n                base_model_to_unfreeze = model_ft.layers[1]\n                print(f\"Warning: Assuming base model is layer '{base_model_to_unfreeze.name}' based on position.\")\n            \n            if not base_model_to_unfreeze:\n                raise ValueError(\"Could not identify base model layer for unfreezing.\")\n            print(f\"Identified base model for unfreezing: '{base_model_to_unfreeze.name}'\")\n\n            base_model_to_unfreeze.trainable = True\n            unfreeze_from_index = -1\n            if unfreeze_from_block: # Only try to find specific block if name is given\n                for i, layer in enumerate(base_model_to_unfreeze.layers):\n                    if layer.name.startswith(unfreeze_from_block):\n                        unfreeze_from_index = i\n                        break\n                if unfreeze_from_index == -1:\n                    print(f\"Warning: Layer prefix '{unfreeze_from_block}' not found in base model '{base_model_to_unfreeze.name}'. Unfreezing ALL base layers.\")\n                    unfreeze_from_index = 0 \n            else: # If unfreeze_from_block is None or empty, unfreeze all\n                print(\"Unfreezing ALL layers in the base model.\")\n                unfreeze_from_index = 0\n            \n            num_frozen_in_base = 0\n            if unfreeze_from_index > 0:\n                print(f\"Freezing layers in '{base_model_to_unfreeze.name}' before index {unfreeze_from_index} ('{base_model_to_unfreeze.layers[unfreeze_from_index].name}')\")\n                for layer in base_model_to_unfreeze.layers[:unfreeze_from_index]:\n                    if layer.trainable: layer.trainable = False; num_frozen_in_base += 1\n                print(f\"Froze {num_frozen_in_base} layers in the base model.\")\n            else:\n                print(f\"All {len(base_model_to_unfreeze.layers)} layers in '{base_model_to_unfreeze.name}' will be trainable (or maintain their current trainable status if set individually).\")\n\n            trainable_count = np.sum([np.prod(w.shape) for w in model_ft.trainable_weights])\n            non_trainable_count = np.sum([np.prod(w.shape) for w in model_ft.non_trainable_weights])\n            print(f\"Total Trainable weights in full model: {trainable_count:,}\")\n            print(f\"Total Non-trainable weights in full model: {non_trainable_count:,}\")\n\n            # --- 4. Re-compile Model ---\n            print(\"\\n[FT-4. Re-compiling Model for Fine-tuning...]\")\n            with strategy.scope():\n                model_ft.compile(\n                    optimizer=keras.optimizers.Adam(learning_rate=fine_tuning_lr),\n                    loss='binary_crossentropy',\n                    metrics=[ 'accuracy', tf.keras.metrics.Precision(name='precision'),\n                              tf.keras.metrics.Recall(name='recall'), tf.keras.metrics.AUC(name='auc')]\n                )\n            print(f\"Model re-compiled with LR={fine_tuning_lr}.\")\n\n            # --- 5. Define Fine-Tuning Callbacks ---\n            print(\"\\n[FT-5. Setting up Fine-Tuning Callbacks...]\")\n            ft_callbacks_list = [\n                EarlyStopping(monitor='val_loss', patience=PATIENCE_EARLY_STOPPING + 2, verbose=1, restore_best_weights=False),\n                ReduceLROnPlateau(monitor='val_loss', factor=0.2, patience=PATIENCE_REDUCE_LR, min_lr=MIN_LR / 5, verbose=1), # Potentially even lower min_lr\n                ModelCheckpoint(filepath=fine_tune_checkpoint_filepath, monitor='val_loss', save_best_only=True, save_weights_only=False, verbose=1)\n            ]\n            print(f\"Best fine-tuned model checkpoint path: {fine_tune_checkpoint_filepath}\")\n\n            # --- 6. Train (Fine-tune) ---\n            print(\"\\n[FT-6. Starting Fine-tuning Training Phase...]\")\n            ft_class_weights_setting = pre_ft_config_data['train_args'].get('class_weights_setting')\n            ft_train_class_weights = class_weights_dict if ft_class_weights_setting == 'balanced' else None\n\n            history_ft, training_duration_ft = train_model(\n                model_ft, train_ds_ft, val_ds_ft,\n                epochs=fine_tuning_epochs, initial_epoch=0, \n                class_weights=ft_train_class_weights, strategy=strategy,\n                learning_rate=fine_tuning_lr, # LR is set during compile, but train_model might use it for logging\n                callbacks=ft_callbacks_list,\n                stage_name=f\"Fine-Tuning ({fine_tuned_run_key})\"\n            )\n            if history_ft is None: raise RuntimeError(\"Model fine-tuning training failed.\")\n\n            # --- 7. Load Best Fine-tuned Weights ---\n            print(\"\\n[FT-7. Loading Best Weights from Fine-Tuning Checkpoint...]\")\n            model_to_eval_ft = model_ft # Default to last epoch if checkpoint missing\n            if os.path.exists(fine_tune_checkpoint_filepath):\n                with strategy.scope():\n                    model_to_eval_ft = keras.models.load_model(fine_tune_checkpoint_filepath, custom_objects=custom_objects_map)\n                print(f\"Successfully loaded best fine-tuned model from {fine_tune_checkpoint_filepath}\")\n            else:\n                print(f\"WARNING: Fine-tuning checkpoint not found at {fine_tune_checkpoint_filepath}. Evaluating with last FT epoch's weights.\")\n\n            # --- 8. Evaluate Final Fine-tuned Model ---\n            print(\"\\n[FT-8. Evaluating Final Fine-tuned Model...]\")\n            eval_metrics_ft, y_true_test_ft, y_pred_proba_test_ft, _ = evaluate_model_optimized_with_viz( # CORRECTED\n                model=model_to_eval_ft, # Use the reloaded best model\n                val_ds=val_ds_ft, \n                test_ds=test_ds_ft,\n                strategy=strategy,\n                inv_label_map=inv_label_dict,\n                target_metric=TARGET_METRIC, # Ensure TARGET_METRIC is globally defined\n                dataset_name=f\"Test ({fine_tuned_run_key})\",\n                save_dir=PLOTS_DIR,\n                config_name=fine_tuned_run_key\n            )\n            if not eval_metrics_ft:\n                print(\"Warning: Fine-tuned evaluation failed or returned no metrics.\")\n                eval_metrics_ft = {}\n\n            # --- 9. Grad-CAM on Fine-tuned Model (REMOVED/COMMENTED as per previous request) ---\n            # print(\"\\n[FT-9. Generating Grad-CAM for Fine-tuned Model...]\")\n            # try:\n            #     # ... (Grad-CAM logic if you re-enable it, ensure it uses model_to_eval_ft) ...\n            #     print(\"Grad-CAM for fine-tuned model is currently disabled.\")\n            # except Exception as grad_e_ft:\n            #     print(f\"Error during fine-tuned Grad-CAM generation: {grad_e_ft}\")\n\n\n            # --- 10. Store Fine-Tuned Results ---\n            print(\"\\n[FT-10. Storing Fine-tuned Results...]\")\n            total_duration_ft = time.time() - start_time_total_ft\n            \n            # Save metrics to JSON for the fine-tuned model\n            metrics_json_save_path_ft = None\n            if eval_metrics_ft:\n                metrics_json_save_path_ft = os.path.join(METRICS_DIR, f\"evaluation_metrics_{fine_tuned_run_key}.json\")\n                try:\n                    serializable_metrics_ft = {}\n                    for m_key, m_value in eval_metrics_ft.items():\n                        if isinstance(m_value, np.generic): serializable_metrics_ft[m_key] = m_value.item()\n                        elif isinstance(m_value, np.ndarray): serializable_metrics_ft[m_key] = m_value.tolist()\n                        else: serializable_metrics_ft[m_key] = m_value\n                    with open(metrics_json_save_path_ft, 'w') as f:\n                        json.dump(serializable_metrics_ft, f, indent=4)\n                    print(f\"Fine-tuned evaluation metrics saved to {metrics_json_save_path_ft}\")\n                except Exception as e:\n                    print(f\"Error saving fine-tuned evaluation metrics to JSON: {e}\")\n                    metrics_json_save_path_ft = None\n            \n            results[fine_tuned_run_key] = {\n                'config': pre_ft_config_data, \n                'fine_tune_params': {\n                    'unfreeze_from_block': unfreeze_from_block, 'lr': fine_tuning_lr,\n                    'epochs_run': len(history_ft.epoch) if history_ft and hasattr(history_ft, 'epoch') else 0,\n                },\n                'metrics': eval_metrics_ft,\n                'training_duration_sec': training_duration_ft,\n                'total_duration_sec': total_duration_ft,\n                'checkpoint_path': fine_tune_checkpoint_filepath if os.path.exists(fine_tune_checkpoint_filepath) else None,\n                'metrics_json_path': metrics_json_save_path_ft\n            }\n            print(f\"Results for fine-tuned model '{fine_tuned_run_key}' stored.\")\n\n            # --- 11. Plotting Fine-Tuned Results ---\n            print(\"\\n[FT-11. Plotting Fine-tuned Results...]\")\n            if history_ft:\n                plot_training_history_enhanced( # CORRECTED\n                    history_ft,\n                    title_suffix=f\"({fine_tuned_run_key} - FT Phase)\",\n                    save_dir=PLOTS_DIR,\n                    config_name=fine_tuned_run_key\n                )\n            # Other plots (CM, ROC/PR) are now handled by evaluate_model_optimized_with_viz in FT-8.\n\n            print(f\"--- Fine-Tuning Experiment {fine_tuned_run_key} Complete ---\")\n            # Update current_best_config_key if fine-tuned model is better\n            # This comparison should ideally happen in the next cell (Final Summary Comparison)\n            # For now, we just record the fine_tuned_run_key.\n            # The next cell (Cell 18) will compare this fine_tuned_run_key with best_overall_pre_finetune_key.\n\n        except Exception as e:\n            print(f\"\\n\\n ****** ERROR during FINE-TUNING experiment {fine_tuned_run_key} ****** \")\n            print(f\"Error Type: {type(e).__name__}\")\n            print(f\"Error Details: {e}\")\n            import traceback # Moved import here for when it's actually needed\n            traceback.print_exc()\n            if fine_tuned_run_key: # Only store if key was generated\n                results[fine_tuned_run_key] = {'status': 'failed', 'error': str(e), 'config': pre_ft_config_data if 'pre_ft_config_data' in locals() else {}}\n            fine_tuned_run_key = None # Mark as failed by nullifying the key for later checks\n\n        # finally:\n        #     # --- 12. Fine-Tuning Cleanup ---\n        #     print(\"\\n[FT-12. Cleaning up fine-tuning resources...]\")\n        #     del model_ft, train_ds_ft, val_ds_ft, test_ds_ft, history_ft \n        #     if 'keras' in globals() or 'tensorflow.keras' in globals():\n        #          keras.backend.clear_session()\n        #     gc.collect()\n        #     print(\"Fine-tuning cleanup complete.\")\n            \n# --- Update current_best_config_key AFTER fine-tuning IF it was successful AND better ---\n# This comparison and update is typically done in the *next* cell (Cell 18 - Final Summary)\n# For now, Cell 17 just runs the fine-tuning experiment and records its result.\n# Cell 18 will compare fine_tuned_run_key with best_overall_pre_finetune_key.\n# So, current_best_config_key is NOT updated here. It's updated by the comparison cells.\n# best_overall_pre_finetune_key remains the best pre-finetuning key.\n# fine_tuned_run_key (if successful) is the key for the fine-tuned version.\n\nprint(\"-\" * 70)\nif fine_tuned_run_key and fine_tuned_run_key in results and results[fine_tuned_run_key].get('status') != 'failed':\n    print(f\"Fine-tuning experiment '{fine_tuned_run_key}' completed and results stored.\")\n    print(f\"The best model before this fine-tuning was: '{best_overall_pre_finetune_key}'\")\n    print(f\"Compare metrics of '{fine_tuned_run_key}' with '{best_overall_pre_finetune_key}' in the next cell (Cell 18) to determine the ultimate winner.\")\nelif fine_tuned_run_key and fine_tuned_run_key in results and results[fine_tuned_run_key].get('status') == 'failed':\n    print(f\"Fine-tuning experiment '{fine_tuned_run_key}' FAILED.\")\nelse:\n    print(\"Fine-tuning was not performed or key was not generated due to earlier errors.\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-03T16:52:33.41655Z","iopub.execute_input":"2025-06-03T16:52:33.416928Z","iopub.status.idle":"2025-06-03T16:56:07.180198Z","shell.execute_reply.started":"2025-06-03T16:52:33.416889Z","shell.execute_reply":"2025-06-03T16:56:07.179388Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"","metadata":{"trusted":true,"execution":{"execution_failed":"2025-06-03T13:33:32.214Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# CELL 18: Stage 7 - Final Summary Comparison\nprint(\"\\n--- Cell 18: Stage 7 - Final Summary Comparison ---\")\n\nimport os\nimport json\nimport numpy as np # For isinstance checks in JSON serialization, and pd.isna in plot func\nimport pandas as pd # For pd.isna in plot func\n\n# These global variables should be defined from previous cells:\n# METRICS_DIR (str): Path to evaluation_metrics_{key}.json files.\n# PLOTS_DIR (str): Path to save plots.\n# results (dict): Global dictionary holding all experiment results.\n# plot_comparison_bars_enhanced (function from Cell 6 that reads from JSON)\n\n# Variables holding keys from previous stages (ensure these are correctly set by Cells 8, 10, 12, 14, 16, 17)\n# Example:\n# key1 = \"Baseline_StdAug\" # From Cell 8 (or your actual baseline key)\n# imbalance_baseline_key # Winner of CLAHE, input to Imbalance stage (Cell 12)\n# pooling_baseline_key # Winner of Imbalance, input to Pooling stage (Cell 14)\n# attention_baseline_key # Winner of Pooling, input to Attention stage (Cell 16)\n# best_overall_pre_finetune_key # Winner of Attention stage (Cell 16)\n# fine_tuned_run_key # Key of the fine-tuned model from Cell 17 (use the variable holding the actual key)\n\n\nprint(\"Gathering results for final milestone comparison...\")\n# Store original keys that correspond to milestones\nmilestone_original_keys = []\n# Map descriptive labels to original keys for sorting or direct access if needed\nmilestone_label_to_original_key = {}\n\n# 1. Baseline (StdAug only)\n# Ensure 'key1' is the actual key for your initial baseline experiment.\n# If you have a variable holding this, use it. For example:\n# initial_baseline_key = \"Baseline_StdAug\" # Or whatever it was named\nkey1 = \"Baseline_StdAug\" # Assuming this is your absolute first baseline key\nmilestone_label = \"1. Baseline (Aug)\"\nif key1 and os.path.exists(os.path.join(METRICS_DIR, f\"evaluation_metrics_{key1}.json\")):\n    milestone_original_keys.append(key1)\n    milestone_label_to_original_key[milestone_label] = key1\n    print(f\"- Found Milestone 1: '{key1}' -> '{milestone_label}'\")\nelse:\n    print(f\"- WARNING: Milestone 1 key '{key1}' or its metrics JSON not found.\")\n\n# 2. Best CLAHE\n# This should be the key that won the CLAHE stage (was 'current_best_config_key' after CLAHE cell,\n# and became 'imbalance_stage_baseline_key' at the start of Imbalance cell)\nkey2 = imbalance_baseline_key # Key after CLAHE stage\nmilestone_label = \"2. +CLAHE\" if key2 and \"_CLAHE\" in key2 else \"2. Baseline (No CLAHE)\"\nif key2 and os.path.exists(os.path.join(METRICS_DIR, f\"evaluation_metrics_{key2}.json\")):\n    if key2 not in milestone_original_keys : milestone_original_keys.append(key2) # Avoid duplicates if same as key1\n    milestone_label_to_original_key[milestone_label] = key2\n    print(f\"- Found Milestone 2: '{key2}' -> '{milestone_label}'\")\nelse:\n    print(f\"- WARNING: Milestone 2 key '{key2}' or its metrics JSON not found.\")\n\n# 3. Best Imbalance Handling\nkey3 = pooling_baseline_key # Key after Imbalance stage\nmilestone_label = \"3. +Imbalance\"\nif key3:\n    if \"_ClassWeights\" in key3: milestone_label += \" (Weights)\"\n    elif \"_Oversample\" in key3: milestone_label += \" (Oversample)\"\n    # Add else if neither, assume baseline for imbalance stage didn't change or no specific imbalance won\nif key3 and os.path.exists(os.path.join(METRICS_DIR, f\"evaluation_metrics_{key3}.json\")):\n    if key3 not in milestone_original_keys : milestone_original_keys.append(key3)\n    milestone_label_to_original_key[milestone_label] = key3\n    print(f\"- Found Milestone 3: '{key3}' -> '{milestone_label}'\")\nelse:\n    print(f\"- WARNING: Milestone 3 key '{key3}' or its metrics JSON not found.\")\n\n# 4. Best Pooling\nkey4 = attention_baseline_key # Key after Pooling stage\nmilestone_label = \"4. +Pooling\"\nif key4:\n    if \"_PoolMAX\" in key4: milestone_label += \" (Max)\"\n    elif \"_PoolHYBRID\" in key4: milestone_label += \" (Hybrid)\"\n    elif \"_PoolAVG\" in key4: milestone_label += \" (Avg)\"\nif key4 and os.path.exists(os.path.join(METRICS_DIR, f\"evaluation_metrics_{key4}.json\")):\n    if key4 not in milestone_original_keys : milestone_original_keys.append(key4)\n    milestone_label_to_original_key[milestone_label] = key4\n    print(f\"- Found Milestone 4: '{key4}' -> '{milestone_label}'\")\nelse:\n    print(f\"- WARNING: Milestone 4 key '{key4}' or its metrics JSON not found.\")\n\n# 5. Best Attention (Best Overall Pre-FT)\nkey5 = best_overall_pre_finetune_key # Key after Attention stage\nmilestone_label = \"5. +Attention\"\nif key5:\n    if \"_AttnCBAM\" in key5: milestone_label += \" (CBAM)\"\n    elif \"_AttnCHANNEL\" in key5: milestone_label += \" (Channel)\"\n    elif \"_AttnSPATIAL\" in key5: milestone_label += \" (Spatial)\"\n    # Add other attention types if used\nif key5 and os.path.exists(os.path.join(METRICS_DIR, f\"evaluation_metrics_{key5}.json\")):\n    if key5 not in milestone_original_keys : milestone_original_keys.append(key5)\n    milestone_label_to_original_key[milestone_label] = key5\n    print(f\"- Found Milestone 5: '{key5}' -> '{milestone_label}'\")\nelse:\n    print(f\"- WARNING: Milestone 5 key '{key5}' or its metrics JSON not found.\")\n\n# 6. Fine-Tuned (Result from Stage 6 - Fine-tuning)\nkey6 = fine_tuned_run_key # Use the specific variable from Cell 17 that holds the fine-tuned key\nmilestone_label = \"6. Fine-Tuned\"\nif key6 and os.path.exists(os.path.join(METRICS_DIR, f\"evaluation_metrics_{key6}.json\")):\n    if key6 not in milestone_original_keys : milestone_original_keys.append(key6)\n    milestone_label_to_original_key[milestone_label] = key6\n    print(f\"- Found Milestone 6: '{key6}' -> '{milestone_label}'\")\nelse:\n    print(f\"- WARNING: Milestone 6 key '{key6}' (Fine-Tuned) or its metrics JSON not found.\")\n\n# Ensure milestone_original_keys only contains unique keys that were actually found\nmilestone_original_keys = list(dict.fromkeys(milestone_original_keys)) # Preserves order, removes duplicates\n\n# Plot Final Comparison using the original keys if data available\nif milestone_original_keys:\n    print(\"\\nPlotting Final Milestone Comparison...\")\n    \n    # Define the metrics you want to plot from the JSON files\n    metrics_to_plot_final = ['f1_opt', 'accuracy_opt', 'precision_opt', 'recall_opt', 'roc_auc_proba', 'pr_auc']\n    if TARGET_METRIC not in metrics_to_plot_final: # Ensure target metric is plotted\n        metrics_to_plot_final.insert(0, TARGET_METRIC)\n    metrics_to_plot_final = list(dict.fromkeys(metrics_to_plot_final))\n\n\n    # The plot function now expects original keys.\n    # If you want the bars to be labeled with descriptive names (like \"1. Baseline (Aug)\"),\n    # you would need to modify plot_comparison_bars_enhanced to accept a mapping for y-tick labels,\n    # or ensure the 'Configuration' column in its internal DataFrame is set to these descriptive labels.\n    # For now, it will use the original keys as labels. We can sort milestone_original_keys\n    # based on the milestone order for plotting if the map `milestone_label_to_original_key` is correctly populated.\n\n    # To sort the keys for plotting in milestone order:\n    sorted_descriptive_labels = sorted(milestone_label_to_original_key.keys())\n    ordered_original_keys_for_plot = [milestone_label_to_original_key[label] for label in sorted_descriptive_labels if milestone_label_to_original_key[label] in milestone_original_keys]\n    \n    # Make sure all keys in ordered_original_keys_for_plot are valid and unique\n    ordered_original_keys_for_plot = [k for k in ordered_original_keys_for_plot if k in milestone_original_keys]\n    ordered_original_keys_for_plot = list(dict.fromkeys(ordered_original_keys_for_plot))\n\n\n    if ordered_original_keys_for_plot:\n        plot_comparison_bars_enhanced(\n            config_keys_to_plot=ordered_original_keys_for_plot, # Pass the original keys in desired order\n            metrics_dir=METRICS_DIR,\n            title=\"Final Model Performance Milestones\",\n            save_dir=PLOTS_DIR, # Make sure PLOTS_DIR is defined\n            metrics_to_display=metrics_to_plot_final\n        )\n    else:\n        print(\"\\nNo valid, ordered keys found to generate the final comparison plot from JSONs.\")\n\nelse:\n    print(\"\\nNot enough valid milestone results found to generate the final comparison plot.\")\n\nprint(\"\\n\" + \"=\"*70)\nprint(\"                      End of Experiment Pipeline                      \")\nprint(\"=\"*70)\n\n# --- Optional: Save final results dictionary (the big 'results' dict) to JSON ---\n# This part needs a robust serializer if 'results' contains complex objects like Keras layers.\n# The existing serialization logic you had was a good start.\n# For simplicity, I'm focusing on the plotting part from JSONs.\n# The 'final_experiment_results.json' in your provided file list suggests you do save this.\n\nresults_summary_json_path = os.path.join(PLOTS_DIR, \"pipeline_summary_results.json\") # Save in PLOTS_DIR for outputs\ntry:\n    serializable_summary = {}\n    print(f\"\\nAttempting to create a serializable summary for JSON export to {results_summary_json_path}...\")\n    for milestone_label, original_key in milestone_label_to_original_key.items():\n        if original_key and os.path.exists(os.path.join(METRICS_DIR, f\"evaluation_metrics_{original_key}.json\")):\n            with open(os.path.join(METRICS_DIR, f\"evaluation_metrics_{original_key}.json\"), 'r') as f:\n                metrics = json.load(f)\n            serializable_summary[milestone_label] = {\n                'original_key': original_key,\n                'metrics': metrics\n            }\n            if original_key in results and 'config' in results[original_key]:\n                 # Attempt to serialize basic config parts\n                serializable_summary[milestone_label]['config_summary'] = {\n                    k: str(v) for k, v in results[original_key]['config'].items() if not callable(v) and not isinstance(v, keras.Model) and not isinstance(v, keras.layers.Layer)\n                }\n\n\n    if serializable_summary:\n        with open(results_summary_json_path, 'w') as f:\n            json.dump(serializable_summary, f, indent=4)\n        print(f\"Final milestone metrics summary saved to: {results_summary_json_path}\")\n    else:\n        print(\"No milestone data to save in summary JSON.\")\n\nexcept Exception as json_e:\n    print(f\"\\nWarning: Could not save final milestone summary to JSON. Error: {json_e}\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-03T16:58:01.904402Z","iopub.execute_input":"2025-06-03T16:58:01.904724Z","iopub.status.idle":"2025-06-03T16:58:03.333888Z","shell.execute_reply.started":"2025-06-03T16:58:01.904692Z","shell.execute_reply":"2025-06-03T16:58:03.332971Z"}},"outputs":[],"execution_count":null}]}